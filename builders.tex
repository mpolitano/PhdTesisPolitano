\chapter[Identificación de métodos builders]{Enfoques para la identificación de Métodos Builders}
\label{cap:builders}
\pp{Reemplazar builders por generadores de objetos o algo parecido}

\pp{Tenemos que hacer una meet para ponernos de acuerdo en como llamamos a los builders, armar un
 orden para la presentación de ideas, definir bien los conceptos y evitar
 mencionar cosas que no hay sido definidas.}

\pp{Lo primero que habría que hacer en esta sección es introducir la idea de
builders, y nunca se hace. El párrafo que sigue está fuera de contexto.}
El análisis de software es una tarea crucial en el campo de la Ingeniería de
Software, ya sea para generar tests para el software bajo test (SUT) o para
realizar verificación de modelos de software. Estos enfoques  requieren que los
programadores identifiquen manualmente un subconjunto de los métodos de un
módulo con el fin de dirigir el análisis \pp{Ojo, no hay muchos enfoques que requiran
builders}. En general, al analizar un módulo
\pp{los builders no se usan en general, quizás en algún análisis particular},
los programadores seleccionan un subconjunto de sus métodos que serán
considerados como constructores de objetos (\emph{builders}) para definir lo que
se conoce como un controlador o "driver", que se utilizará para construir
objetos automáticamente para el análisis, combinándolos de manera no
determinista, aleatoria, etc. Esto requiere una inspección minuciosa del módulo
y su estructura, ya que la exhaustividad relativa del análisis (omitir métodos
importantes puede evitar sistemáticamente la generación de diferentes objetos) y
su eficiencia (las diferentes combinaciones acotadas de métodos crecen de manera
exponencial a medida que aumenta el número de métodos) se ven afectados por la
selección \pp{acá estás asumiendo que el que lee sabe de builders, y todavía no
fueron introducidos}.

En este capitulo, proponemos tres enfoques para seleccionar automáticamente un
conjunto de constructores de objetos a partir de la API, basados en un algoritmo
evolutivo, un algoritmo greedy y un algoritmo de particionado en clases de equivalencias. Estos algoritmos favorecen conjuntos de métodos cuyas combinaciones conducen a la generación de conjuntos más grandes de objetos o a aquellos conjuntos que obtienen mejor cobertura de ramas del SUT. Además, los algoritmos tienen en cuenta otras características sobre estos conjuntos de métodos, tratando de priorizar la selección de aquellos con menor cantidad de métodos y con más simples parámetros.

% Para evaluar experimentalmente nuestra propuesta, hemos realizado una evaluación en un conjunto de clases con estado de referencia, que representan casos de uso típicos. Los resultados muestran que nuestro enfoque puede identificar automáticamente conjuntos de constructores de objetos que son suficientes (se pueden utilizar para crear cualquier instancia del módulo) y mínimos (no contienen métodos superfluos), en un tiempo razonable.

% En resumen, en este trabajo abordamos el desafío de seleccionar automáticamente un conjunto óptimo de constructores de objetos a partir de la API de un módulo, con el objetivo de mejorar tanto la exhaustividad como la eficiencia del análisis de software. Nuestros resultados experimentales demuestran la viabilidad y efectividad de nuestra propuesta, lo que abre nuevas perspectivas para la automatización de la selección de constructores de objetos en el análisis de software.


\section{Motivacion}
\label{sec:motivacion}
\begin{table}[H]
\center
{\scriptsize
\begin{tabular}{|l|l|l|l|}
\hline
No. &Return type & Method name & Obs? \\
\hline
    0 && NCL() & no \\
    1& & NCL(int) & no \\
    2&& NCL(Collection) & no \\
    3&boolean & add(Object) & no \\
    4&void&add(int,Object) & no \\
    5&boolean&addAll(Collection) & no\\
    6&boolean&addAll(int,Collection) & no \\
    7&boolean&addFirst(Object) & no \\
    8&boolean&addLast(Object) & no\\
    9&void&clear() & no\\
    10&boolean&contains(Object) & yes \\
    11&boolean&containsAll(Collection) & yes \\
    12&boolean&equals(Object) & yes \\
    13&Object&get(int) & yes\\
    14&Object&getFirst() &yes \\
    15&Object&getLast() & yes\\
    16&int&indexOf(Object) &yes\\
    17&boolean&isEmpty() & yes\\
    18&Iterator&iterator() & no\\
    19&int&lastIndexOf(Object) &yes \\
    20&ListIterator&listIterator() &no \\
    21&ListIterator&listIterator(int) & no\\
    22&Object&remove(int) &no\\
    23&boolean&remove(Object) & no \\
    24&boolean&removeAll(Collection) & no \\
    25&Object&removeFirst() &no\\
    26&Object&removeLast() &no\\
    27&boolean&retainAll(Collection) &no \\
    28&Object&set(int,Object) &no\\
    29&int&size() &yes\\
    30&List&subList(int,int) & no \\
    31&Object[]&toArray() & yes \\
    32&Object[]&toArray(Object[]) &yes\\
    33&String&toString() & yes \\
\hline
\end{tabular}
}
\caption{Apache's NodeCachingLinkedList API}
\label{tab:ncl-api}
\end{table} 
En esta sección, motivamos nuestro enfoque mediante un ejemplo práctico. La estructura de datos NodeCachingLinkedList (NCL) de Apache \cite{apache} consta de una lista principal circular doblemente enlazada  que contiene los elementos de la colección y una lista secundaria simplemente enlazada que actúa como caché para los nodos que se han eliminado de la lista principal. Los nodos almacenados en la caché pueden ser reutilizados y añadidos de nuevo a la lista principal al insertar elementos en ella. Gracias a su caché, en las aplicaciones en las que las inserciones y eliminaciones de la lista son muy frecuentes, NCL puede reducir significativamente la sobrecarga necesaria para la asignación de memoria y la recolección de basura de los nodos. 
Como ilustración, la Figura~\ref{fig:ncl-instances} muestra las tres instancias de NCL que se pueden construir con exactamente dos nodos.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{NCL-instances.png}
    \caption{Tres instancias de NodeCachingLinkedList con exactamente dos nodos}
    \label{fig:ncl-instances}
\end{figure}


NCL tiene una API muy completa, como se muestra en la Tabla~\ref{tab:ncl-api}.
Sin embargo, para construir cualquier objeto de NCL, sólo se necesitan algunos
métodos de la API. Por ejemplo, las combinaciones de los métodos en la
Figura~\ref{fig:NCLbuilders}, cuando se instancian con los parámetros
apropiados, se pueden utilizar para construir cualquier objeto de NCL acotado
con el scope deseado \pp{Acá aparece scope y nunca hablaste de scope todavía}.
Por lo tanto, los siguientes métodos \pp{Los metiste en una figura, y venís
hablando de ellos, esto no se lee bien}:
\\
\begin{lstlisting}[numbers=none,label=fig:NCLbuilders, caption=Conjunto de metodos sufiente para NCL]
  (0)  NodeCachingLinkedList()
  (7)  addFirst(Object)
  (25) removeFirst()
\end{lstlisting}

 son un ejemplo de un conjunto suficiente de constructores (\emph{builders}) de
 NCL \pp{Nunca definiste lo que es un conjunto suficiente, ni builders}.
 Hay que tener en cuenta que, después de utilizar el método constructor \pp{hay
     que buscarle la vuelta para nombrar a builders algo que no sea
 constructores, porque sino se confunde con los constructores de la clase. Para
 mi métodos generadores de objetos suena bien}, la
 lista principal de NCL se puede completar simplemente utilizando el método
 \texttt{addFirst}. Sin embargo, si queremos generar instancias en las que la
 lista caché no esté vacía, podemos hacerlo a través del método
 \texttt{removeFirst}, como sugiere el conjunto suficiente de constructores.
 Para la mayoría de las técnicas de análisis automáticas \pp{no son la mayoría},
 nos gustaría considerar tantos escenarios (entradas) variables como sea
 posible, de ahí la motivación para construir estas con un un conjunto
 suficientes de métodos de la API \pp{acá hay un salto a generación de entradas
 que no se entiende}. Además, los metodos \emph{builders} de la
 figura~\ref{fig:NCLbuilders} también son mínimos \pp{no definido}, ya que la falta de alguno de
 ellos implicaría que algunos objetos de NCL ya no se pueden construir bajo el
 scope deseado \pp{no definido}.

Hay que tener en cuenta que puede haber muchos conjuntos de constructores suficientes \pp{no definido}. Por ejemplo, se pueden obtener constructores suficientes reemplazando el método \texttt{addFirst} en la Figura~\ref{fig:NCLbuilders} por cualquier otra variante de \texttt{add} que se muestra en la Figura~\ref{fig:NCLadds}, ya que para cualquier manera de llenar la lista principal de NCL con \texttt{addFirst}, existe una forma diferente de construir el mismo objeto utilizando otra variante de \texttt{add} (quizás invocada con diferentes parámetros y cambiando el orden de ejecución).
\\
\begin{lstlisting}[numbers=none,label=fig:NCLadds, caption=Variantes del método 'Add' que puedo ser utilizado para rellanar la lista principal en NCL, captionpos=b, frame=tb , basicstyle=\scriptsize]
  (3) add(Object)
  (4) add(int,Object)
  (7) addFirst(Object)
  (8) addLast(Object)
\end{lstlisting}

Además, es importante remarcar la importancia de obtener subconjuntos que sean
mínimos\pp{acá aparece recién la definición de minímos}, lo que significa que 
no contengan métodos adicionales \pp{qué signfica esto?}. Esto es crucial para
utilizar la combinación de estos métodos en la construcción de objetos. Cuantos
más métodos tenga el subconjunto de builders, más costosa será la combinación de
los mismos para generar los objetos \pp{no está definido como se piensa generar
los objetos}.

Los siguientes subconjuntos son suficientes pero no son mínimos para NCL:
\\

\begin{lstlisting}[numbers=none,label=fig:NCLnoMin, frame=tb , basicstyle=\scriptsize]
  (0)  NodeCachingLinkedList()
  (7)  addFirst(Object)
  (3)  addF(Object)
  (25) removeFirst()
\end{lstlisting}

\begin{lstlisting}[numbers=none,label=fig:NCLnoMin1, caption= Conjuntos de metodos builders suficientes pero no mínimos, captionpos=b, frame=tb , basicstyle=\scriptsize]
  (0)  NodeCachingLinkedList()
  (7)  addFirst(Object)
  (3)  remove(Object)
  (25) removeFirst()
\end{lstlisting}

También observamos que cuanto más simples sean los parámetros de una rutina, más
fácil será utilizarla para generar entradas en el contexto de un análisis de
programas. Por ejemplo, entre las alternativas de rutinas para insertar datos en
la lista principal de NCL (Figura.~\ref{fig:NCLadds}), \texttt{add(int,Object)}
recibe más parámetros que los otros tres métodos, por lo tanto es más difícil
generar parámetros para brindale a esta rutina a la hora de generar entradas
\pp{No está definido como se generarían entradas}. Esto hace que las otras tres alternativas tengan mas preferencia sobre esta. Así, nuestro enfoque tiene en cuenta el número de parámetros y sus complejidades para seleccionar los constructores mejores posibles.

Muchos métodos en la tabla \ref{tab:ncl-api} están marcados como observadores
(columna Obs?), lo que significa que no modifican los objetos en los que operan,
ni son útiles para crear objetos. Por lo tanto, los observadores siempre son
superfluos y nunca deben incluirse en un conjunto de constructores mínimos
\pp{no definido}. Nuestro enfoque intenta reconocerlos de antemano y
descartarlos de la búsqueda para reducir significativamente el espacio de
búsqueda \pp{Buscaría hablar del concepto de builders antes que de los enfoques}.

Todos estos problemas planteados en este capítulo fueron lo que nos motivó a desarrollar enfoques para obtener automáticamente métodos constructores de manera eficiente y mínima. Para lograrlo, utilizamos una variedad de técnicas y algoritmos que explicaremos en la próxima sección. 


\section{Algoritmos}


\label{sec:algorithms}
Para encontrar un conjunto suficiente \pp{y mínimo?} de constructores a partir
de una API de un programa, diseñamos tres algoritmos de búsqueda que se
describen a continuación. El primero es una modificación de los algoritmos
genéticos \pp{un algoritmo genético?}, el segundo es un algoritmo \emph{greedy}
más precisamente una variante de un algoritmo \emph{Hill Climbing}, y el tercero
es un algoritmo de búsqueda de acuerdo a las \emph{clases de equivalencias}  de
los subconjuntos respecto al valor de su función de valoración \pp{no definida}.
La función de evaluación, también conocida como función objetivo o función de
fitness, desempeña un papel crucial en los algoritmos de búsqueda
\pp{informada}. Esta función asigna un valor numérico a cada solución candidata
en función de su calidad o idoneidad \pp{idoneidad no se usa} para resolver el
problema en cuestión. El propósito de la función de evaluación es proporcionar
una medida cuantitativa de la calidad de cada solución en relación con los
objetivos y restricciones del problema \pp{no son estándar estas definiciones,
hay que buscar mejores}. Dependiendo del tipo de problema, la función de
evaluación puede buscar maximizar o minimizar ciertas variables, satisfacer
restricciones específicas o lograr un equilibrio entre múltiples criterios
. Es fundamental diseñar una función de evaluación adecuada que capture las
características esenciales del problema y que permita seleccionar y guiar de
manera efectiva la búsqueda hacia soluciones de alta calidad. Una función de evaluación bien
definida puede influir en la eficiencia y efectividad de los algoritmos de
búsqueda, ya que determina qué soluciones son consideradas prometedoras y
merecen ser exploradas en mayor detalle \pp{van como 5
definiciones distintas de función de fitness, ninguna muy acertada}. Sobre las funciones de evaluacion que hemos implementado lo explicaremos en la seccion \ref{sec:fitness}


Los métodos observadores son aquellos que no modifican el estado de un objeto al
ejecutarse. En el ejemplo motivador presentado en la sección
\ref{sec:motivacion} y la tabla \ref{tab:ncl-api}, se puede observar que existen
muchos métodos clasificados como observadores. Esto nos indica que estos métodos
no son relevantes para ser considerados como métodos builders. Antes de ejecutar
cualquiera de nuestros algoritmos, aplicamos una técnica de análisis estático
para eliminar de la búsqueda aquellos métodos que son irrelevantes para nuestros
métodos builders \pp{castellano}. En otras palabras, necesitamos identificar y eliminar aquellos métodos que son considerados observadores en la API.
Para realizar esta identificación de métodos observadores, utilizamos una
herramienta de análisis estático llamada
\emph{Infer}\footnote{https://fbinfer.com/}. Esta herramienta ha sido
desarrollada por \emph{Facebook} y ofrece varias funcionalidades. En el contexto
de nuestra tesis, utilizamos únicamente su analizador estático para detectar qué
métodos son considerados puros (observadores) \pp{falta cita a paper de
métodos puros}.

El análisis estático realizado con \emph{Infer} nos permite obtener una lista de
métodos que son clasificados como observadores en la API bajo prueba. Esta
información es utilizada como paso previo a la ejecución de nuestros algoritmos,
para asegurarnos de que solo consideramos los métodos relevantes y adecuados
para comenzar la búsqueda de los métodos \emph{builders}. \pp{este párrafo es
una repetición de lo que dice el anterior}

\subsection{Algoritmo Genético}
\label{alg:approachGA}

\pp{hay varios errores técnicos en esta sección que podemos charlar. Hay que
    mirar la explicación de algún libro y tratar de mejorar las explicaciones}

En esta sección presentamos los detalles para la detección de los subconjuntos
de métodos builders utilizando un algoritmo evolutivo. Para lograr realizar
esto, implementamos un algoritmo genético (\ref{sec:geneticoPrev}) que busca el
subconjunto de métodos que sean mínimo y suficientes \pp{constructores?}, que describiremos a continuación.


% % basados en una estrategia de escalada de colinas \cite{Russell:2009}.
% Durante esta sección explicaré en detalle cada algoritmo.


% Dare mas detalles de este algoritmo en la seccion {TODO!!!!}.


\subsubsection{Cromosomas}
\label{ge:cromosomas}

Los elementos de la población de nuestro algoritmo genético son subconjuntos de métodos de la API. Para representar esto, necesitamos codificarlos como \emph{cromosomas}. En nuestro caso, necesitamos alguna forma de representar los métodos de la clase como un vector. Por ejemplo, consideremos el ejemplo de NCL explicado en el capítulo anterior, que tiene 34 métodos (consultar Tabla \ref{tab:ncl-api}). Para representar nuestras posibles soluciones, crearemos un vector de genes booleanos, donde cada posición $i$ es verdadera si y solo si el cromosoma contiene el $i$-ésimo método de la API. Para hacer esto, enumeraremos los métodos de la API desde 0 hasta $n$, donde $n$ es el número total de métodos en la API. Cabe destacar que cada cromosoma tendrá la misma longitud, lo cual es común en los algoritmos genéticos.

Para el ejemplo de NCL, que tiene 34 métodos (Tabla \ref{tab:ncl-api}), tendríamos la siguiente representación del cromosoma:

\begin{center}
$c = \begin{array}{@{}cccccc@{}}
\scriptsize g_0 & \scriptsize g_1 & \scriptsize g_2 & \ldots & \scriptsize g_{34}
\end{array}$
\end{center}
\pp{este ejemplo quedó de más con el de abajo me parece, no aclara mucho así
como está}

Aquí, cada gen (\emph{g}) es un valor booleano que indica la presencia o
ausencia del método en ese cromosoma. \pp{está dicho arriba esto}

Como ejemplo más concreto, consideremos la representación del cromosoma que representa el conjunto de métodos suficientes y mínimos para NCL, como se muestra en la Figura \ref{fig:NCLbuilders}:
\begin{center}
$c = \begin{array}{|*{8}{c|}}
\hline
0 & 1 & \ldots & 7 & \ldots & 25 & \ldots & 33 \\
\hline
1 & 0 & \ldots & 1 & \ldots & 1 & \ldots & 0 \\
\hline
\end{array}$
\end{center}
\pp{los índices no son parte del cromosoma y deberían ir abajo, como está el dibujo parece una
matriz, y no coincide el formato con el dibujo anterior}

En este caso, las posiciones 0, 7 y 25 están establecidas como verdaderas (según el orden asignado a los métodos de la API), mientras que las demás posiciones están establecidas como falsas.


\subsubsection{Poblacion Inicial \pp{por qué solo la inicial?}}

El proceso comienza con un conjunto de individuos llamado Población. Recordemos
que cada individuo es una solución \pp{los individuos no son soluciones} al problema que deseas resolver.

En nuestro algoritmo genético, la población inicial se crea generando
aleatoriamente un conjunto de cromosomas, donde cada cromosoma representa una
solución potencial al problema \pp{los individuos no son soluciones}. Cada gen
en el cromosoma se establece aleatoriamente como 1 o 0, lo que indica la
presencia o ausencia del método que se codificó en esa posición \pp{repetición}.
Además, todos nuestros cromosomas iniciales tienen un tamaño igual a la cantidad
de métodos que tiene la API bajo test \pp{repetición}.

Es importante destacar que el número de cromosomas en la población inicial
desempeña un papel fundamental en el algoritmo genético \pp{solo en la
inicial?}, ya que proporciona la base para la evolución y mejora gradual de las
soluciones\pp{no son soluciones} a lo largo de las generaciones. En nuestro algoritmo, tenemos una
población inicial de 100 individuos \pp{por qué 100? No se debería buscar
experimentalmente?}. A medida que el algoritmo avanza, se
aplican operadores genéticos como la selección, el cruce \pp{mala traducción} y
la mutación para crear nuevas soluciones \pp{no son soluciones} a partir de la población actual. A continuación, se explican estos operadores en detalle.

\subsubsection{Operadores Geneticos \pp{Noté algunos errores ortográficos, usar un spell checker}}

A continuación explicaremos los operadores genéticos principales utilizados en
el algoritmo evolutivo. Cada uno desempeña un papel importante en la exploración
y explotación \pp{explotación?} del espacio de búsqueda y en la mejora de la
calidad de las soluciones \pp{no son soluciones}a lo largo de las generaciones
\pp{no definido}.

\paragraph{Cross-over (Cruce\pp{mala traducción})}

El cruzamiento (o \emph{cross-over} en inglés) es un operador genético que
combina una parte de dos cromosomas $c1$ y $c2$ elegidos aleatoriamente para
crear un nuevo cromosoma descendiente \pp{no se eligen aletoriamente, se eligen
con el procedimiento de selección. Selección debería explicarse antes}. Esa parte que toma de cada cromosoma padre
depende de la configuración con la que se lo setee. En nuestro algoritmo,
utilizamos un ratio de 0.30\pp{No se debería buscar
experimentalmente? Hay una mezcla entre la definición de algoritmos genéticos
en general y la implementación particular}. Además, un cruzamiento de dos puntos \pp{no
definido} de cruce dentro de los cromosomas de los padres. Su objetivo principal es explorar y explotar la información genética existente en la población.
El cruce de varios puntos permite una mayor variedad en los descendientes
generados, ya que se intercambian segmentos más largos de genes entre los
padres. Esto puede ayudar a explorar el espacio de búsqueda de soluciones de
manera más efectiva y encontrar soluciones potencialmente mejores \pp{errores
técnicos}.
\pp{un ejemplo hubiera estado bueno}

\paragraph{Mutacion\pp{spell checker}}

La mutación es un operador fundamental en un algoritmo genético que introduce
pequeños cambios aleatorios en los cromosomas para explorar nuevas soluciones
\pp{no son soluciones} y mantener la diversidad dentro de la población. Juega un
papel crucial en la prevención de la convergencia prematura \pp{no me convence}
y garantiza que el algoritmo explore de manera efectiva el espacio de búsqueda.
Existen varios operadores de mutación que se pueden utilizar en un algoritmo
genético. Uno de los operadores de mutación más comúnmente utilizados es la
Mutación por Inversión de Bits \pp{por qué mayúsculas?}, que es adecuada para algoritmos genéticos codificados en binario como en el caso de nuestra codificación.
En la Mutación por Inversión de Bits, se selecciona uno o más bits aleatorios en
el cromosoma y se invierten. Si un bit está inicialmente establecido en 1, se
cambia a 0, y viceversa. Esta alteración aleatoria introduce cambios sutiles en
la información del cromosoma, creando potencialmente nuevas soluciones \pp{no son soluciones}que no
estaban presentes en la población original. El número de bits invertidos y sus
posiciones se determinan típicamente mediante una baja probabilidad de mutación
\pp{en realidad se hace experimentalmente}.
\pp{un ejemplo hubiera estado bueno}


\paragraph{Selección}
La operación de selección es un componente clave en nuestro algoritmo genético,
ya que determina qué individuos serán preservados en la siguiente generación
\pp{error técnico}. En nuestro enfoque, hemos desarrollado un operador de
selección que se basa en el nivel de aptitud de cada individuo \pp{no
desarrollamos ningún operador nuevo}.

Utilizamos un enfoque de selección tipo torneo (\emph{tournament}) con un tamaño
de torneo de 4 en nuestro algoritmo. En este enfoque, se selecciona
aleatoriamente un grupo de individuos de la población y se \pp{los pone a
competir} compite entre ellos. El individuo con el mejor valor de fitness en el
torneo es seleccionado para formar parte de la próxima generación \pp{error técnico}. Este proceso se repite hasta que se hayan seleccionado todos los individuos necesarios\pp{error técnico}.

Es importante destacar que el operador de selección basado en torneo tiene varias ventajas. No solo permite seleccionar a los individuos más aptos\pp{error técnico}, sino que también introduce una presión de selección ajustable. Esto significa que los individuos más débiles tienen menos posibilidades de ser seleccionados en torneos más grandes, lo que ayuda a mejorar gradualmente la calidad de la población a lo largo de las generaciones.


El siguiente es el algoritmo genético que utilizamos para nuestra búsqueda
\\
\cacho{Algortimo? esta en preliminares, es standard para Genetico. Pongo los valores que utilizamos en cada parametros aca? Lo tengo en evaluacion.}
\\
%Pongo ejemplo de codigo???%
\begin{algorithm}[H]

\SetAlgoLined
\KwResult{Best solution}
Inicializar población inicial $P$ de cromosomas aleatorios\;

Evaluar la aptitud de cada cromosoma en $P$\;

\While{Criterio de terminación no se cumple}{ 
    Seleccionar cromosomas padres de $P$ para la reproducción\;
    Aplicar operador de cruce para generar descendencia\;
    Aplicar operador de mutación a la descendencia\;
    Evaluar la aptitud de los nuevos cromosomas\;
    % Reemplazar cromosomas menos aptos en $P$ por la descendencia\;
}
\caption{Algoritmo Genético}
\end{algorithm}
\pp{se queda muy corto este pseudocódigo, no explica casi nada.}
\pp{En este punto todavía no se sabe cuál es la función de fitness. No se
entiende que hace el algoritmo en el contexto de identificación de builders.}
\cacho{Explicar algortimo}

\subsection{Hill Climbing}
\label{alg:approachHC}
El algoritmo Hill Climbing, también conocido como búsqueda por ascenso de
colina\cite{Russell:2009}, es un algoritmo de búsqueda local que se utiliza para
optimizar una función de valoracion. El objetivo del algoritmo es encontrar la
solución óptima \pp{no es óptima} para un problema determinado.  Su enfoque se
basa en realizar movimientos ascendentes, es decir, buscar soluciones que
mejoren continuamente el valor objetivo o la función de evaluación \pp{o?}.
Este algoritmo es un algoritmo Greedy(Perezoso) explicado en la seccion de
preliminares, (\ref{sec:greedyPrev})\pp{no hay mucho que explicar en
preliminares sobre hill climbing, ya está todo acá por lo que veo}. Este selecciona un buen estado sucesor
para el momento actual, sin pensar en dónde ir a continuación \pp{no se entiende}. 
La idea principal detrás del algoritmo Hill Climbing es comenzar con una solución inicial y, en cada iteración, realizar un movimiento hacia una solución vecina que mejore el valor objetivo. Este proceso se repite hasta que no se pueda encontrar una solución vecina que mejore el valor actual. En ese punto, el algoritmo se detiene y devuelve la mejor solución encontrada hasta el momento.
Es importante destacar que el algoritmo Hill Climbing puede quedar atrapado en óptimos locales, es decir, puede converger hacia soluciones que son mejores en comparación con sus vecinos inmediatos, pero no son óptimas en el contexto global

La representación que hemos utilizado del problema fue igual a la que utilizamos para representar cromosomas en el Algoritmo genético explicado en la sección anterior. Esto quiere decir que utilizamos vectores de 0 y 1 para representar una posible solución. 

A continuación mostraremos un pseudocódigo del algoritmo \emph{Hill Climbing} que hemos implementado:
\pp{En este punto no se sabe cuál es la función de fitness, entonces no se
entiende que significa el algoritmo en el contexto de identificación de builders.}

\begin{algorithm}[H]
  \caption{Algoritmo de Hill Climbing}
  \label{algo:hill_climbing}
  \SetAlgoLined
  \KwResult{Solución óptima $curr$}
  $curr \gets c$\; 
  
  \While{existe un mejor candidato}{
    $S \gets$ GenerarSucesores($curr$)\;
    
    $best \gets$ SeleccionarMejorSucesor($S$)\;
    
    \If{$f(best) > f(curr)$}{
      $curr \gets best$\;
    }
  }
  \Return{$best$}\;
\end{algorithm}

Este algoritmo representa el esquema básico de Hill Climbing, comienza
calculando la funcion de valoración de todos los singletones ${c}$ de métodos
constructores.  El mejor de los singletones (mayor objectos puedo crear con ese
constructor) se establece como el candidato actual $curr$, y Hill Climbing
inicia un proceso de búsqueda típico e iterativo (Línea 1) \pp{Esto que se
explica acá no está en el algoritmo, hay una asignación nomás}.

En cada iteración (Línea 2 a 8), \emph{Hill Climbing} calcula $f(succ)$ para
cada succ $\in$ $GenerarSucesores(curr)$. El método que genera los sucesores nos
devuelve un conjunto de posibles soluciones que se crean a partir de sumarle un
método a la solución óptima corriente ($curr$). Es decir, los sucesores  $S$
generados con \emph{GenerarSucesores(curr)} de un candidato $curr$ son los
conjuntos {$curr\cup{mi}$}, para cada $mi$ $\in$ API (Línea 3) \pp{
Se podría poner el pseudocódigo de generar sucesores, y quizás poner un ejemplo
para que se entienda}.
Sea $best$ el sucesor con el valor de valoración más alto. Observe que $best$
tiene exactamente un método más que el mejor candidato de la iteración anterior,
$curr$ (Línea 4) \pp{hay que implementar con código el existe un mejor
candidato. Mirar el libro de AI.}.

Si $f(best) > f(curr)$, los métodos en $best$ se pueden utilizar para crear un
conjunto más grande de estructuras que los en $curr$ (Línea 5 a 7) \pp{f no está
definida}. Por lo tanto, \emph{Hill Climbing} asigna $best$ a $cur$r y continúa con la siguiente iteración. En cambio, si $f(best) <= f(curr)$, $curr$ ya genera el conjunto más grande de estructuras posible (no se puede agregar ningún método que aumente el número de estructuras generadas a partir de $curr$). En este punto, $curr$ se devuelve como el conjunto de constructores identificados. (Linea 9) 

% \cacho{
% En este algoritmo no se necesitan setear ningun parametros para guiar la busqueda a que sea mas efectiva}

Se puede observar que este algoritmo puede quedar atrapado en un máximo local y
no generar alguna combinación específica que podría ser aún mejor. Aquí se puede
ver que es un algoritmo greedy, ya que obtiene una solución rápida pero puede no
ser eficiente \pp{eficaz?} en encontrar la mejor solución global. 


\subsection{Clases de equivalencia}
\label{alg:approachCE}
 Las clases de equivalencia son una técnica utilizada para agrupar conjuntos de datos de entrada en categorías o clases que tienen un comportamiento similar o producen resultados equivalentes. Esta técnica es ampliamente utilizada en el diseño y la realización de pruebas de software.
 En términos generales, una clase de equivalencia representa  un conjunto de que se espera que produzcan resultados idénticos. La idea es que si un conjunto de la clase de equivalencia produce un resultado entonces todas las demás entradas de esa clase deberían producir el mismo resultado.
 Al trabajar con clases de equivalencia, se selecciona una entrada representativa, llamada caso de prueba, de cada clase para ser evaluada. En lugar de probar todas las posibles entradas, se eligen casos de prueba que representen cada clase de equivalencia para minimizar la cantidad total de pruebas necesarias.

 Bajo esta introducción, hemos desarrollado un tercer algoritmo donde agrupamos en clases de equivalencia aquellos subconjuntos de métodos que tengan el mismo valor de valoración con alguna de las funciones explicadas en \ref{sec:fitness}.


\begin{algorithm}[H]
  \caption{Algoritmo basado en Clases de Equivalencia}
  \label{algo:clases_equivalencia}
  \SetAlgoLined
  \KwResult{Conjunto de metodos builders $best$}
  $curr \gets c$\; 
  $equivalenceClasses \gets$ CrearClasesDeEquivalencia($curr$)\;
  
  \While{se ha creado una nueva clase de equivalencia}{
    $newCandidates \gets$ CandidatosPorClase($equivalenceClasses$)\;
    
    \ForEach{$candidate$ en $newCandidates$}{
      $successors \gets$ GenerarSucesores($candidate$)\;   
      
      \ForEach{$successor$ en $successors$}{
        $key \gets$ f($successor$)\;
        $equivalenceClasses[key]$.put($successor$)\;
      }
    }
        % $best \gets$ SeleccionarMejorCandidato($candidates$)\;
  }
  
  $result \gets$ obtenerMejor($best$)\;
  \Return{$result$}\;
\end{algorithm}

En este algoritmo, se comienza obteniendo los conjuntos singletons (Línea 1), que son los métodos constructores individuales de la misma manera que el algoritmo de \ref{algo:hill_climbing}. A partir de estos singletons, se selecciona el mejor candidato inicial $curr$, que servirá como punto de partida. Luego, se crean las clases de equivalencia basadas en la función de valoración y agrupando los candidatos que tienen el mismo valor de valoración. En el algortimo las clases de equivlencias estan guardadas en $equivalenceClasses$ (Línea 2).

A continuación, se itera (Línea 5 a 12) mientras se haya creado una nueva clase de equivalencia. En cada iteración, se generan nuevos candidatos por cada clase de equivalencia. Es decir, se selecciona un candidato de cada clase de equivalencia (el de menor métodos y menor parámetros) $CandidatosPorClase(equivalenceClasses)$ y se lo guarda en un cola ($newCandidates$) (Línea 5). A continuación, se generan los sucesores (línea 6) para cada candidato elegido, $GenerarSucesores(candidate)$, utilizando el mismo enfoque que en el algoritmo de \emph{Hill Climbing}. Estos se guardan en $successors$ Es decir, si tenemos $N$ clases de equivalencias vamos a tener $N$ candidatos a los cuales le vamos a calcular sus sucesores.  Luego, por cada uno de estos sucesores en  se le calcula su función de valoración y de acuerdo a esta se lo guarda en la clase de equivalencia representada por el valor de valoración.
El algoritmo itera hasta que no hayamos creado una nueva clase de equivalencia. Esto quiere decir que no hay cambios y hemos explorado todas las alternativas posibles. 
Luego vamos a obtener el mejor conjunto de métodos builders, accediendo a la clase de equivalencia con mayor key y obteniendo el representante de ese conjunto que tenga menor métodos y parámetros. Esto lo realiza el método $obtenerMejor(equivalenceClasses)$



\section{Función de Valoración}
\label{sec:fitness}


En la búsqueda de soluciones óptimas mediante algoritmos genéticos u otros algoritmos de búsqueda, la función de valoración \cite{goldberg1989genetic} desempeña un papel fundamental. Su diseño adecuado es crucial, ya que determina la dirección y el éxito del proceso de búsqueda.
La función de valoración evalúa la calidad de cada solución candidata y la compara con otras soluciones en la población. Proporciona una medida objetiva de la idoneidad de cada individuo en relación con los objetivos y restricciones del problema. La calidad se expresa generalmente mediante un valor numérico, donde valores más altos indican soluciones más deseables. Por ejemplo, si estamos resolviendo un problema de optimización en el que buscamos maximizar una función objetivo, la función de valoración puede asignar un valor más alto a las soluciones que se acercan más a la solución óptima. Por otro lado, si estamos resolviendo un problema de minimización, la función de valoración puede asignar un valor más alto a las soluciones que se alejan más de la solución óptima.
Definir una buena función de valoración implica considerar cuidadosamente los requisitos y características del problema en cuestión. Puede implicar ponderar diferentes objetivos, como maximizar o minimizar ciertas variables, satisfacer restricciones específicas o lograr un equilibrio entre múltiples criterios. 

Una función de valoración efectiva debe ser capaz de discriminar entre soluciones prometedoras y soluciones subóptimas. Debe proporcionar una evaluación objetiva y precisa, permitiendo la selección de soluciones de alta calidad y la eliminación de soluciones menos deseables.
Además, la función de valoración debe ser adecuada para el dominio y el tipo de problema abordado. Puede requerir conocimiento especializado y experiencia en el campo para definir correctamente las métricas y ponderaciones apropiadas. 

Es importante tener en cuenta que la función de valoración puede evolucionar y adaptarse a lo largo del proceso de búsqueda. A medida que el algoritmo de busqueda avanza y produce nuevas generaciones de soluciones, la función de valoración puede ser ajustada y refinada para enfocarse en las características más relevantes y deseadas.


Por lo tanto, en los algoritmos de búsqueda la función de valoración desempeña un papel crucial para evaluar y comparar soluciones candidatas en función de criterios específicos del problema. Proporciona una medida cuantitativa de la calidad de las soluciones y guía el proceso de selección y toma de decisiones en cada etapa del algoritmo. El diseño adecuado de la función de valoración es esencial para obtener resultados óptimos y eficientes en la resolución de problemas.
A continuación, explicaremos cada función de valoración implementada en nuestros algoritmos.


% En el caso de algoritmos greedy, la función de valoración juega un papel similar pero con un enfoque más local. En lugar de trabajar con una población de soluciones, los algoritmos greedy toman decisiones en cada paso basándose en una evaluación local de las opciones disponibles. La función de valoración en un algoritmo greedy se utiliza para seleccionar la mejor opción en cada iteración o paso del algoritmo, maximizando o minimizando la función de valoración según el objetivo del problema \cite{cormen2009introduction}.




\subsection{Fitness: Generador Exhaustivo}

Dado un candidato que representa un conjunto de métodos $M$, nuestra función de valoración intenta calcular una aproximación del número de objetos acotados que se pueden construir utilizando combinaciones de métodos habilitados en el cromosoma. Los candidatos con valores de aptitud más altos se estima que construyen más objetos que aquellos que tienen valores de aptitud más pequeños.
Nuestra función de valoración tiene como objetivo principal maximizar la cantidad de objetos que puedo generar utilizando el Generador exhaustivo que hemos desarrollado (explicado en el capítulo anterior,  \ref{cap:beapi}. Este enfoque nos permite evaluar la capacidad de los candidatos para construir una variedad de objetos y proporciona una base para la selección y mejora de los mejores candidatos en nuestro algoritmo.

Esta función de valoración en esta tesis se utiliza para evaluar la calidad de un candidato, el cual representa un conjunto de métodos $M$. Para cada $M$ , nuestra función devuelve un valor real que se construye de la siguiente manera:
{\small
\[
f(M) = \text{{\#Objectos}}(M) \ , \ (\text{{\#MétodosAPI}} - \text{{\#M}}) + w_1 \times \text{{PP}}(M)
\]
}

La función de valoración se compone de tres componentes. En primer lugar, $\text{{\#Objetos}}(M)$ representa el número de objetos generados por el conjunto de métodos $M$. La coma en la fórmula indica que este valor es independiente de los otros componentes, y se le da prioridad a la cantidad de objetos creados con el subconjunto de métodos $M$.

El segundo componente, $(\text{{\#MétodosAPI}} - \text{{\#M}})$, refleja la diferencia entre el número total de métodos disponibles en la API y el número de métodos en el conjunto $M$. Este término se utiliza para desempatar en caso de que otro subconjunto de métodos $M_1$ construya la misma cantidad de objetos que $M$.

\begin{lstlisting}[label=fig:NCLeqbuilders2, caption=Sufficient and minimal builders for NCL with more complex parameters than the ones in Figure \ref{fig:NCLbuilders}, captionpos=b, frame=tb, float=t]
  (0)  NodeCachingLinkedList()
  (4)  add(int,Object)
  (23)remove(Object)
\end{lstlisting}

Finalmente, $\text{{PP}}(M)$ es un factor de ponderación que permite ajustar la importancia de tener menor complejidad y cantidad de parámetros en el conjunto de métodos $M$. Este valor es entre 1 y 0 es solo para ajustar en caso de que haya empate de cantidad de objetos y métodos.
Los métodos con más parámetros o parámetros con tipos más complejos requieren más esfuerzo para generar entradas útiles, lo que los hace más exigentes para el análisis del programa. Por lo tanto, definimos un criterio de parámetro y adaptamos nuestra función de valoración para favorecer a los métodos con menos parámetros. Por ejemplo, ambos conjuntos de constructores en las Figuras \ref{fig:NCLbuilders} y \ref{fig:NCLeqbuilders2} son suficientes y mínimos (con 3 rutinas cada uno), pero el subconjunto de métodos en la Figura \ref{fig:NCLeqbuilders2} tienen más parámetros que deben ser instanciados. 

\begin{lstlisting}[label=fig:NCLeqbuilders2, caption=Sufficient and minimal builders for NCL with more complex parameters than the ones in Figure \ref{fig:NCLbuilders}, captionpos=b, frame=tb, float=t]
  (0)  NodeCachingLinkedList()
  (4)  add(int,Object)
  (23)remove(Object)
\end{lstlisting}

Al comparar las Figuras \ref{fig:NCLbuilders} y \ref{fig:NCLeqbuilders2}, podemos observar que \texttt{addFirst} ha sido reemplazado por \texttt{add}, que tiene un parámetro entero adicional, y que \texttt{removeFirst} se intercambió con \texttt{remove}, que tiene un parámetro no primitivo de tipo Object. Además, nuestro enfoque tiene en cuenta el número de parámetros y sus complejidades para seleccionar los constructores más adecuados. Hicimos un ranking de los tipos de parámetros más comunes para conocer su complejidad al instanciarlos.

\begin{lstlisting}[label=fig:rankParameters, caption=Ranking con los tipos de parametros, captionpos=b, frame=tb, float=t]
Boolean=1
Integer,Char,Object=2
Float,Double=3
String=6
Collection=10
Other Types=16
\end{lstlisting}

Siguiendo el criterio explicado, nuestro algoritmo elige el set de la Figure \ref{fig:NCLbuilders} sobre la Figura~\ref{fig:NCLeqbuilders2}



Esta función de valoración nos proporciona una medida cuantitativa de la calidad del candidato representado por el conjunto de métodos $M$. Los candidatos con valores de función de valoración más altos se consideran de mayor calidad y son preferidos en los algoritmos genéticos y en los algoritmos greedys que explicaremos en la sección \ref{sec:algorithms}.
Idealmente, nos gustaría explorar todos los objetos factibles dentro de un límite pequeño $k$ que se pueden construir utilizando los métodos del cromosoma actual. En otras palabras, necesitamos un generador exhaustivo acotado para el conjunto de métodos $BE(M, k)$. El límite $k$ representa el número máximo de objetos que se pueden crear para cada clase (en la Figura \ref{fig:ncl-instances}, el número de nodos en los objetos NCL está acotado por $k=2$) y el número máximo de valores primitivos disponibles (por ejemplo, enteros del 0 a $k-1$).

Para este propósito, desarrollamos la herramienta BEAPI, que se discute con más detalle en la Capítulo \ref{cap:beapi}. En resumen, primero exploramos exhaustivamente todas las posibles combinaciones de secuencias de los métodos de $M$. Luego, utilizamos un conjunto fijo de valores primitivos (enteros del 0 a $k-1$) con los cuales probar nuestros métodos cuando requieren valores primitivos.

En segundo lugar, descartamos las secuencias de métodos que crean objetos con más de $k$ objetos (de cualquier tipo) para evitar construir objetos más grandes de lo necesario. Para lograr esto, canonicamos los objetos generados por la ejecución de cada secuencia y descartamos la secuencia si algún objeto tiene un índice igual o mayor que $k$.

En tercer lugar, ampliamos esta generación con coincidencia de estado. Esto se debe a que, en la generación de pruebas, a menudo hay muchas secuencias de pruebas que producen el mismo objeto. Por ejemplo, insertar en una colección y luego eliminar el mismo elemento resulta en muchos casos en exactamente la misma estructura antes de la inserción. Nuestro enfoque asume que las ejecuciones de rutinas son deterministas con respecto a sus entradas. Bajo esta suposición, se deduce que, para generar un conjunto exhaustivo acotado de estructuras, solo necesitamos guardar una secuencia de prueba para crear cada estructura diferente en el conjunto, y que todas las siguientes secuencias de prueba que generen la misma estructura se pueden descartar.

Nuestra justificación para usar conjuntos acotados de objetos es similar a la \emph{hipótesis de la cota pequeña} \cite{Andoni:2003}.Si un conjunto de métodos no puede usarse para construir objetos pequeños que permitan diferenciarlo de otro conjunto de métodos, es poco probable que estos dos conjuntos puedan distinguirse con objetos más grandes. Esta hipótesis se mantuvo durante nuestra evaluación empírica en todos nuestros casos de estudio.

Para obtener más información sobre el algoritmo y más detalles sobre BEAPI, invitamos al lector a consultar el Capítulo \ref{cap:beapi}.


\subsection{Fitness: Cobertura de ramas con Randoop}
\
En el contexto de nuestra tesis, hemos realizado modificaciones en la herramienta \emph{Randoop} descrita en la sección \ref{sec:feedback-directed-test-gen}.

Nuestras modificaciones se centran en dar prioridad a un subconjunto específico de métodos, representado en la sección anterior por el conjunto $M$. El objetivo de esta modificación es generar secuencias de pruebas que se enfoquen principalmente en la ejecución de estos métodos seleccionados. Sin embargo, es importante destacar que esto no implica excluir por completo otros métodos, ya que aún buscamos ejercitar la API en su totalidad para obtener una buena cobertura de todos los métodos.

La motivación detrás de esta modificación radica en que los subconjuntos de métodos que obtienen una mayor cobertura de la API nos proporcionarán indicios valiosos sobre la calidad de dicho subconjunto. Por lo tanto, nuestra función de valoración tiene como objetivo maximizar la cobertura de ramas de la API bajo prueba, tal como se describe en la sección de preliminares \ref{sec:coverage}.

La función de valoración que utilizamos para evaluar la calidad de un candidato, que representa un conjunto de métodos $M$, se construye de la siguiente manera:

{\small
\[
f(M) = \text{{\#CoberturaRama}}(M) \ , \ (\text{{\#MétodosAPI}} - \text{{\#M}}) + w_1 \times \text{{PP}}(M)
\]
}

La función de valoración consta de tres componentes principales. En primer lugar, $\text{{\#CoberturaRama}}(M)$ representa el número de ramas cubiertas por las suites de prueba generadas por Randoop utilizando el subconjunto de métodos $M$. Este componente refleja la cantidad de ramas cubiertas específicamente por este subconjunto priorizado de métodos.

El segundo componente, $\text{{\#MétodosAPI}} - \text{{\#M}}$, tiene en cuenta la cantidad de métodos restantes en la API que no están presentes en el subconjunto $M$. Este componente incentiva a que el conjunto de métodos sea menor.

Por último, penalizamos sobre los parámetros de cada método en el subconjunto como lo hacíamos en la sección anterior.

En resumen, nuestras modificaciones en Randoop nos permiten priorizar un subconjunto de métodos durante la generación de secuencias de pruebas, y nuestra función de valoración se encarga de maximizar la cobertura de ramas de la API. Estas adaptaciones nos proporcionan una mejor comprensión de la calidad y efectividad de los subconjuntos de métodos generados.

\cacho{Tengo que agregar un cierre.}
