%!TEX root = main.tex
\chapter[Evaluaci\'on]{Evaluaci\'on}
\label{cap:experimental}


En los capítulos \ref{cap:builders} y \ref{cap:beapi} presentamos enfoques para
identificar automáticamente un conjunto de métodos generadores de objetos, y una
técnica novedosa de generación exhaustiva acotada basada en la API. 
En este capítulo, realizamos una evaluación experimental de las técnicas
mencionadas. La sección \ref{sec:experimentalIdentificacion} analiza experimentalmente los
algoritmos de identificación de generadores de objetos, mientras que en la
sección \ref{sec:experimentalBeapi} se evalúa la generación exhaustiva acotada basada en la API.


\section{Algoritmos de identificación de métodos generadores de objetos}
\label{sec:experimentalIdentificacion}

En esta sección, evaluamos experimentalmente los enfoques presentados en el
Capítulo~\ref{cap:builders}. Analizaremos la eficiencia y precision de cada
algoritmo utilizando las funciones objetivo introducidas en la
Sección~\ref{sec:fitness}. Además, mostraremos cómo los métodos identificados
por nuestras técnicas pueden ser aprovechados en el contexto de herramientas
de verificación de software.

Con respecto a estas técnicas, las siguientes preguntas de investigación guían esta experimentación:

\begin{itemize}
\item \emph{RQ1}: ¿Qué tan eficientes son los algoritmos propuestos para
    identificar conjuntos de métodos generadores de objetos?
\item \emph{RQ2}: ¿Qué tan precisos son los algoritmos presentados para identificar métodos generadores de objetos?
\item \emph{RQ3}: ¿Cuál es el impacto de utilizar métodos generadores de objetos
    en el contexto del análisis automático de software?
\end{itemize}

\subsection{Configuraci\'on experimental}

\subsubsection{Casos de estudio.}


La evaluación se llevó a cabo sobre un conjunto de clases Java 
 que manipulan estructuras dinámicas complejas,
incluyendo: \verb"NCL" de Apache Commons Collections~\cite{apache};
\verb"BinaryTree", \verb"BinomialHeap" y \verb"FibonacciHeap", extraídos
de~\cite{Visser:2006}; y \verb"UnionFind", una implementación de conjuntos
disjuntos tomada de JGrapht~\cite{jgrapht}.
\cacho{
También se incluyeron componentes de proyectos reales de software, como 
\verb"Lits" de la implementación de Sat4j~\cite{sat4j}, utilizada previamente 
en~\cite{Loncaric:2018}. \verb"Lits" implementa la interfaz \verb"ILits" y 
administra el “vocabulario” interno del solver (variables y literales): traduce 
literales desde DIMACS a la representación interna, registra el nivel de decisión 
o propagación asociado a cada literal, y mantiene las listas de observación 
(\emph{watch lists}) usadas en la propagación booleana unitaria. En la práctica, 
esto equivale a guardar cuándo se asignó por última vez un literal y qué 
restricciones lo observan}

Por otro lado, se consideraron estructuras adicionales como \verb"Scheduler",
un planificador de procesos tomado del conjunto de benchmarks SIR~\cite{sir},
y varias colecciones del paquete estándar \verb"java.util" de Java%
\footnote{\url{https://docs.oracle.com/javase/8/docs/api/java/util/
package-summary.html}}, incluyendo \verb"TreeMap", \verb"TreeSet",
\verb"HashMap",\\
 \verb"HashSet" y \verb"LinkedList". 

\input{tables/groundTruth}

Para evaluar la precisión de nuestros algoritmos, se construyó manualmente un
conjunto de referencia o \emph{ground truth} que contiene los métodos
generadores considerados minimales y suficientes en cada caso de estudio. Esta
tarea implicó un análisis detallado y manual de cada API, lo que representó un
trabajo considerable, especialmente en los casos más complejos. La
Tabla~\ref{tab:groundTruth} resume este conjunto de referencia, indicando para
cada clase el número total de métodos en su API pública (\#API) y los métodos
identificados manualmente como generadores de objetos. Cabe destacar que en algunos
casos ciertos métodos pueden ser intercambiables (por ejemplo,
\texttt{addFirst} y \texttt{addLast} en \texttt{NCL}).


\subsubsection{Algoritmos evaluados.}

\cacho{NUEVO:}
El objetivo de esta sección es identificar automáticamente el subconjunto de métodos generadores de cada API. 
Modelamos la tarea como un problema de búsqueda sobre subconjuntos \(M \subseteq API\) y empleamos distintos algoritmos 
para explorar ese espacio. La calidad de cada candidato \(M\) se evalúa mediante funciones objetivo 
(\emph{fitness}) definidas en las secciones citadas.

\paragraph{Algoritmos}
\begin{itemize}
  \item \textbf{GA} (\emph{Algoritmo Genético}; véase Sección~\ref{alg:approachGA}): explora subconjuntos de métodos generadores mediante operadores de cruce y mutación, con selección por torneo.
  \item \textbf{HC} (\emph{Hill-Climbing}; \S~\ref{alg:approachHC}): búsqueda local que parte de una solución inicial y aplica modificaciones incrementales aceptando únicamente mejoras según la función objetivo.
\end{itemize}

\paragraph{Funciones objetivo}
Usamos dos funciones objetivo alternativas para evaluar cada conjunto candidato de métodos, 
que dan lugar a las variantes reportadas en las tablas:

\begin{itemize}
  \item \textbf{GE} (\emph{Generación Exhaustiva}; Sección~\ref{sec:fitnessGE}):
mide la cantidad/calidad de objetos distintos alcanzados mediante generación exhaustiva hasta un \emph{scope} fijado.
  \item \textbf{RC} (\emph{Randoop Coverage}; Sección~\ref{sec:fitnessRandoop}): maximiza la cobertura de código (líneas y ramas) alcanzada por los tests generados con Randoop restringidas al conjunto candidato. 
\end{itemize}

% \paragraph{Convenciones de nombres}
% Cuando un algoritmo se evalúa con una función objetivo específica, usamos la notación \textbf{Algoritmo–Fitness}. 
% Por ejemplo: \textbf{GA–GE}, \textbf{GA–RC}, \textbf{HC–GE}, \textbf{HC–RC}.

\pp{Falta describir brevemente los algoritmos evaluados, citando las secciones
correspondientes, y las funciones de fitness. Y darles los nombres abreviados
que después aparecen en las tablas.}

% Como se discutió en el Capítulo~\ref{cap:beapi}, nuestros algoritmos requieren
% la configuración de diversos parámetros que influyen en su eficiencia y
% efectividad. En particular, los parámetros más relevantes para el algoritmo
% genético incluyen la tasa de mutación, la tasa de cruce, el tamaño de la
% población y la estrategia de selección utilizada.

\cacho{Esto es nuevo}
Nuestros algoritmos requieren la configuración de diversos parámetros que influyen en su eficiencia y efectividad.
En particular, para el algoritmo genético (Sección~\ref{alg:approachGA}) utilizamos una tasa de cruce de 0{,}40, una tasa de mutación de 0{,}05, 
selección por torneo con cuatro participantes, una población de 100 individuos y 20 generaciones por ejecución. 
Estos valores se determinaron empíricamente mediante prueba y error, priorizando la eficiencia y la estabilidad de las soluciones y se mantuvieron constantes en todos los experimentos.

Para las evaluaciones con la función objetivo basada en Randoop (Sección~\ref{sec:fitnessRandoop}), 
fijamos un tiempo máximo de ejecución de 30 segundos por candidato 
y controlamos la aleatoriedad mediante semillas. Randoop utiliza esta semilla para controlar la generación de tests y
permitir la reproducción de resultados. Dado que Randoop introduce un grado
considerable de aleatoriedad, optamos por utilizar dos semillas distintas para capturar la variabilidad introducida por el 
generador aleatorio, registrándolas para permitir la reproducción de resultados. 
Esta configuración proporciona una medida más robusta del desempeño de los algoritmos en contextos con 
elementos no determin\'isticos.

En el caso de la función objetivo basada en generación exhaustiva (Sección~\ref{sec:fitnessGE}),
el parámetro central es el \emph{scope}, entendido como la cantidad máxima de objetos por clase/tipo que pueden crearse durante la evaluación.
En nuestros experimentos fijamos \(k=5\), es decir, un máximo de cinco objetos por tipo para cada conjunto candidato de métodos generadores. 
Además, para evitar diferencias espurias que no reflejan cambios semánticos en la representación, 
omitimos campos internos que no afectan la estructura lógica de los objetos generados (por ejemplo, \texttt{modCount}, común en colecciones de Java), 
el cual sólo contabiliza modificaciones internas sin alterar la estructura observada del objeto. \pp{Yo no pondría esto último porque no recuerdo que lo
hayamos explicado en ningún lado.} \cacho{Hay un apartado pequeño, lo podemos charlar}

Para cada combinación de caso de estudio y función objetivo, el algoritmo
genético y el de \emph{Hill Climbing} fue ejecutado cinco veces con los parámetros mencionados. Los
resultados reportados en las siguientes secciones corresponden al promedio de
estas ejecuciones. En el caso particular de la función objetivo basada en
generación exhaustiva, no fue necesario realizar múltiples repeticiones, dado
que dicha evaluación es determinística bajo una misma configuración.

Todos los experimentos fueron ejecutados en una máquina con procesador Intel
Core i7-6700 a 3.4\,GHz y 8\,GB de RAM, corriendo el sistema operativo
GNU/Linux.

% En términos de analizar el peor caso, con \textbf{HC+RC} se alcanza 5162s en
% \emph{TreeMap} (aprox 86 minutos) y 3842s en \emph{HashSet} para
% para \textbf{HC+GA} (aprox. 64 minutos). 
% Con \textbf{GE+GA}, se observa que el peor caso tarda 1803s (aprox 30 minutos) en \emph{HashMap} con \textbf{GE+RC} alcanza 113s para \emph{TreeMap} (aprox 2 minutos).

%Esto se debe a que los algoritmos
%de Hill Climbing comienzan desde abajo hacia arriba, considerando menos métodos antes de considerar más métodos, a diferencia de los 
%algoritmos genéticos, que generan sucesores mediante operadores de cruce y mutación. Esta estrategia favorece al algoritmo Hill Climbing 
%para encontrar los métodos mínimos en tiempos más cortos. Cabe mencionar que en casos de estudio donde el algoritmo genético supera al 
%Hill Climbing, se debe a que la clase bajo prueba tiene pocos métodos, lo que permite que los algoritmos evolutivos converjan más rápidamente que el algoritmo Hill Climbing.
%

Esto se aplica a todos los algoritmos implementados.
Sin embargo, la función objetivo basada en el algoritmo \emph{HC} no requiere que el usuario provea la definición del scope, y
podría ser más escalable que la generación exhaustiva acotada en casos donde
la cantidad de objetos a generar crezca exponencialmente respecto de los scopes 
(\cacho{ver si hay alguno}). Como trabajos futuros se planean realizar más experimentos 
para evaluar esta hipótesis.


\subsection{RQ1: Eficiencia del cómputo de generadores de objetos}
\cacho{Nuevo esto:}
Medimos la eficiencia como el tiempo promedio (en segundos) necesario para
finalizar la búsqueda de métodos generadores a partir de la API. Evaluamos los dos
algoritmos de búsqueda (\emph{GA} y \emph{HC}) combinados con dos funciones
objetivo: \emph{FGE} (Función objetivo con generación exhaustiva acotada) y \emph{FRC} (Función objetivo con cobertura). 
Los resultados se muestran en la Tabla~\ref{tab:eficiencia}. Cada fila
corresponde a un caso de estudio; debajo del nombre se indica el tamaño de la API
(\#API) y la cantidad de métodos generadores del \emph{ground truth}
(\#MétodosGen). Las columnas se agrupan por algoritmo (GA y HC) y, dentro de
cada uno, por función objetivo (FGE y FRC). Para cada configuración
(algoritmo~$\times$~objetivo) reportamos el promedio de 5 ejecuciones.


Para comparar la eficiencia de \emph{GA} y \emph{HC}, utilizamos la misma 
función objetivo (\emph{FGE} o \emph{FRC}) y medimos el tiempo de ejecución 
en los 12 casos de estudio. A partir de estos resultados obtuvimos los siguientes 
tiempos promedios:

\begin{itemize}
  \item \textbf{GA+FGE}: 259 segundos
  \item \textbf{HC+FGE}: 19.6 segundos
  \item \textbf{GA+FRC}: 1809.5 segundos
  \item \textbf{HC+FRC}: 622.8 segundos
\end{itemize}

Estos valores muestran con claridad que \emph{HC} supera sistemáticamente a 
\emph{GA}. En particular, bajo \emph{FGE} el tiempo de \emph{HC} es 
aproximadamente 13 veces menor que el de \emph{GA}, mientras que bajo 
\emph{FRC} es casi 3 veces menor.A nivel de casos
individuales, \emph{HC} supera a \emph{GA} en todos los casos con \emph{FGE} y en 11 de 12 casos con \emph{FRC},
siendo \emph{HashSet} la única excepción donde \emph{GA+FRC} fue más veloz. \cacho{Revisar esto, no me acuerdo bien, ver el paper}
Esta diferencia se entiende por la propia naturaleza de los algoritmos.
El algoritmo genético (\textbf{GA}) mantiene y evoluciona poblaciones completas de posibles subconjuntos, lo que implica evaluar muchos candidatos en paralelo.
En consecuencia, puede tardar más en encontrar el subconjunto minimal de métodos generadores, ya que su búsqueda es estocástica y no siempre converge rápidamente hacia la mejor solución.

En cambio, \emph{HC} sigue una estrategia incremental y determinista: parte de una solución inicial y la mejora paso a paso en el espacio de búsqueda. Esto lo vuelve más eficiente en recorrer dicho espacio y encontrar soluciones minimales con menor costo computacional. Por ello, en la mayoría de los casos, \textbf{HC} resulta más rápido que \textbf{GA}.
Comparando entre funciones objetivo dentro de un mismo algoritmo, \emph{FGE}
termina antes que \emph{FRC}. En \emph{GA}, FRC requiere en promedio
unas siete veces más tiempo que \emph{FGE}; en \emph{HC}, \emph{FRC} requiere unas casi 32 veces
más tiempo que FGE. La función objetivo FGE termina antes en los algoritmos debido a que genera objetos 
para un scope que debe ser especificado manualmente por el usuario. Todas tienen un \emph{timeout} de 
30 segundos \pp{esto va antes, cuando se presentan los algoritmos}, pero muchas veces
la generación exhaustiva acotada termina la generación de objetos antes del llegar al timeout. En cambio, el la función objetivo FRC debe agotar su presupuesto de tiempo
para cada candidato considerado.
Esto explica porque FGE es más rápida en comparación con la función objetivo FRC.

A diferencia de \emph{FGE}, como la función objetivo basada en cobertura con Randoop
(\emph{FRC}) no requiere que el usuario fije un \emph{scope} para enumerar
objetos, puede volverla más escalable en dominios donde la cantidad de
objetos válidos crece combinatoriamente con el scope. 
El costo de \emph{FGE} escala con la enumeración de todas
las instancias dentro del scope, mientras que \emph{FRC} evalúa cada candidato
mediante generación/ejecución de tests sin explorar exhaustivamente el
espacio de objetos, lo que puede resultar más económico para scopes grandes.

Esto respalda el uso de FGE cuando el objetivo es minimizar el tiempo de cómputo de los generadores.
Sin embargo, la función objetivo \emph{FRC} no requiere que el usuario provea la definición del scope, y
podría ser más escalable que la generación exhaustiva acotada en casos donde
la cantidad de objetos a generar crezca exponencialmente respecto de los scopes. Como trabajos futuros se planean realizar más experimentos 
para evaluar esta hipótesis.

\input{tables/eficienciaBuilders}

\subsection{RQ2: Precisión de los algoritmos de cómputo de generadores de objetos}
\label{sec:experimentalIdentificacionPrecision}

Medimos la efectividad como la capacidad de cada configuración para identificar
el \emph{conjunto minimal} de métodos generadores de la API (según el
\emph{ground truth} indicado como \#MetodosGeneradores en la tabla). Por construcción, en
todas las corridas nuestros algoritmos devuelven siempre un subconjunto
suficiente (nunca omiten un generador necesario); cuando el conjunto
devuelto coincide en tamaño con el minimal, lo consideramos exacto, y
cuando incluye métodos adicionales lo consideramos sobredimensionado.

La Tabla~\ref{tab:efectividad} resume los tamaños promedios (sobre 5
corridas) del conjunto hallado por cada configuración (GA/HC $\times$ FGE/FRC).
Se observan cuatro patrones claros. Primero, \textbf{HC+FRC} encuentra el conjunto
\emph{minimal exacto en 12/12} sujetos. Segundo, \textbf{HC+GE} es exacto en
\emph{11/12} sujetos y sólo sobredimensiona en \emph{Lits} (retorna 7 en lugar de
6).\cacho{Revisar esto, no me acuerdo bien, ver el paper}
Tercero, \textbf{GA+GE} es exacto en \emph{10/12} sujetos; sobredimensiona en
\emph{Lits} (7 vs.\ 6) y \emph{HashMap} (2.60 vs.\ 2). Cuarto, \textbf{GA+FGE} es
exacto en \emph{8/12} sujetos y tiende a incluir métodos adicionales en
\emph{NCL} (3.80 vs.\ 3), \emph{TreeMap} (5.60 vs.\ 3), \emph{TreeSet}
(3.20 vs.\ 3) y \emph{HashSet} (3 vs.\ 2).

Estas desviaciones tienen dos causas principales. Por un lado, en \textbf{GE} la
evaluación se realiza por enumeración acotada; bajo scope fijo y valores
iniciales concretos, ciertas combinaciones pueden no ser alcanzables sin
incorporar alguna rutina adicional (como ocurre en \emph{Lits}), lo que hace que
el conjunto devuelto sea suficiente pero no minimal. Por otro lado, en
\textbf{FGE} la métrica de cobertura puede favorecer métodos que
aumentan rápido la cobertura de ramas/líneas aunque no sean estrictamente
necesarios para construir todas las instancias, por lo que el algoritmo puede estabilizarse en soluciones
ligeramente sobredimensionadas.

En el caso particular del algoritmo genético (\textbf{GA}), además de la
estocasticidad propia de la búsqueda (selección, cruce, mutación), empleamos un
criterio de corte del algoritmo que detiene la evolución cuando el mejor
fitness no mejora durante 5 generaciones consecutivas (ver
Sección~\ref{alg:approachGA}). Este criterio es eficaz para acotar el presupuesto,
pero puede congelar soluciones suficientes levemente mayores que el
mínimo cuando la presión selectiva ya no impulsa mejoras adicionales.

En síntesis, todas las configuraciones producen conjuntos suficientes, y
\textbf{HC+FGE} además logra el mínimo exacto de forma sistemática. Cuando
el objetivo primario es minimizar el costo de generación/análisis, recomendamos
emplear el conjunto devuelto por \textbf{HC+FGE} (minimal y suficiente).

\input{tables/eficaciaBuilders.tex}

\subsection{RQ3: Impacto de los generadores de objetos en el análisis de
software}

\subsubsection{Cantidad de objectos generados en Randoop}
\cacho{Explico como usamos randoop? me parace que no viene al caso}

En esta sección evaluamos la utilidad práctica de los métodos generadores de objetos 
identificados previamente, en el contexto de la generación automatizada de casos de test. 
Estos métodos permiten crear instancias válidas de estructuras de datos, que pueden ser utilizadas como 
entradas en tests unitarios o tests basadas en propiedades.

Para aquellos casos de estudio cuyas clases permiten medir el tamaño de los objetos generados 
(mediante el método \texttt{size()}) y comparar objetos por igualdad (a través de \texttt{equals()}), 
realizamos un experimento con la herramienta \emph{Randoop} (Ver preliminares, \ref{sec:feedback-directed-test-gen}). Generamos casos de test en dos 
configuraciones distintas: (i) utilizando toda la API disponible de la clase bajo análisis 
(denominada \texttt{API}), y (ii) utilizando únicamente los métodos generadores de objetos
(\texttt{MGO}) identificados por nuestro enfoque en el experimento anterior 
(ver Tabla~\ref{tab:results-obj}).

Para ambas configuraciones, medimos la cantidad de objetos distintos 
generados (\emph{No. of Objs.}). Además, definimos tres valores de presupuesto de tiempo para la generación de 
test: 60, 120 y 180 segundos. Los resultados obtenidos se resumen en la
Tabla~\ref{tab:results-obj}.

En promedio global sobre las 39 combinaciones (clase X presupuesto),
\texttt{MGO} genera 6355 objetos frente a 1597 de
\texttt{API}, es decir, 4758 más de objetos en promedio, lo que significa un
incremento de 4 veces en cantidad de objetos (\(6355/1597=3.98\)). Desagregando por presupuesto:

En el presupuesto de \textbf{60 segundos}, la configuración con solo los 
métodos generadores produce en promedio 4783 objetos, mientras 
que la basada en toda la API alcanza apenas 1275. Esto implica 
que con métodos generadores se obtienen, aproximadamente, tres veces más objetos, que con toda la API,
lo que representa una diferencia de alrededor de 3500 objetos adicionales. 

Con un presupuesto de \textbf{120 segundos}, los métodos generadores 
alcanzan en promedio 6546 objetos, frente a 1623 con 
toda la API. En este caso, se generan aproximadamente cuatro veces más 
objetos con los \texttt{MGO} en comparación que con \texttt{API},
lo que equivale a una ventaja de más de 4900 objetos adicionales. 

Finalmente, para \textbf{180 segundos}, la configuración con métodos 
generadores llega a 7736 objetos, mientras que con toda la API 
se logran 1893. Nuevamente, la diferencia es notable, se 
construyen casi cinco veces más objetos con los \texttt{MGO} en comparación que con \texttt{API}, 
con un excedente cercano a los 5800 objetos adicionales.

La ventaja es consistente todos los casos. Si
promediamos por clase (sobre los tres presupuestos), \texttt{MGO} es mejor
en todas las clases, con ganancias especialmentes marcadas en \emph{NCL} (casi 15 veces mas en promedio por clase) y \emph{LinkedList}
(casi 10 veces). Esto sugiere que focalizar la generación sobre métodos generadores de objetos mejora la
capacidad de exploración de Randoop bajo un mismo presupuesto de tiempo, incrementando la diversidad de entradas disponibles para los tests.
Por lo tanto, concluimos que la identificación automatizada de métodos generadores de objetos 
constituye una estrategia efectiva y valiosa para potenciar la generación automatizada de tests.
La calidad de los objetos generados se evalúa más adelante en la Sección~\ref{sec:parametrizedTesting}, 
donde los utilizamos como entradas de tests parametrizados sobre las estructuras de datos. 

\input{tables/objectsBuilders.tex}

\subsubsection{Impacto en verificación formal con JPF}

En un segundo experimento, evaluamos el impacto del uso de métodos métodos generadores en la eficiencia 
del análisis de programas mediante verificación formal. Para ello, utilizamos \emph{Java PathFinder} 
(JPF)~\cite{Visser:2005}, explicado en la seccion \ref{sec:jpf}.
Este experimento lo evaluamos en los casos de estudio del paquete \texttt{java.util}.

El primer gran aporte a JPF se encuentra en la escritura de los controladores. Queda claro que programar el controlador
a partir de todos los métodos de la API es una tárea compleja que requiere
mas conocimento de la API bajo análisis como muestra el algoritmo \ref{lst:driverAPI}. Para
compensar esto, mostramos que el algoritmo que se utiliza como controlador en caso
de obtener previamente los métodos generadores es más sencillo y entendible para el usuario (Ver \ref{lst:driverBLD}).
\\
\begin{lstlisting}[language=Java,caption={Controlador con los métodos generadores de objectos},label={lst:driverBLD},captionpos=b]
    private static TreeMap generateStructure(int scope) {
       int maxLength = Verify.getInt(0, scope);
       TreeMap t = new TreeMap();
       for (int i = 1; i <= maxLength; i++) {
          switch (Verify.random(n_methods)) {
             case 0:
                t.put(Verify.getInt(0,scope),Verify.getInt(0,scope));
                break;
             case 1:
                t.remove(Verify.getInt(0,scope));
                break;						
          }
       }
       return t;
    }
\end{lstlisting}

Luego con este generador se puede verificar que el método \texttt{put} preserva el \emph{repOK} de la estructura de datos, como se muestra en ~\ref{lst:propiedadJPF}.

\begin{lstlisting}[caption={Probando el método put de TreeMap con JPF},label={lst:propiedadJPF},language=Java,captionpos=b]
    public static void main(String[] args) {
       int scope = 3;
       TreeMap t = generateStructure(scope);
       t.put(Verify.getInt(0,scope),Verify.getInt(0,scope));
       assert t.repOK();
    }
\end{lstlisting}

Los resultados de los experimentos, presentados en las Tablas~\ref{tab:results-jpf} y \ref{tab:results-jpf1}, demuestran que los controladores construidos a partir de nuestros 
métodos generadores permiten una mejora sustancial en eficiencia y escalabilidad. Se observa una reducción considerable en el tiempo de análisis y en la cantidad de estados explorados, sin 
comprometer la cobertura de las estructuras objetivo. Esta evidencia refuerza la utilidad práctica 
de nuestra técnica en entornos de verificación formal, donde el costo computacional de la exploración 
del espacio de estados es un factor crítico.

% Para ilustrar este mecanismo, ponemos bajo análisis el método \texttt{put} de \texttt{TreeMap}.
% La generación de entradas para probar este método puede realizarse mediante un controlador como el mostrado en la immplentación mostrada en ~\ref{lst:driverBLD}.
% Para mitigar el problema de la explosión combinatoria, propusimos utilizar únicamente los métodos generadores 
% detectados automáticamente por nuestro enfoque con el algoritmo de \emph{HC} y la funcion objetivo de \emph{FRC} (ver Sección~\ref{sec:experimentalIdentificacionPrecision}). 
% Como se muestra en el framento de codigo ~\ref{lst:driverBLD}, el controlador basado exclusivamente en los métodos generadores de objectos de 
% \texttt{TreeMap} selecciona sólo dos métodos, suficientes y minimal, para generar todas las instancias 
% válidas dentro del alcance definido. En consecuencia, JPF puede explorar las mismas configuraciones 
% de objetos que con la API completa, pero con un espacio de búsqueda significativamente más reducido.

% El patrón se mantiene de forma consistente a través de múltiples estructuras de datos de 
% \texttt{java.util}, incluyendo \texttt{HashMap}, \texttt{HashSet}, \texttt{LinkedList}, 
% \texttt{TreeMap} y \texttt{TreeSet}. Por ejemplo, en el análisis del método \texttt{remove} 
% en \texttt{HashMap}, al aumentar el \textit{scope} de 4 a 6, el número de estados explorados 
% con el controlador basado en la API completa crece de 627361 a más de 22 millones, con tiempos 
% de ejecución que alcanzan los 3011 segundos (aproximadamente 50 minutos). En cambio, utilizando únicamente los métodos generadores de objetos, 
% se recorren menos de 2.3 millones de estados, en un tiempo de análisis significativamente menor.

% Este mismo comportamiento se replica en otras estructuras. En el caso de \texttt{LinkedList}, 
% el uso de todos los métodos de la API lleva a una explosión de estados (más de 39 millones en 
% \textit{scope} 6), mientras que con los métodos generadores de objectos, el número de estados se reduce a aproximadamente 
% 1.1 millones para el mismo scope. En términos de tiempo, esta diferencia se traduce en 2771 segundos contra apenas 221. 
% Asimismo, para \texttt{TreeMap} con el método \texttt{put}, el uso de los métodos generadores de objectos en 
% \textit{scope} 5 permite mantener el análisis por debajo de los 3100 segundos, mientras que 
% la versión con todos los métodos alcanza el límite de tiempo (TO que es igual a 5000 segundos).

% Además de la reducción en el tiempo de verificación y en los estados explorados, la comparación 
% entre ambos controladores confirma que el conjunto reducido de métodos generadores de objectos es suficiente 
% para generar todas las configuraciones relevantes de entrada que permiten evaluar correctamente 
% las propiedades del programa. Esto implica que, en la práctica, los métodos adicionales disponibles 
% en la API no contribuyen a la diversidad estructural de las entradas, sino que solo introducen 
% complejidad innecesaria en el espacio de búsqueda.

% En resumen, estos resultados evidencian que la identificación automática de métodos generadores no 
% solo mejora la calidad de las entradas en contextos como la generación de tests, sino que también 
% es una estrategia efectiva para escalar técnicas de verificación formal a estructuras de datos más 
% complejas. Al eliminar métodos superfluos y reducir el espacio de búsqueda, se mejora significativamente 
% el rendimiento de herramientas como JPF, sin sacrificar precisión ni completitud en el análisis.

Para focalizar la comparación en los distintos casos, 
consideramos en cada clase–método todos los niveles de \emph{scope} que finalizan sin \emph{timeout}
De esta manera, los resultados reflejan el comportamiento completo de cada técnica en el rango alcanzable por \emph{JPF}. 
Cuando \textsf{API} alcanza \emph{timeout} en un determinado \emph{scope}, ese resultado indica un límite práctico 
de escalabilidad para dicha variante
% , mientras que \textsf{MGO} (Métodos generadores de Objetos) en general logra completar en el mismo \emph{scope}
Así, los casos con \emph{timeout} no se eliminan del análisis, 
sino que se interpretan como evidencia de la explosión combinatoria que introduce la API completa, 
en contraste con la reducción sustancial que se obtiene al restringirse al conjunto suficiente de métodos generadores.

En el análisis global sobre estas 30 combinaciones, el \emph{driver} con
\textsf{MGO} acumula 14070756 estados en 1591s, mientras que \textsf{API} explora 198130649 estados en 17,240 segundos,
En promedio por casos, \textsf{MGO} explora 234513 estados en 26.5 segundos, mientras que \textsf{API} 
explora 3302177 estados en 297.3 segundos. Esto implica que \textsf{MGO} reduce el espacio de estados en 
92.9\%\ y el tiempo en 91\%\ aproximadamente respecto de \textsf{API}. 
% Equivalentemente, \textsf{MGO} alcanza un \emph{speedup} de 11 veces y reduce el espacio de estados en 14 veces respecto de \textsf{API}.
Se observa tambien, que bajo el mismo \emph{scope}, \textsf{MGO} completa en todos los casos, mientras que \textsf{API} alcanza \emph{timeout} en 9/30 combinaciones (30\%).
Bajo el mismo \emph{scope}, \textsf{API} explora aproximadamente 1308\%\ mas de estados y tarda 984\%\ mas de tiempo que \textsf{MGO}.
Este patrón es consistente en todas las clases.
En cuanto a la interpretación, los resultados indican que construir \emph{drivers}
exclusivamente con los métodos generadores identificados por nuestro enfoque,
poda de manera efectiva el factor de ramificación en los puntos de elección
no deterministas del verificador (ej., \texttt{Verify.random}), lo que reduce
tanto el número de estados alcanzables como el costo de retroceso durante la
exploración. Al mismo tiempo, la suficiencia del conjunto de \textsf{MGO}
garantiza que se preserva la capacidad de construir todas las instancias
(acotadas por el \emph{scope}) y de poder escalar a \emph{scope} mas altos que pueden ser 
relevantes para la verificación de las propiedades,
sin sacrificar cobertura estructural dentro del alcance definido.

\input{tables/tableJPFBuilders.tex}
\input{tables/tableJPFBuilders1.tex}

\section{Generación exhaustiva acotada basada en la API}
\label{sec:experimentalBeapi}

\cacho{Esto estaba en la seccion de beapi, tengo que verlo y acomodarlo aca}
\pp{De acá hasta el final de esta sección son resultados que se deberían
discutir en la parte de la sección experimental que tiene los resultados de builders} 

En la mayoría de nuestros casos de estudio, los métodos generadores de objetos se limitan a un 
constructor y un único método para agregar elementos a la estructura, como ocurre en las listas 
simplemente enlazadas.  

Sin embargo, nuestro enfoque automatizado de identificación de métodos generadores evidenció que, 
en el caso de los árboles Rojo–Negro, también es necesario incluir un método de eliminación 
(para alcances mayores que tres).  
Esto se debe a que este tipo de árboles requiere configuraciones específicas de equilibrio, 
que involucran el coloreo de nodos en rojo y negro, las cuales no pueden obtenerse únicamente 
mediante operaciones de inserción.

En contraste, los árboles AVL,
que también son estructuras balanceadas, no requieren el método de eliminación como parte de su subconjunto de métodos generadores.
Los árboles AVL están siempre equilibrados, 
de tal modo que para todos los nodos, la altura de la rama izquierda no debe diferir en más de una unidad de la altura de la rama derecha o viceversa. 
Gracias a esta forma de equilibrio (o balanceo), la complejidad de una búsqueda en uno de estos árboles se mantiene siempre en orden de complejidad O(log n). 
Para este caso de árboles, solo el constructor de la clase y una rutina de adición son suficientes. 
Esto demuestra que la identificación de métodos generadores no es trivial de realizar manualmente.




En la evaluación de BEAPI buscaremos responder a las siguientes preguntas de
investigación:

\begin{itemize}
\item \emph{RQ4}: ¿Qué tan eficiente es BEAPI para la generación exhaustiva                                                                                       
    acotada de objetos?
\item\emph{RQ5}: ¿Cuál es el impacto de las optimizaciones propuestas en el
    rendimiento de BEAPI?
\item\emph{RQ6}: ¿Puede BEAPI ayudar a encontrar discrepancias entre
    especificaciones de invariantes de clase (repOK) y la capacidad de generación de objetos de la API?
\item\emph{RQ7}: ¿Qué tan efectivos son los objetos producidos por BEAPI para el
    testing parametrizado?  
\end{itemize}

\subsection{Configuración experimental}

Como estudios de caso, utilizamos implementaciones de estructuras de datos tomadas de cuatro bancos 
de pruebas reconocidos: \textsf{Korat}~\cite{Boyapati02}, \textsf{Kiasan}~\cite{Deng06}, 
\textsf{FAJITA}~\cite{Abad13}, \textsf{ROOPS} \cacho{VER CITA}y un conjunto de clases de proyectos reales de software que la denominamos \textsf{realWorld}
donde incluyen clases de \emph{java.util} (TreeMap, TreeSet, LinkedList, Hasmap), de \emph{Apache Commons Collections} (NodeCachingLinkedList) y un Scheduler de SIR

Estos bancos cubren una amplia variedad de estructuras de datos complejas, tales como listas doblemente enlazadas, árboles binarios de búsqueda, 
árboles rojo-negro, montículos binomiales, heaps de Fibonacci, entre otras. Una característica clave 
de estos bancos es que incluyen especificaciones explícitas de invariante de clase (métodos 
\texttt{repOK}) escritas por sus propios autores, lo que garantiza una base confiable para evaluar 
algoritmos de generación de estructuras válidas.

Los experimentos se llevaron a cabo en una computadora con procesador Intel Core i7-8700 
(3.2 GHz) y 16 GB de RAM, ejecutando Ubuntu Linux 20.04. Para cada ejecución individual se estableció 
un tiempo máximo de espera de 60 minutos. Todos los experimentos fueron diseñados para ser reproducibles, 
y el artefacto experimental con el código fuente se encuentra disponible en: ???.

Cada técnica evaluada se ejecutó sobre los mismos casos de estudio, con parámetros estandarizados 
en términos de alcance (\textit{scope}), número de métodos disponibles, y criterios de generación. 
Para la comparación con \textsf{Korat}, se emplearon los \texttt{repOK}s provistos por los autores 
de cada benchmark, sin modificaciones, a fin de preservar la equivalencia semántica entre enfoques 
y evitar sesgos en la evaluación del rendimiento. En los casos donde se identificaron errores o 
especificaciones incompletas en los \texttt{repOK}s, estos se señalan de forma explícita en los 
resultados y se analizan en la Sección~\ref{sec:existing-specs-analysis}.

\cacho{los parametros usados en GA}


\subsection{RQ4: Eficiencia de BEAPI}

Esta pregunta busca evaluar si \emph{BEAPI} es lo suficientemente eficiente como para ser una 
alternativa viable frente a otras técnicas de generación exhaustiva acotada, en particular 
\textsf{Korat}. Para ello, comparamos el desempeño de ambas herramientas en términos de tiempo 
de ejecución, número de estructuras generadas y exploradas, y escalabilidad con respecto al 
\emph{scope} de entrada.

Los resultados experimentales, presentados en la Tabla~\ref{table:korat-beapi}, cubren una muestra 
representativa de casos tomados de los bancos de pruebas \textsf{Korat}, \textsf{FAJITA}, 
\textsf{ROOPS} y \textsf{Kiasan}. Para cada clase evaluada, informamos los tiempos de generación 
(en segundos), la cantidad de estructuras válidas generadas y el total de estructuras exploradas 
por cada técnica. Incluimos, para cada enfoque, el mayor \emph{scope} alcanzado exitosamente dentro 
del tiempo límite de 60 minutos, indicando en negrita los tiempos correspondientes a los límites 
de escalabilidad. Vale aclarar que no se utilizo el benchamarks \textsf{realWorld} en esta sección, ya que
no se dispone de \texttt{repOK}s para estas clases, lo que impide una comparación justa con \textsf{Korat}.

En estos experimentos, se aseguraron condiciones comparables: los \texttt{repOK}s utilizados con 
\textsf{Korat} fueron los provistos originalmente por los autores de cada benchmark y no se 
modificaron para este estudio, evitando así alterar la eficiencia inherente a la técnica. 
Para \emph{BEAPI}, se probó exhaustivamente que los métodos de la API empleados fueran correctos 
y suficientes para el análisis. Como era de esperarse, pueden existir diferencias en los conjuntos 
de estructuras exploradas por cada técnica, debido a los distintos espacios de búsqueda que inducen. 
Sin embargo, para un mismo caso y alcance, ambas herramientas deberían generar la misma cantidad 
de estructuras válidas, salvo en casos donde hay errores en los \texttt{repOK}s o diferencias sutiles 
en la definición de \emph{scope}.

Los resultados muestran un comportamiento mixto. En estructuras con restricciones fuertes —como los 
árboles rojo-negro (\texttt{RBT}) y los árboles binarios de búsqueda (\texttt{BST})— \emph{BEAPI} 
supera a \textsf{Korat}, al explorar menos estados y terminar en menos tiempo. Esto ocurre porque 
en estas estructuras el número de configuraciones válidas es más bajo, lo que permite a \emph{BEAPI} 
recorrer el espacio de soluciones de forma eficiente. Por el contrario, en estructuras más permisivas, 
como listas doblemente enlazadas (\texttt{DLList}) o montículos binomiales (\texttt{BinHeap}), 
\textsf{Korat} logra un mejor rendimiento gracias a su estrategia de poda temprana basada en 
\texttt{repOK}.

Cabe destacar dos tipos de excepciones en los resultados. En primer lugar, algunos casos incluyen 
\texttt{repOK}s con errores, que impactan negativamente en la eficiencia de \textsf{Korat}; estos 
casos están sombreados en la tabla y se analizan más en detalle en la Sección~\ref{sec:existing-specs-analysis}. 
En segundo lugar, en estructuras como \texttt{RBT} y \texttt{FibHeap}, la diferencia semántica en la 
noción de \emph{scope} entre ambas técnicas produce discrepancias. Por ejemplo, hay estructuras de 
tamaño $n$ que solo pueden obtenerse mediante secuencias que superan temporalmente dicho tamaño 
(inserciones seguidas de eliminaciones y nuevas inserciones para provocar reordenamientos de balanceo). 
Como \emph{BEAPI} descarta automáticamente las secuencias que exceden el \emph{scope} máximo, no puede 
llegar a estas configuraciones, mientras que \textsf{Korat} sí lo logra.

Un aspecto clave que influye en la eficiencia de \emph{BEAPI} es la cantidad de estructuras válidas 
generables para un determinado alcance. A medida que crece esta cardinalidad, aumenta también el 
número de secuencias que \emph{BEAPI} debe evaluar, lo cual impacta directamente en su tiempo de 
ejecución. Sin embargo, un punto a favor de \emph{BEAPI} es que no depende de la eficiencia del 
\texttt{repOK}; esto lo hace más robusto ante especificaciones incorrectas o mal escritas, mientras 
que \textsf{Korat} puede verse severamente afectado por ellas.

En cuanto al rendimiento observado, \textsf{Korat} muestra mejor desempeño en 4 de los 6 casos 
pertenecientes a su propio banco de pruebas. En el benchmark \textsf{FAJITA}, \emph{BEAPI} supera 
a \textsf{Korat} en 3 de 4 casos, mientras que en \textsf{ROOPS} lo hace en 5 de 7. Finalmente, 
en el banco \textsf{Kiasan}, \textsf{Korat} resulta más rápido en 6 de 7 casos. Esta tendencia confirma 
que \emph{BEAPI} ofrece un mejor rendimiento en estructuras altamente restringidas —donde el número 
de configuraciones válidas es reducido— mientras que \textsf{Korat} mantiene la ventaja en dominios 
más amplios, donde la cantidad de estructuras válidas crece de forma acelerada. Esto se debe a que 
en tales escenarios, \emph{BEAPI} debe generar y evaluar muchas más secuencias de entrada en cada 
iteración, lo cual penaliza su desempeño.

Un caso ilustrativo es el de \texttt{BinHeap}, donde \textsf{Korat} alcanza el \emph{scope} 8 utilizando 
el \texttt{repOK} de \texttt{ROOPS}, el \emph{scope} 10 con el de \texttt{FAJITA}, y el \emph{scope} 11 
con su propia especificación, todas ellas equivalentes en términos de estructuras generadas. Esto evidencia 
la sensibilidad de \textsf{Korat} a la forma en que se escriben los \texttt{repOK}s, los cuales, si están 
ajustados específicamente para su motor de búsqueda, pueden mejorar significativamente su rendimiento. 
No obstante, cuando los \texttt{repOK}s presentan errores o están mal diseñados, su impacto negativo 
puede ser severo, como se analiza en profundidad en la Sección~\ref{sec:existing-specs-analysis}.

En conclusión, \emph{BEAPI} es una herramienta eficiente para la generación exhaustiva acotada. 
Ofrece tiempos competitivos y, en varios casos, mejores que \textsf{Korat}, especialmente cuando 
se trata de estructuras con invariantes complejas o bajo número de soluciones. Su independencia 
respecto al \texttt{repOK} lo vuelve una alternativa robusta y complementaria para contextos donde 
la calidad de la especificación no está garantizada.
\input{tables/tableEficienciaBEAPI.tex}

\subsection{RQ5: Impacto de las optimizaciones propuestas}
\label{sec:optimizations}

En esta pregunta de investigación evaluamos el impacto que tienen las optimizaciones propuestas 
de \emph{BEAPI} en su rendimiento general frente al enfoque de generación exhaustiva tradicional. 
Concretamente, analizamos cuatro configuraciones de la herramienta: \textsf{SM/BLD}, que habilita tanto 
la coincidencia de estados (State Matching) como la identificación de constructores (\emph{builders}); 
\textsf{SM}, que activa únicamente la coincidencia de estados; \textsf{BLD}, que habilita únicamente 
la identificación de constructores; y \textsf{NoOPT}, que desactiva ambas optimizaciones, funcionando 
como un enfoque de fuerza bruta.

Para este análisis, nos enfocamos en el benchmark \texttt{Real World}, que incluye seis 
implementaciones reales de estructuras de datos ampliamente utilizadas: \texttt{LinkedList} 
(67 métodos en la API), \texttt{TreeSet} (34), \texttt{TreeMap} (61), \texttt{HashMap} (45), \texttt{NCL} (34) y 
\texttt{Schedule} (12). La Tabla~\ref{tab:results-realWorld} 
resume los resultados obtenidos. Cabe mencionar que se realizaron experimentos equivalentes para 
el resto de los benchmarks utilizados en este capítulo, pero se omiten en las tablas en esta tesis por motivos de espacio.
El conjunto \texttt{Real World} representa adecuadamente el comportamiento observado en los demás casos, 
y resulta suficiente para sustentar las conclusiones extraídas en esta sección.

Los resultados muestran de forma contundente que la configuración sin optimizaciones (\textsf{NoOPT}) 
tiene un rendimiento deficiente incluso en los estudios de caso más simples y con alcances bajos. 
Estos \emph{scopes}, si bien pequeños, ya resultan inadecuados para generar conjuntos de test de 
calidad y revelan de inmediato la falta de escalabilidad del enfoque de fuerza bruta. Esta limitación 
es representativa de la problemática general que enfrentan las técnicas de generación exhaustiva en 
presencia de APIs amplias y estructuras complejas.

Entre las optimizaciones propuestas, la coincidencia de estados (\textsf{SM}) demuestra ser la más 
impactante a nivel general. Por sí sola, permite escalar a alcances considerablemente mayores y reduce 
los tiempos de ejecución en varios órdenes de magnitud respecto a \textsf{NoOPT}. Su efecto se vuelve 
crítico en aquellas estructuras donde múltiples secuencias de llamadas a métodos pueden generar el mismo estado 
intermedio, lo que genera redundancia innecesaria si no se cuenta con un mecanismo de detección y poda 
de estados previamente explorados.

La segunda optimización, la identificación previa de métodos \emph{builders} (\textsf{BLD}), adquiere 
mayor relevancia a medida que se incrementa la cantidad de métodos disponibles en la API. En 
estructuras reales como \texttt{TreeMap}, \texttt{LinkedList} o \texttt{HashMap}, donde existen 
docenas de métodos públicos con efectos variados, limitar la generación de secuencias a combinaciones 
de constructores válidos tiene un impacto significativo en la eficiencia del proceso de generación. 
Aunque por sí sola esta optimización no siempre permite escalar a scopes altos, su combinación con la 
coincidencia de estados resulta esencial para lograr tiempos razonables de ejecución.

El mayor beneficio se observa en la configuración \textsf{SM/BLD}, donde ambas optimizaciones están 
habilitadas. En esta configuración, \emph{BEAPI} escala significativamente mejor, alcanzando scopes 
mucho mayores en todos los casos analizados, con tiempos de ejecución hasta cientos de veces menores 
que con las demás configuraciones. Por ejemplo, para \texttt{NCL}, se logra alcanzar \textit{scope} 6 
en 73.78 segundos con \textsf{SM/BLD}, mientras que con \textsf{SM} apenas se alcanza \textit{scope} 4 
en más de 3 segundos, y \textsf{NoOPT} no logra ejecutar siquiera scopes bajos. Para \texttt{TreeSet}, 
el contraste es aún más pronunciado: \textsf{SM/BLD} alcanza \textit{scope} 13 en 226 segundos, mientras 
que \textsf{SM} requiere casi 900 segundos para el mismo alcance.

Estos resultados evidencian que las optimizaciones propuestas no solo reducen tiempos de ejecución, 
sino que amplían sustancialmente los límites de escalabilidad de la herramienta. Sin estas mejoras, 
\emph{BEAPI} se comporta como un generador exhaustivo tradicional, enfrentando rápidamente la explosión 
combinatoria del espacio de búsqueda al aumentar el número de métodos o el tamaño de las secuencias. 
En cambio, con las optimizaciones activadas, la técnica logra mantener el proceso dentro de márgenes 
computacionalmente viables incluso en contextos realistas y exigentes.

En conclusión, la activación conjunta de coincidencia de estados e identificación de \emph{builders} 
resulta crucial para obtener un rendimiento competitivo. Estas optimizaciones transforman a \emph{BEAPI} 
en una herramienta capaz de escalar a scopes altos sin sacrificar exhaustividad ni precisión, 
permitiendo su aplicación efectiva en casos de uso reales.

Para todos los experimentos presentados en esta sección, se empleó un \emph{scope} fijo de 5 durante la 
identificación de métodos generadores, y el tiempo máximo requerido por esta etapa fue de 132 segundos, 
correspondiente a la estructura \emph{TreeMap}, que posee 61 métodos públicos. Verificamos manualmente que, 
en todos los casos, el conjunto de métodos identificados fuera suficiente para construir los objetos válidos.

Cabe señalar que la identificación de métodos generadores se realiza típicamente una única vez por clase, 
y puede reutilizarse en múltiples ejecuciones posteriores de \emph{BEAPI}. En consecuencia, su costo se 
amortiza a lo largo del tiempo, para los scopes más grandes, que son los más importantes, el tiempo de identificación de 
métodos generadores es insignificante en relación con los tiempos de generación.Por esta razón, decidimos no incluir los tiempos de identificación 
en los tiempos de ejecución reportados para \textsf{BEAPI} en los distintos experimentos.
No obstante, es importante destacar que este preprocesamiento introduce una optimización significativa en 
términos de eficiencia, con un costo computacional bajo en relación con el proceso completo de generación. 


\input{tables/optimizaciones}


\subsection{RQ6: BEAPI para analizar invariantes de representación}
\label{sec:existing-specs-analysis}

Esta pregunta de investigación evalúa si \textsf{BEAPI} puede ser útil para asistir a los usuarios 
en la detección de fallas en los métodos \texttt{repOK}, a través de la comparación entre el conjunto 
de objetos que se pueden generar mediante la API y aquellos que se pueden generar a partir del 
invariante de representación.

Para ello, diseñamos un procedimiento automatizado en tres pasos. En primer lugar, ejecutamos 
\emph{BEAPI} para generar un conjunto de estructuras, denominado \texttt{SA}, utilizando únicamente 
la API de la clase, y ejecutamos \emph{Korat} para generar un segundo conjunto, \texttt{SR}, 
utilizando únicamente el método \texttt{repOK}. Ambas ejecuciones se realizaron con el mismo 
\emph{scope}. En segundo lugar, canonizamos todas las estructuras de ambos conjuntos utilizando el 
proceso de linearización (Sección~\ref{sec:stateMatching}), de modo que estructuras isomorfas puedan 
compararse de forma directa. Finalmente, comparamos los conjuntos \texttt{SA} y \texttt{SR} para 
identificar diferencias que indiquen discrepancias semánticas entre la definición operativa de la 
clase (la API) y su especificación declarativa (\texttt{repOK}).

El procedimiento puede producir tres resultados distintos. Cuando \texttt{SA} $\subset$ \texttt{SR}, 
puede deberse a que la API genera un subconjunto de las estructuras válidas, que \texttt{repOK} 
sufre de subespecificación (es decir, permite estructuras que no deberían ser válidas), o ambos. 
En estos casos, las estructuras que están en \texttt{SR} pero no en \texttt{SA} son evidencia potencial 
de errores, y fueron inspeccionadas manualmente para confirmar si se trataba efectivamente de fallas 
en \texttt{repOK}. Clasificamos estos errores como \texttt{under}, ya que el invariante permite más 
de lo que debería.

Por el contrario, cuando \texttt{SR} $\subset$ \texttt{SA}, puede indicar que \texttt{repOK} es demasiado 
restrictivo (sobreespecificado), que la API permite estructuras no válidas, o ambas cosas. En este 
caso, se analizan las estructuras que aparecen en \texttt{SA} pero no en \texttt{SR}. Los errores 
confirmados manualmente en esta categoría se etiquetan como \texttt{over}.

En algunos casos, se encontraron diferencias en ambas direcciones: estructuras válidas generadas por 
\textsf{Korat} que no están presentes en \texttt{SA}, y estructuras generadas por \emph{BEAPI} que 
no son aceptadas por \texttt{repOK}. Estas diferencias simultáneas pueden deberse a fallas en ambas 
fuentes (API y \texttt{repOK}), o a errores más profundos de especificación. En estos casos, cuando se 
confirma una falla manualmente, la clasificamos como \texttt{error}.

Cabe destacar que diferencias menores en la definición de \emph{scope} entre los enfoques pueden 
producir "falsos positivos" en esta comparación. Esto ocurrió solamente en las estructuras 
\texttt{RBT} y \texttt{FibHeap}, donde \emph{BEAPI} no generó ciertas estructuras válidas dentro del 
alcance debido a restricciones de balanceo interno que requieren secuencias más largas que el 
\emph{scope} establecido. Estos casos se identificaron y descartaron manualmente, observando que al 
aumentar el \emph{scope}, las estructuras de \textsf{Korat} sí aparecen en la salida de \emph{BEAPI}.

Los resultados del experimento se resumen en la Tabla~\ref{table:bugs}. Encontramos errores en 9 de 
los 26 métodos \texttt{repOK} analizados. Esta proporción evidencia la dificultad de escribir 
invariantes de representación correctos, incluso en bibliotecas diseñadas por expertos, y pone de 
manifiesto el valor de \textsf{BEAPI} como herramienta de apoyo para detectar inconsistencias.

En particular, se detectaron errores de subespecificación en clases como \texttt{RBTree}, \texttt{AVL}, 
\texttt{BinTree}, \texttt{FibHeap} y \texttt{NCL}, donde \texttt{repOK} permite estructuras 
incompletas, con campos nulos o claves no válidas, que no pueden ser generadas mediante la API 
original. También se identificaron casos de sobreespecificación, como en \texttt{FibHeap}, donde 
el método \texttt{repOK} rechaza heaps válidos cuyo nodo mínimo es nulo de forma transitoria. 
Finalmente, se confirmaron errores estructurales en las definiciones de altura y ordenamiento en 
clases como \texttt{AVL}, donde las hojas están mal inicializadas o los cálculos son inconsistentes 
con la implementación.

En conclusión, \emph{BEAPI} no solo es útil para generar estructuras exhaustivamente, sino que 
también actúa como un verificador complementario de especificaciones de invariante, permitiendo 
descubrir errores difíciles de detectar mediante revisión manual o prueba dinámica convencional. 
Este enfoque ofrece una vía práctica y automatizada para mejorar la calidad de las especificaciones 
de clase, lo que redunda en mayor confiabilidad del software verificado.

\input{tables/bugRepOk}

\subsection{RQ7: Usando BEAPI para el testing parametrizado}
\ref{sec:parametrizedTesting}
En esta etapa de la evaluación, se llevó a cabo un análisis exhaustivo para determinar la 
utilidad de los métodos generadores identificados mediante nuestro enfoque, particularmente 
en el contexto de la generación automatizada de casos de prueba. Estos métodos permiten construir 
objetos que pueden utilizarse como entradas en test suites parametrizadas.

Los tests parametrizados constituyen una técnica eficiente para mejorar la cobertura en tests 
automatizados. En lugar de definir casos de prueba específicos para cada configuración, se 
establece una única prueba que se ejecuta múltiples veces con distintos parámetros. En nuestro 
contexto, dichos parámetros son objetos generados por distintas técnicas, los cuales sirven como 
entrada a los métodos de la clase bajo prueba.

Para llevar a cabo este experimento, diseñamos una test suite parametrizada en la que cada método 
público de la clase bajo análisis es ejercitado con objetos generados por distintas técnicas. Las 
herramientas utilizadas para generar estos objetos fueron: la versión estándar de Randoop, que 
genera directamente una suite de tests sin reutilizar objetos previos; una variante de Randoop 
denominada \emph{R-Serialize}, que serializa los objetos construidos durante la generación de 
tests, permitiendo su reutilización como entrada en tests parametrizados; una versión modificada 
de Randoop que prioriza el uso de métodos generadores de objetos identificados previamente mediante 
nuestro enfoque (denominada \emph{R-Builders}); y por último, la herramienta \emph{BEAPI}, que 
produce directamente un conjunto reducido pero válido de objetos a partir de su propia lógica de 
exploración exhaustiva acotada.

A fin de evaluar y comparar el impacto de cada enfoque, se utilizó un benchmark basado en clases 
del paquete \emph{java.util}: \emph{HashSet}, \emph{HashMap}, \emph{TreeMap}, \emph{TreeSet} y \emph{LinkedList}.
Para cada clase se definió un invariante que permite distinguir objetos válidos de inválidos, permitiendo así evaluar la calidad de los generadores.

Las métricas consideradas fueron: la cantidad de objetos válidos e inválidos generados, el tiempo 
de generación (\emph{GTime}), este tiempo es el que se le da cada herramienta para ejecutar. Vale aclarar,
que en el caso de BEAPI, este tiempo es el equivalente a cierto scope. Vale recordar al lector, que BEAPI, al ser un generador exhaustivo,
tiene limite de scope y no de tiempo para generar. 
Ademas se tuvo en cuenta las metricas de la cantidad de casos de prueba ejecutados (\emph{Test}), el 
tiempo total de ejecución de la test suite (\emph{T(Seg)}), la cobertura de ramas alcanzada 
(\emph{Ramas}) y la cantidad de mutantes eliminados (\emph{Mutacion}). Es importante destacar 
que la test suite parametrizada es común a los enfoques \emph{R-Serialize}, \emph{R-Builders} 
y \emph{BEAPI}, permitiendo así una comparación justa sobre los objetos generados. En el caso de 
\emph{Randoop}, la herramienta genera su propia test suite, por lo que los resultados deben 
interpretarse con cautela.

\input{tables/randoopvsBeapi}

La Tabla~\ref{tab:hashSetTools} muestra una comparativa de diferentes técnicas aplicadas sobre 
\texttt{HashSet}. Se observa que al priorizar métodos generadores de objectos en Randoop, \emph{R-Builders}, la cantidad de 
objetos generados se incrementa significativamente (más de 300\% en comparación con la versión 
estándar, \emph{R-Serialize}), lo cual impacta positivamente en la cobertura, principalmente en tiempos de generacion más cortos.

En contraste, \textsf{BEAPI} genera sólo 32 objetos válidos, pero logra una cobertura de ramas y 
mutación equivalente a \texttt{R-Builders}, y superior a la alcanzada por \texttt{R-Serialize} y 
\texttt{Randoop} en lapsos de tiempo cortos. Esta eficiencia evidencia que BEAPI puede generar un 
conjunto reducido de objetos, pero de alta calidad para testing, en muy poco tiempo (menos de cinco 
segundos).

En términos de ejecución, mientras \texttt{R-Builders} y \texttt{R-Serialize} requieren ejecutar 
millones de tests para alcanzar altos niveles de cobertura, \texttt{BEAPI} logra el mismo nivel 
ejecutando apenas unos pocos miles. Esto indica que los objetos generados por \textsf{BEAPI} son 
altamente efectivos como entradas, permitiendo detectar fallas y explorar el espacio de estados 
de manera eficiente.

La Tabla~\ref{tab:treeSetTools} presenta los resultados obtenidos para \texttt{TreeSet}. En 
este caso, las diferencias comienzan a notarse con mayor claridad. \texttt{R-Builders} logra 
una cobertura y mutación ligeramente superior a \texttt{BEAPI}. Una posible explicación para 
este comportamiento radica en que ciertas configuraciones válidas de \texttt{TreeSet} pueden 
requerir secuencias de inserciones y eliminaciones que BEAPI no alcanza debido a su 
limitación por scope. Por el contrario, al ejecutarse sin restricciones estructurales, 
\texttt{R-Builders} puede explorar más libremente el espacio de estados, incluso si requiere 
pasar por configuraciones intermedias más complejas que luego se simplifican.

El comportamiento observado se repite en la Tabla~\ref{tab:treeMapTools}, donde se evalúa la 
clase \texttt{TreeMap}. En este caso, \texttt{R-Builders} vuelve a superar a \texttt{BEAPI} 
en métricas de cobertura y mutación. Nuevamente, la explicación podría estar relacionada con 
estructuras internas que BEAPI no puede construir directamente bajo un scope fijo, como 
árboles parcialmente desbalanceados que luego se ajustan mediante rotaciones o reordenamientos. 
A pesar de estas diferencias, \texttt{BEAPI} mantiene una cobertura muy competitiva y logra 
resultados consistentes con una fracción del esfuerzo computacional, lo que refuerza su valor 
como técnica efectiva, especialmente en contextos donde se requiere control y exhaustividad 
estructural.

En cuanto a la Tabla~\ref{tab:linkedListTools} que presenta los resultados obtenidos para la clase 
\texttt{LinkedList}. En este caso, se observa nuevamente que \texttt{BEAPI}, a pesar de 
generar un número reducido de objetos, mantiene niveles constantes de cobertura y mutación. 
Las técnicas \texttt{R-Serialize} y \texttt{R-Builders} logran cubrir las mismas ramas y casi los mismos 
mutantes que \texttt{BEAPI}, pero requieren ejecutar un volumen mucho mayor de tests. Con respecto a la tabla ~\ref{tab:hashMapTools}, 
correspondiente a \texttt{HashMap}, se aprecia un caso 
similar. \texttt{BEAPI} logra altos niveles de cobertura desde tiempos de generación muy 
reducidos, mientras que las otras técnicas necesitan un volumen considerable de objetos y 
tests para alcanzar resultados comparables. Un aspecto distintivo de este caso es que las 
estructuras de tipo clave-valor introducen combinaciones adicionales, y aún así, la técnica 
mantiene su eficacia. Esto evidencia que los objetos generados por \texttt{BEAPI} no solo son 
válidos, sino también representativos de situaciones relevantes para la clase, permitiendo 
maximizar el valor de los tests ejecutados.
\cacho{pensar porque R-builders le gana en mutacion a beapi, creo que eran sobren estructuras que no la usa BEAPI}

Los resultados obtenidos permiten concluir que los objetos generados por \textsf{BEAPI} son 
efectivos y eficientes para ser utilizados en tests parametrizados. Aunque la cantidad total de 
objetos generados es menor, su calidad y representatividad permiten alcanzar los máximos niveles 
de cobertura con una mínima ejecución. Además, se observa que guiar a generadores aleatorios como 
Randoop mediante la identificación previa de métodos generadores de objectos también mejora significativamente 
la cobertura. Esto hace que se reafirme el valor del otro aporte de esta tesis: la identificación automática de métodos generadores.
Este conocimiento no solo permite mejorar herramientas existentes como Randoop (caso de \texttt{R-Builders}), 
sino que también da lugar a técnicas completamente nuevas como \textsf{BEAPI}, 

En contextos donde el tiempo de ejecución es un factor crítico, o donde se necesita eficiencia sin 
sacrificar cobertura, \textsf{BEAPI} se posiciona como una herramienta valiosa para la generación 
de objetos de prueba. Estos resultados también reafirman la importancia de identificar y utilizar 
adecuadamente los métodos generadores para mejorar la calidad de los tests automatizados.












\cacho{Esto es viejo}

\cacho{ya esta en el capitulo, lo movi para alla
}
Para implementar el algoritmo genético, utilizamos una biblioteca muy popular en Java llamada \emph{Jenetics}\footnote{https://jenetics.io/}. Esta biblioteca está diseñada específicamente para algoritmos evolutivos y nos proporcionó las herramientas necesarias para desarrollar nuestro enfoque genético.

\emph{Jenetics} es una biblioteca robusta y versátil que ofrece una amplia gama de funcionalidades para la implementación de algoritmos genéticos. Nos permitió definir y manipular genes, cromosomas y poblaciones, así como utilizar operadores genéticos como selección, cruzamiento y mutación. Además, cuenta con un sólido conjunto de herramientas de optimización y técnicas de evolución que nos permitieron adaptar el algoritmo a nuestras necesidades específicas.
Gracias a \emph{Jenetics}, pudimos implementar el algoritmo genético de manera eficiente y efectiva, lo que nos permitió explorar y encontrar subconjuntos óptimos de métodos \emph{builders} para las diferentes estructuras de datos en nuestro estudio. Además, \emph{Jenetics} es muy fácil de usar, con una documentación completa y una comunidad activa de usuarios que proporciona soporte y ayuda. 


% \subsubsection{Variacion de acuerdo a los Parametros}

% Ademas, examinamos la sensibilidad de los parámetros del Algoritmo Genético y exploramos los usos de los builders generados. Los resultados obtenidos proporcionaron información valiosa sobre el rendimiento y la utilidad de nuestra técnica en la generación de builders y su aplicación en diversas tareas.

% Nuestra comparacion se basa en medir la cantidad de tiempo que le lleva a cada Algortimo terminar la ejecucio, en la cantidad de candidatos que evalua la funcion de valoracion y cuan bueno es en eficacia para encontrar el minimo y suficiente subconjunto de metodos que pudimos observar en nuestro ground truth \cacho{Agregar seccion}. Ejecutamos el algoritmo 10 veces con el resto de los parametros que no esta en evaluacion con un valor promedio (Crossover=0.5, mutation 0.1, Tournanament 4).
% Tambien utilizamos 30 segundos para la fitness con randoop y scope 6 para BEAPI.

% En la tabla \ref{tab:CrossOverGA} se puede observar como se comporta el algoritmo cuando se utiliza diferente rate para el operador de CrossOver. 



% \subsection{Uso de BEAPI para analizar especificaciones}
% \label{sec:existing-specs-analysis}
% \input{tables/bugRepOk}
% La RQ3 aborda si \textsf{BEAPI} puede ser útil para ayudar al usuario a encontrar fallas en \texttt{repOK}s, mediante la comparación del conjunto de objetos que se pueden generar utilizando la API y el conjunto de objetos generados a partir de utilizar un invariante, como es el caso del \texttt{repOK}. Diseñamos el siguiente procedimiento automatizado. Primero, ejecutamos \emph{BEAPI} para generar un conjunto, \texttt{SA}, 
% de estructuras a partir de la API, y utilizamos \emph{Korat} para generar un conjunto, \texttt{SR}, a partir de \texttt{repOK}, 
% utilizando el mismo ámbito para ambas herramientas. En segundo lugar,  . En tercer lugar, comparamos 
%  los conjuntos \texttt{SA} y \texttt{SR} en cuanto a igualdad. Las diferencias en esta comparación 
%  señalan una discrepancia entre \texttt{repOK} y la API. Existen tres posibles resultados para este 
%  procedimiento automatizado. Si \texttt{SA} $\subset$ \texttt{SR}, es posible que la API genere un 
%  subconjunto de las estructuras válidas, que \texttt{repOK} sufra de subespecificación (\texttt(under)) 
%  (restricciones faltantes), o ambos. En este caso, las estructuras en \texttt{SR} que no pertenecen a
%   \texttt{SA} son evidencia del problema, y el usuario debe analizarlas manualmente para descubrir 
%   dónde está el error. Aquí, informamos los errores de subespecificación (confirmados manualmente) 
%   en \emph{repOK}s que son evidenciados por las estructuras mencionadas. En contraste, cuando \texttt{SR}
%    $\subset$ \texttt{SA}, puede ser el caso de que la API genere un superconjunto de las estructuras válidas
%    , que \texttt{repOK} sufra de sobreespecificación, \texttt(over), (\texttt{repOK} es demasiado restrictivo)
%    , o ambos. Las estructuras en \texttt{SA} que no pertenecen a \texttt{SR} podrían indicar la raíz del error,
%     y nuevamente deben ser analizadas manualmente por el usuario. Informamos los errores de sobreespecificación 
%     (confirmados manualmente) en \texttt{repOK}s que son evidenciados por estas estructuras. 
%     Finalmente, puede darse el caso de que haya estructuras en \texttt{SR} que no pertenecen 
%     a \texttt{SA}, y que haya estructuras (distintas de las anteriores) en \texttt{SA} que no
%      pertenecen a \texttt{SR}. Estos pueden ser debidos a fallos en la API, fallas en \texttt{repOK}, 
%      o ambos. Informamos las fallas confirmadas manualmente en \texttt{repOK}s que son evidenciadas por
%       tales estructuras simplemente como errores (\texttt{repOK} describe un conjunto de estructuras 
%       diferente al que debería). Observa que las diferencias en las definiciones de ámbito de los enfoques 
%       pueden hacer que los conjuntos \texttt{SA} y \texttt{SR} difieran. Esto solo fue el caso en las 
%       estructuras \texttt{RBT} y \texttt{FibHeap}, donde \textsf{BEAPI} generó un conjunto más pequeño de 
%       estructuras para el mismo ámbito que \textsf{Korat} debido a restricciones de balance (como se explica
%        en la Sección \ref{sec:evaluation-vs-korat}). Sin embargo, estos "falsos positivos" se pueden revelar 
%        fácilmente, ya que todas las estructuras generadas por \textsf{Korat} siempre estuvieron incluidas en
%         las estructuras generadas por \textsf{BEAPI} si se utilizaba un ámbito más amplio para este último
%          enfoque. Utilizando esta información, descartamos manualmente los "falsos positivos" debido a las 
%          diferencias de ámbito en \texttt{RBT} y \texttt{FibHeap}.

% Los resultados de este experimento se resumen en la Tabla \ref{table:bugs}. Encontramos fallas en 9 de 26 \texttt{repOK}s utilizando el enfoque descrito anteriormente. El alto número de fallas descubiertas evidencia que los problemas en \texttt{repOK}s son difíciles de encontrar manualmente, y que \textsf{BEAPI} puede ser de gran ayuda para esta tarea.

% \subsection{Comparativa de BEAPI con otras tecnicas de generacion de test}

% \hspace{1cm}

% En esta etapa de la evaluación, se llevó a cabo un análisis exhaustivo para determinar la utilidad de los métodos builders identificados en el contexto del análisis de programas, específicamente en la generación automatizada de casos de prueba. Estos builders se consideran objetos clave que pueden ser utilizados como entradas en test parametrizadas.

% Los test parametrizados son una técnica utilizada en el campo de la generación automatizada de casos de prueba para aumentar la eficiencia y la cobertura de las pruebas. En lugar de escribir casos de prueba individuales para cada escenario posible, los test parametrizados permiten definir un conjunto de parámetros que se utilizan para generar automáticamente múltiples casos de prueba.

% En el contexto de la evaluación experimental, se utilizó la técnica de test parametrizados para alimentar una test suite con objetos creados por diferentes técnicas. Esto significa que se definieron parámetros que representan diferentes características o propiedades de los objetos, y luego se generaron automáticamente casos de prueba utilizando estos parámetros.

% Los test parametrizados son una técnica poderosa en la generación automatizada de casos de prueba, ya que permiten explorar diferentes combinaciones de parámetros y generar una variedad de casos de prueba de manera eficiente. En el contexto de la evaluación experimental, se utilizaron para evaluar y comparar el desempeño de diferentes técnicas en la generación de objetos y su impacto en la calidad de las pruebas.

% Para llevar a cabo este experimento, se creó una test suite parametrizada que serviría como marco de prueba para evaluar los distintos enfoques. Esta test suite parametrizada fue, básicamente, crear un test por método que contiene la clase y ejercitarlos con los objetos creados por las diferentes técnicas. Se utilizaron varias técnicas y herramientas para generar los objetos necesarios. En primer lugar, se empleó la conocida herramienta \texttt{Randoop} utilizando como es su forma estander, con todos los métodos disponibles en su API, lo que da lugar a una suite de pruebas tradicional generada por Randoop. En este caso no se utiliza una test suite parametrizada, ya que \texttt{Randoop}  no genera objetos, sino que crea sus propias tests suite.

% Ahora si, se utilizó una variante de Randoop llamada \texttt{R-Serialize} para serializar las secuencias de pruebas generadas anteriormente. Esto permitió generar objetos que, a su vez, se utilizaron para alimentar la test suite parametrizada, ampliando así el alcance de los casos de prueba. Estos objetos fueron generados utilizando todos los métodos de la API de Randoop, tal como se hizo en el enfoque anterior.

% Otra herramienta utilizada en el experimento fue una versión modificada de \texttt{Randoop}, diseñada específicamente para utilizar únicamente los métodos builders identificados previamente mediante nuestro enfoque (\ref{cap:builders}). Los objetos generados por esta variante especial de Randoop también se serializaron y se incorporaron a la test suite parametrizada, permitiendo una comparación directa entre los objetos generados por los builders identificados y los generados sin utilizar la información de estos métodos builders.

% Por último, se emplearon los objetos generados por la herramienta \texttt{BEAPI}. Estos objetos, creados utilizando su propia API, se integraron en la test suite parametrizada para evaluar su efectividad en la generación de casos de prueba. De esta manera, se obtuvo una visión completa y comparativa de las diferentes técnicas utilizadas en términos de generación de objetos y su impacto en la calidad de las pruebas.

% Mediante este enfoque meticuloso y riguroso, se buscó determinar la capacidad de los builders identificados para mejorar la generación automatizada de casos de prueba y, en última instancia, contribuir a la mejora de la calidad del análisis de programas. Los resultados obtenidos en esta evaluación experimental proporcionaron información valiosa sobre la utilidad y efectividad de los builders en el contexto del análisis de programas, abriendo así nuevas oportunidades para futuras investigaciones y desarrollos en este campo.

% A continuación, se realiza la comparativa de las diferentes herramientas utilizadas en el benchmark de \emph{java.util} previamente mencionado. Los casos de estudio son: \emph{HashSet}, \emph{HashMap}, \emph{TreeMap}, \emph{TreeSet} y \emph{LinkedList}. La tabla de resultados muestra varias métricas, como el tiempo en segundos (\texttt{GTime}) que es tiempo que lleva generar estos objectos/tests, la herramienta utilizada (\texttt{Tool}), la cantidad de objetos válidos generados (\texttt{Valid}) y la cantidad de objetos inválidos generados (\texttt{Invalid}).
% Es importante destacar que se definió un invariante para cada clase bajo evaluación, el cual establece qué estructuras son válidas e inválidas. Todos los objetos generados fueron sometidos a la prueba de estos invariantes para determinar su validez. Sin embargo, es importante mencionar que la técnica \texttt{Randoop} genera directamente una test suite en lugar de objetos individuales, por lo que no se aplica directamente el concepto de validez e invalidez a los casos generados por esta herramienta.

% Además, se realiza una comparación en función de la cantidad de tests generados por cada técnica. Esta medida se refleja en las columnas \texttt{Test}, \texttt{T(Seg)}, que indica la cantidad de tests que se ejecutan y sus respectivos segundos que son necesarios para ejecutar la test suite parametrizada, o no (en el caso de \texttt{Ranndop}), correspondiente.

% Finalmente, se evalúa la calidad de las test suites generadas mediante la comparación de la cobertura de ramas y la cantidad de mutantes eliminados. Estas métricas permiten determinar qué tan efectivas son las test suites en términos de su capacidad para cubrir diferentes ramas del código y eliminar mutantes generados para introducir fallas.

% A continuación analizaremos caso por caso con sus respectivas tablas.

% \input{tables/randoopvsBeapi}


% En esta etapa de la evaluación, se llevó a cabo un análisis exhaustivo para determinar la utilidad de los metodos builders identificados en el contexto del análisis de programas, específicamente en la generación automatizada de casos de prueba. Estos builders se consideran objetos clave que pueden ser utilizados como entradas en test parametrizadas. 
% Los test parametrizados son una técnica utilizada en el campo de la generación automatizada de casos de prueba para aumentar la eficiencia y la cobertura de las pruebas. En lugar de escribir casos de prueba individuales para cada escenario posible, los test parametrizados permiten definir un conjunto de parámetros que se utilizan para generar automáticamente múltiples casos de prueba.
% En el contexto de la evaluación experimental, se utilizó la técnica de test parametrizados para alimentar una test suite con objetos creados por diferentes técnicas. Esto significa que se definieron parámetros que representan diferentes características o propiedades de los objetos, y luego se generaron automáticamente casos de prueba utilizando estos parámetros.
% Los test parametrizados son una técnica poderosa en la generación automatizada de casos de prueba, ya que permiten explorar diferentes combinaciones de parámetros y generar una variedad de casos de prueba de manera eficiente. En el contexto de la evaluación experimental, se utilizaron para evaluar y comparar el desempeño de diferentes técnicas en la generación de objetos y su impacto en la calidad de las pruebas.

% Para llevar a cabo este experimento, se creó una test suite parametrizada que serviría como marco de prueba para evaluar los distintos enfoques. Esta test suite parametrizada fue, basicamente, crear un test por metodos que contiene la clase y ejercitalos con los objectos creados por las distintas técnicas. Se utilizaron varias técnicas y herramientas para generar los objetos necesarios. En primer lugar, se empleó la conocida herramienta \texttt{Randoop}, utilizando todos los métodos disponibles en su API estándar, lo que dio lugar a una suite de pruebas tradicional generada por Randoop.

% Además, se utilizó una variante de Randoop llamada \texttt{R-Serialize} para serializar las secuencias de pruebas generadas anteriormente. Esto permitió generar objetos que, a su vez, se utilizaron para alimentar la test suite parametrizada, ampliando así el alcance de los casos de prueba. Estos objetos fueron generados utilizando todos los métodos de la API de Randoop, tal como se hizo en el enfoque anterior.

% Otra herramienta utilizada en el experimento fue una versión modificada de \texttt{Randoop}, diseñada específicamente para utilizar únicamente los métodos builders identificados previamente mediante nuestro enfoque (\ref{cap:builders}). Los objetos generados por esta variante especial de Randoop también se serializaron y se incorporaron a la test suite parametrizada, permitiendo una comparación directa entre los objetos generados por los builders identificados y los generados sin utilizar la informacion de estos metodos builders.

% Por último, se emplearon los objetos generados por la herramienta \texttt{BEAPI}. Estos objetos, creados utilizando su propia API, se integraron en la test suite parametrizada para evaluar su efectividad en la generación de casos de prueba. De esta manera, se obtuvo una visión completa y comparativa de las diferentes técnicas utilizadas en términos de generación de objetos y su impacto en la calidad de las pruebas.

% Mediante este enfoque meticuloso y riguroso, se buscó determinar la capacidad de los builders identificados para mejorar la generación automatizada de casos de prueba y, en última instancia, contribuir a la mejora de la calidad del análisis de programas. Los resultados obtenidos en esta evaluación experimental proporcionaron información valiosa sobre la utilidad y efectividad de los builders en el contexto del análisis de programas, abriendo así nuevas oportunidades para futuras investigaciones y desarrollos en este campo.


%  A continuacion hacemos la comparativa de las diferentes tools, explicada en el parrafo anterior, en el benchmarks de \emph{java.util} utilizado previamente. Los casos de estudios son; \emph{HashSet},\emph{HashMap},\emph{TreeMap},\emph{TreeSet}, \emph{LinkedList}. La comparacion se realiza sobre, \texttt{GTime}, que representa el tiempo en milisegundos, \texttt{Tool} indica la herramienta utilizada, \texttt{Valid} muestra la cantidad de objectos válidos, \texttt{Invalid} muestra la cantidad de objetos inválidos que son generados. Para saber que objetos son válidos e inválidos, escribimos un invariante para cada clase bajo evaluacion, y este nos dice que estructura es valida y que estructura es invalida. Puede observar que para la tecnica \texttt{Randoo} esto no cuenta, ya que recuerde que aqui utilizamos la tool \texttt{Randoop} que generan test suite directamnete y no objectos. Ademas, BEAPI siempre va a generar objectos validos, esto se debe a que como estos objectos son construidos desde la API, no hay posibilidad de generar objectos invalidos. Vale aclarar que estos objectos tambien fueron puesto bajo prueba del mismo invariante que escribimos para cada clase. 
%  Ademas, comparamos de acuerdo a la cantidad de test que cada tecnica genera. Para realizar esta medida, en las tools que utilizan la test suite parametrizada, indica la cantidad de test que se ejecutan y es un valor de que tiene relacion con los objectos que se alimentan a la test suite. En base a esto \texttt{T(Seg)} es la cantidad de segundo que lleva ejecutar esta test suite.
%  Por ultimo, comparamos que tan buena es estas test suite, comprando la cobertura de ramas y de mutantes que matan.


