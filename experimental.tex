%!TEX root = main.tex
\chapter[Evaluaci\'on]{Evaluaci\'on}
\label{cap:experimental}


En los capítulos \ref{cap:builders} y \ref{cap:beapi} presentamos enfoques para
identificar automáticamente un conjunto de métodos generadores de objetos, y una
técnica novedosa de generación exhaustiva acotada basada en la API. 
En este capítulo, realizamos una evaluación experimental de las técnicas
mencionadas. La sección \ref{sec:experimentalIdentificacion} analiza experimentalmente los
algoritmos de identificación de generadores de objetos, mientras que en la
sección \ref{sec:experimentalBeapi} se evalúa la generación exhaustiva acotada basada en la API.


\section{Algoritmos de identificación de métodos generadores de objetos}
\label{sec:experimentalIdentificacion}

En esta sección, evaluamos experimentalmente los enfoques presentados en el
Capítulo~\ref{cap:builders}. Analizaremos la eficiencia y precision de cada
algoritmo utilizando las funciones de valoración introducidas en la
Sección~\ref{sec:fitness}. Además, mostraremos cómo los métodos identificados
por nuestras técnicas pueden ser aprovechados en el contexto de herramientas
de verificación de software.

Con respecto a esta técnica, las siguientes preguntas de investigación guían esta experimentación:

\begin{itemize}
\item \emph{RQ1}: ¿Qué tan eficientes son los algoritmos propuestos para
    identificar conjuntos de métodos generadores de objetos?
\item \emph{RQ2}: ¿Qué tan precisos son los algoritmos presentados para identificar métodos generadores de objetos?
\item \emph{RQ3}: ¿Cuál es el impacto de utilizar métodos generadores de objetos
    en el contexto del análisis automático de software?
\end{itemize}

\subsection{Configuración experimental}
\label{sec:experimentalIdentificacionConfig}
La evaluación se llevó a cabo sobre un conjunto de implementaciones de
estructuras de datos que sirven como casos de estudio de referencia,
incluyendo: \verb"NCL" de Apache Commons Collections~\cite{apache};
\verb"BinaryTree", \verb"BinomialHeap" y \verb"FibonacciHeap", extraídos
de~\cite{Visser:2006}; y \verb"UnionFind", una implementación de conjuntos
disjuntos tomada de JGrapht~\cite{jgrapht}.

También se incluyeron componentes de proyectos reales de software, tales como
\verb"Lits" de la implementación de Sat4j~\cite{sat4j}, utilizada previamente
en~\cite{Loncaric:2018}. Esta clase representa un conjunto de variables
booleanas, encargado de registrar cuándo se realizó la última asignación y si
existen observadores monitoreando su estado.

Por otro lado, se consideraron estructuras adicionales como \verb"Scheduler",
un planificador de procesos tomado del conjunto de benchmarks SIR~\cite{sir},
y varias colecciones del paquete estándar \verb"java.util" de Java%
\footnote{\url{https://docs.oracle.com/javase/8/docs/api/java/util/
package-summary.html}}, incluyendo \verb"TreeMap", \verb"TreeSet",
\verb"HashMap",\\
 \verb"HashSet" y \verb"LinkedList".

Para evaluar la precisión de nuestros algoritmos, se construyó manualmente un
conjunto de referencia o \emph{ground truth} que contiene los métodos
generadores considerados mínimos y suficientes en cada caso de estudio. Esta
tarea implicó un análisis detallado y manual de cada API, lo que representó un
trabajo considerable, especialmente en los casos más complejos. La
Tabla~\ref{tab:groundTruth} resume este conjunto de referencia, indicando para
cada clase el número total de métodos en su API pública (\#API) y los métodos
identificados manualmente como generadores de objetos. Cabe destacar que en algunos
casos ciertos métodos pueden ser intercambiables (por ejemplo,
\texttt{addFirst} y \texttt{addLast} en \texttt{NCL}).

Todos los experimentos fueron ejecutados en una máquina con procesador Intel
Core i7-6700 a 3.4\,GHz y 8\,GB de RAM, corriendo el sistema operativo
GNU/Linux.

\input{tables/groundTruth}

% Como se discutió en el Capítulo~\ref{cap:beapi}, nuestros algoritmos requieren
% la configuración de diversos parámetros que influyen en su eficiencia y
% efectividad. En particular, los parámetros más relevantes para el algoritmo
% genético incluyen la tasa de mutación, la tasa de cruce, el tamaño de la
% población y la estrategia de selección utilizada.

Nuestros algoritmos requieren la configuración de diversos parámetros que influyen en su eficiencia y
efectividad. En particular, los parámetros más relevantes para el algoritmo
genético ((Sección~\ref{sec:approachGA})) incluyen la tasa de mutación, la tasa de cruce, el tamaño de la
población y la estrategia de selección utilizada.

Para las evaluaciones que utilizan la función de valoración basada en Randoop
(Sección~\ref{sec:fitnessRandoop}), es importante considerar el tiempo
disponible para la generación de objetos y el valor de la semilla aleatoria.
Randoop utiliza esta semilla para controlar la generación de pruebas y
permitir la reproducción de resultados. Dado que Randoop introduce un grado
considerable de aleatoriedad, optamos por utilizar una semilla aleatoria
diferente en cada ejecución para evaluar la robustez de nuestro enfoque bajo
distintos escenarios.

En el caso de la función de valoración basada en generación exhaustiva
(Sección~\ref{sec:fitnessGE}), es crucial definir el \emph{scope}, es decir, la
cantidad máxima de objetos por clase que se pueden generar. Además, se omiten
ciertos campos internos de las clases bajo análisis que no afectan la
estructura lógica de los objetos generados, tales como el campo
\texttt{modCount}, común en muchas colecciones de Java, que simplemente cuenta
el número de modificaciones internas pero no altera la estructura observada
del objeto.

Los valores de configuración utilizados en los experimentos fueron determinados empíricamente mediante un proceso de prueba y error,
como es habitual en el diseño y ajuste de algoritmos genéticos. 
Se seleccionaron aquellos parámetros que ofrecieron los mejores resultados en términos de eficiencia y estabilidad de las soluciones encontradas. 
Para el algoritmo genético (descrito en la Sección~\ref{sec:fitnessGE}), se utilizó una tasa de cruce de 0.4 y una tasa de mutación de 0.05. 
La estrategia de selección adoptada fue la de torneo, con cuatro participantes por torneo, y se empleó una población de 100 individuos, 
evolucionando durante 20 generaciones por ejecución.

En lo que respecta a la función de valoración basada en Randoop (descrita en la Sección~\ref{sec:fitnessRandoop}),
se configuró un tiempo máximo de ejecución de 30 segundos por candidato, 
y se emplearon dos semillas distintas en cada ejecución para capturar la variabilidad introducida por 
el carácter aleatorio del generador. Esta configuración permite obtener una medida más robusta del desempeño 
de los algoritmos en contextos con elementos no determinísticos.

Por otro lado, en la función de valoración basada en generación exhaustiva (también discutida en la Sección~\ref{sec:fitnessGE}), 
se estableció un scope de 5, es decir, el número máximo de objetos de cada tipo que se generan para evaluar cada conjunto candidato de métodos
constructores. Además, se ignoraron campos de las clases evaluadas que no alteran la estructura lógica de los objetos generados, tales como el campo \texttt{modCount}.

Para cada combinación de caso de estudio y función de valoración, el algoritmo
genético fue ejecutado cinco veces con los parámetros mencionados. Los
resultados reportados en las siguientes secciones corresponden al promedio de
estas ejecuciones. En el caso particular de la función de valoración basada en
generación exhaustiva, no fue necesario realizar múltiples repeticiones, dado
que dicha evaluación es determinística bajo una misma configuración.


%Casos de estudio, técnicas a evaluar, métricas, parámetros usados, ground truth, etc..



\subsection{RQ1: Eficiencia del cómputo de generadores de objetos}


La eficiencia de nuestros algoritmos es la cantidad de segundos que le lleva a cada uno de nuestros algoritmos con cada función de 
valoración para encontrar el subconjunto de métodos generadores de objetos que sean suficientes. Para evaluar la eficiencia de nuestro algoritmo, 
llevamos a cabo experimentos utilizando conjuntos de datos de muestra y medimos el tiempo de promedio de las 5 ejecuciones necesarias 
para obtener las soluciones. Comparamos los resultados obtenidos con la técnica evolutiva (\emph{GA}) y con la técnica greedy (\emph{HC}) 
para evaluar la eficiencia de ambos enfoques en función de la función de valoración utilizada. Los resultados se presentan en la Tabla \ref{tab:eficiencia}.

Como se observa en la tabla, el algoritmo Hill Climbing (\emph{HC}) es más efectivo que el algoritmo genético (\emph{GA}) para calcular los constructores de métodos en todos los casos de estudio considerados. 
El enfoque de Hill Climbing es razonablemente eficiente, tardando solo 20 minutos en el peor de los casos (NCL, debido a la cantidad y complejidad 
de su API), mientras que el algoritmo genético puede tardar mas de una hora en el peor de los casos. Esto se debe a que los algoritmos 
de Hill Climbing comienzan desde abajo hacia arriba, considerando menos métodos antes de considerar más métodos, a diferencia de los 
algoritmos genéticos, que generan sucesores mediante operadores de cruce y mutación. Esta estrategia favorece al algoritmo Hill Climbing 
para encontrar los métodos mínimos en tiempos más cortos. Cabe mencionar que en casos de estudio donde el algoritmo genético supera al 
Hill Climbing, se debe a que la clase bajo prueba tiene pocos métodos, lo que permite que los algoritmos evolutivos converjan más 
rápidamente que el algoritmo Hill Climbing.


En relación con las funciones de valoración, observamos que la función de valoración basada en el generador exhaustivo (GE) logra 
finalizar la búsqueda de métodos generadores en mucho menos tiempo en comparación con la función de valoración basada en la 
cobertura generada por la suite de pruebas modificada del Randoop (RC). Esto se aplica a todos los algoritmos implementados. La función 
de valoración GE converge antes en los algoritmos debido a que genera objetos hasta el alcance especificado (en este caso, 5 para todos 
los casos) y luego finaliza la ejecución del candidato actual para continuar con los siguientes. Todas tienen un \emph{timeout} de 
30 segundos para el caso de pueda generar muchas estrucutras con el scope dado. Esto permite una convergencia más rápida en comparación 
con la función de valoración basada en Randoop, que agota su presupuesto de tiempo para cada candidato ejecutado en el algoritmo. Es 
importante destacar que todos los algoritmos se ejecutan con 10 hilos simultáneamente, evaluando así 10 candidatos a la vez para cada 
algoritmo.

En conclusión, en términos de eficiencia de nuestro algoritmo, para los casos estudiados en esta tesis, es conveniente utilizar el 
algoritmo Hill Climbing con una función de valoración que cuente los objetos generados por el generador exhaustivo (\emph{GE}).
\input{tables/eficienciaBuilders}

\subsection{RQ2: Precisión de los algoritmos de cómputo de generadores de objetos}

La efectividad de nuestros algoritmos se mide en función de cuán cercanos están a identificar el conjunto mínimo de métodos 
generadores de objectos de una API necesario para generar estructuras de datos válidas. 
Cabe destacar que en todos los casos, nuestros algoritmos 
logran identificar al menos un subconjunto suficiente de métodos generadores de objetos. No obstante, cuanto más reducido sea este 
subconjunto, mejor será la calidad del resultado en términos de cobertura de generación y simplicidad de las entradas de prueba.

Los experimentos se realizaron utilizando las estructuras de datos ya detalladas en la primera sección de este capítulo. Se evaluaron 
los dos algoritmos: Hill Climbing (\emph{HC}), un algoritmo genético (\emph{GA}). Para cada uno de ellos, 
se aplicaron dos funciones de valoración: una basada en el generador exhaustivo 
(\emph{GE}) y otra basada en la cobertura de ramas obtenida a partir de una suite de pruebas generada con Randoop (\emph{RC}).
Es importante señalar que la función de valoración basada en Randoop depende de una semilla aleatoria, lo que introduce cierta 
variabilidad en los resultados. Esta función mide la cobertura alcanzada durante la ejecución de la suite generada, y puede arrojar 
diferentes valores dependiendo de la semilla inicial.

\cacho{chequear esto}
En el caso de la estructura \emph{NCL}, el algoritmo \emph{HC} utilizando la función de valoración \emph{GE} logra encontrar el 
conjunto mínimo exacto de métodos generadores en todas las ejecuciones. Sin embargo, cuando se utiliza la función \emph{RC}, 
el mismo algoritmo encuentra en promedio un conjunto con 3.80 métodos, lo que indica que en tres de las cinco ejecuciones se incluyó 
un método adicional no necesario. Por su parte, el algoritmo evolutivo (\emph{GA}) encuentra en todas las ejecuciones un método de 
más con la función de cobertura, y en cuatro de cinco ejecuciones también lo hace con la función \emph{GE}. Esto se debe a la 
sensibilidad del algoritmo genético a sus parámetros (tasa de mutación, cruce, población, etc.), así como al número de generaciones 
y tiempo de ejecución asignado. Si se le otorgara tiempo suficiente y una adecuada calibración de parámetros, este algoritmo 
eventualmente convergería hacia el conjunto mínimo.

En estructuras como \emph{UnionFind}, \emph{FibonacciHeap}, \emph{BTree}, \emph{BinomialHeap} y \emph{Scheduler}, todos los 
algoritmos evaluados lograron encontrar el conjunto mínimo exacto de métodos \emph{builders}. Esto puede atribuirse a que estas 
estructuras presentan un menor número de métodos en su API, y sus invariantes son relativamente simples en comparación con otras 
clases evaluadas.

Para la estructura \emph{Lits}, la función de valoración basada en el generador exhaustivo (\emph{GE}) tiende a incluir al menos un 
método adicional. Este fenómeno se debe a cómo el generador maneja los valores iniciales de los arreglos internos en la clase 
\emph{List}. En particular, \emph{Lits} forma parte de una biblioteca Java utilizada para resolver problemas de satisfacibilidad 
booleana (SAT), y contiene colecciones que almacenan literales. Algunos métodos permiten crear o modificar arreglos con literales, 
pero el generador exhaustivo, sujeto al \emph{scope} limitado y a los valores iniciales, no logra producir ciertos arreglos sin 
utilizar métodos adicionales. Es importante aclarar que esto no implica que dichos métodos adicionales sean verdaderos generadores, 
sino que bajo las restricciones actuales (como el \emph{scope} configurado), no existe otra vía para construir determinadas 
estructuras sin agregarlos. Si se incrementa el \emph{scope} o se modifica la clase para exponer mejor sus constructores, entonces 
se logra encontrar el conjunto mínimo real. En contraste, la función \emph{RC} encuentra siempre el conjunto mínimo, dado que su 
evaluación no depende del \emph{scope}, sino de la cobertura lograda en ejecución.

En las estructuras de \emph{java.util}, como \emph{TreeMap}, \emph{TreeSet}, \emph{HashMap}, \emph{HashSet} y \emph{LinkedList}, 
el comportamiento de los algoritmos varía según el tamaño y complejidad de la API. El algoritmo genético evaluado con la función 
\emph{RC} es más sensible a los parámetros y al tiempo de ejecución, y no siempre logra encontrar el conjunto mínimo debido a la 
combinatoria involucrada. En la mayoría de estas estructuras, la función \emph{GE} permite encontrar el conjunto mínimo en casi 
todas las ejecuciones, con la excepción de \emph{HashMap}, donde suele incluirse un método adicional necesario para construir una 
estructura particular que no es alcanzable con los otros métodos bajo el \emph{scope} configurado. Finalmente, como aún queda por 
afinar el análisis de cobertura alcanzada con Randoop, es posible que en ciertos casos esta función de valoración tampoco logre 
guiar al algoritmo hacia un conjunto suficiente de constructores debido a una saturación prematura de cobertura en los primeros 
métodos explorados.

En resumen, la precisión de los algoritmos depende tanto de la estructura de datos bajo análisis como de la función de valoración 
utilizada. Mientras que en muchos casos se logra identificar el conjunto mínimo exacto de métodos \emph{builders}, en otros la 
limitación impuesta por los parámetros de generación, el \emph{scope} o la aleatoriedad introducida por las herramientas utilizadas 
pueden conducir a la inclusión de métodos adicionales. Estas observaciones destacan la importancia de elegir cuidadosamente tanto 
el algoritmo como la función de evaluación en función del dominio y la clase objetivo.

\input{tables/eficaciaBuilders.tex}

\subsection{RQ3: Impacto de los generadores de objetos en el análisis de
software}

\section{Generación exhaustiva acotada basada en la API}
\label{sec:experimentalBeapi}

En la evaluación de BEAPI buscaremos responder a las siguientes preguntas de
investigación:

\begin{itemize}
\item \emph{RQ4}: ¿Qué tan eficiente es BEAPI para la generación exhaustiva                                                                                       
    acotada de objetos?
\item\emph{RQ5}: ¿Cuál es el impacto de las optimizaciones propuestas en el
    rendimiento de BEAPI?
\item\emph{RQ6}: ¿Puede BEAPI ayudar a encontrar discrepancias entre
    especificaciones de invariantes de clase (repOK) y la capacidad de generación de objetos de la API?
\item\emph{RQ7}: ¿Qué tan efectivos son los objetos producidos por BEAPI para el
    testing parametrizado?  
\end{itemize}

\subsection{Configuración experimental}

Casos de estudio, técnicas a evaluar, métricas, parámetros usados, ground truth, etc..



\subsection{RQ4: Eficiencia de BEAPI}



\subsection{RQ5: Optimizaciones de BEAPI}



\subsection{RQ6: BEAPI para analizar invariantes de representación}



\subsection{RQ7: Usando BEAPI para el testing parametrizado}












\cacho{Esto es viejo}

\cacho{ya esta en el capitulo, lo movi para alla
}
Para implementar el algoritmo genético, utilizamos una biblioteca muy popular en Java llamada \emph{Jenetics}\footnote{https://jenetics.io/}. Esta biblioteca está diseñada específicamente para algoritmos evolutivos y nos proporcionó las herramientas necesarias para desarrollar nuestro enfoque genético.

\emph{Jenetics} es una biblioteca robusta y versátil que ofrece una amplia gama de funcionalidades para la implementación de algoritmos genéticos. Nos permitió definir y manipular genes, cromosomas y poblaciones, así como utilizar operadores genéticos como selección, cruzamiento y mutación. Además, cuenta con un sólido conjunto de herramientas de optimización y técnicas de evolución que nos permitieron adaptar el algoritmo a nuestras necesidades específicas.
Gracias a \emph{Jenetics}, pudimos implementar el algoritmo genético de manera eficiente y efectiva, lo que nos permitió explorar y encontrar subconjuntos óptimos de métodos \emph{builders} para las diferentes estructuras de datos en nuestro estudio. Además, \emph{Jenetics} es muy fácil de usar, con una documentación completa y una comunidad activa de usuarios que proporciona soporte y ayuda. 



\subsection{Efectividad}
\cacho{Poner que beapi va hasta el scope o haste el tiempo dado. Pero casi siempre es scope porq es pequeno}

La efectividad de nuestros algoritmos se basa en qué tan cerca están de encontrar el conjunto mínimo de métodos \emph{builders} para las diferentes estructuras de datos. Es importante tener en cuenta que siempre encontramos al menos un subconjunto de métodos \emph{builders} que es suficiente para su utilización. Sin embargo, cuanto más mínimo sea este subconjunto, mejores resultados obtendremos en términos de generación de inputs para testing.

Realizamos experimentos utilizando las estructuras de datos especificadas en nuestra primera sección de este capítulo. Evaluamos tres algoritmos: Hill Climbing (\emph{HC}). Para cada algoritmo, utilizamos dos funciones de valoración: una basada en el generador exhaustivo (\emph{BE}) y otra basada en la cobertura de ramas (\emph{RC}).
Es importante destacar que la función de valoración que utiliza la test suite generada por Randoop, la cual hemos modificado para medir la cobertura de ramas, produce resultados variables según la semilla utilizada. La semilla tiene importancia en la aleatoriedad que posee Randoop, lo que puede afectar los resultados de la función de valoración.

Para la estructura \emph{NCL}, el algoritmo \emph{HC} con la función de valoración que cuenta objectos de nuestro generador exhaustivo, \emph{BE} encuentra un subconjunto de métodos \emph{builders} que coincide con el conjunto mínimo, es decir, no agrega métodos adicionales. Sin embargo, al utilizar la función de valoración \emph{RC}, el algoritmo \emph{HC} encuentra un subconjunto con 3.40 numeros de métodos en promedio. Esto indica que en 2 de las 5 ejecuciones se agrega un método extra.
El algoritmo evolutivo, en las 5 ejecuciones, encuentra un método de más con la función de valoración que utiliza cobertura, mientras que la función de valoración con el generador exhaustivo en 4 de las 5 ejecuciones encuentra un método de más. Esto se debe a que el algoritmo genético es muy sensible a los parámetros que se utiliza y puede depender de cada caso. Si se le da el tiempo, los parámetros y las evoluciones necesarias, este algoritmo siempre encontraría el mínimo sin ningún método extra. Así mismo con la función de valoración, tiene un método de más en todas las ejecuciones.


En el caso de las estructuras \emph{UnionFind, FibonacciHeap, BTree, BinomialHeap y Scheduler}, todos los algoritmos logran encontrar el conjunto mínimo de métodos \emph{builders}, ya que no se encuentran métodos adicionales en ninguno de los casos. Esto también tiene que ver con que tienen menos cantidad de métodos en sus API y la complejidad de los mismos es diferente al resto de las clases que forman parte de nuestro benchmarks.

En la estructura \emph{Lits}, la función de valoración basada en el generador exhaustivo siempre encuentra al menos un método adicional. Esto se debe a que el generador exhaustivo depende de los valores con los que están inicializados los arreglos en la API de List, lo que puede generar un método adicional en el conjunto mínimo de métodos \emph{builders}. 
En la API de \emph{List}, la cual, es una biblioteca Java para resolver problemas booleanos de satisfacibilidad (SAT). Esta contiene una colección de arreglos que almacena los literales utilizados en un problema SAT. Proporciona métodos para manipular y acceder a literales, como agregar literales a la estructura, recuperar literales por sus índices, negar literales, etc. Nuestro generador exhaustivo depende de con que valores estén inicializados estos arreglos, y el scope que le hemos pasado. Es por esto que agrega un método para crear arreglos que no tiene posibilidad de hacerlo con los métodos builders y el scope que se le dio. Esto no quiere decir que sea builders el método que se agrega, solo que con el scope dado, no tiene posibilidad de crearlo con métodos alternativos. Si aumentamos el scope y modificamos la clase, nos encuentra siempre el subconjunto mínimo.
En cambio, la función de valoración basada en la cobertura de ramas no depende del scope dado y nos encuentra siempre el mismo subconjunto de métodos que es el mínimo y suficiente.

Para las estructuras de \emph{java.util}, el algoritmo evolutivo evaluado a través de la cobertura de ramas depende mucho del tiempo y los parámetros utilizados. Esto se debe a que estas estructuras tienen una mayor cantidad de métodos en su API, lo que hace más desafiante encontrar el conjunto mínimo de métodos \emph{builders}. Por un lado, la función de valoración basada en el generador exhaustivo encuentra el conjunto mínimo en la mayoría de las ejecuciones, con la excepción de HashSet, que agrega un método adicional que es un constructor de la clase que permite generar estructuras que no son posibles con los otros métodos hasta el scope dado.
Por otro lado, la función de valoración de de randoop \cacho{TODO, puede no encontrar el suficiente porq satura coverage.}


En resumen, la efectividad de nuestros algoritmos varía dependiendo de la estructura de datos y la función de valoración utilizada. Algunos algoritmos y funciones de valoración logran encontrar el conjunto mínimo de métodos \emph{builders}, mientras que otros pueden agregar métodos adicionales en ciertos casos.


% La efectividad de nuestros algoritmos se basa en qué tan cerca están de encontrar el conjunto mínimo de métodos \emph{builders}. Es importante destacar que nuestros algoritmos siempre encuentran al menos un subconjunto de métodos \emph{builders} que es suficiente. Sin embargo, cuanto más mínimo sea este subconjunto, mejores resultados obtendremos al utilizarlo, por ejemplo, durante la generación de inputs para testing.

\input{tables/eficaciaBuilders}


% Además de analizar la eficiencia, también investigamos la sensibilidad de algunos parámetros del GA. Ajustamos parámetros como el tamaño de la población, la tasa de mutación y el número de generaciones, y observamos cómo estos cambios afectaron el rendimiento del algoritmo y los resultados obtenidos. Esto nos permitió identificar las configuraciones óptimas de los parámetros y entender cómo influyen en el proceso de búsqueda.

Adicionalmente, examinamos los usos de los builders generados por nuestra técnica. Analizamos las características y las funcionalidades de los builders aprendidos y evaluamos su utilidad en diferentes tareas. Estudiamos cómo los builders pueden ser aplicados en el análisis y la manipulación de datos, y evaluamos su efectividad en términos de rendimiento y calidad de los resultados obtenidos.



% \paragraph{Randoop Objects}
% \label{sec:randoopObjectsExp}
% \paragraph{Bounded Exhuastive}
% \label{sec:BEExp}



% \subsubsection{Variacion de acuerdo a los Parametros}

% Ademas, examinamos la sensibilidad de los parámetros del Algoritmo Genético y exploramos los usos de los builders generados. Los resultados obtenidos proporcionaron información valiosa sobre el rendimiento y la utilidad de nuestra técnica en la generación de builders y su aplicación en diversas tareas.

% Nuestra comparacion se basa en medir la cantidad de tiempo que le lleva a cada Algortimo terminar la ejecucio, en la cantidad de candidatos que evalua la funcion de valoracion y cuan bueno es en eficacia para encontrar el minimo y suficiente subconjunto de metodos que pudimos observar en nuestro ground truth \cacho{Agregar seccion}. Ejecutamos el algoritmo 10 veces con el resto de los parametros que no esta en evaluacion con un valor promedio (Crossover=0.5, mutation 0.1, Tournanament 4).
% Tambien utilizamos 30 segundos para la fitness con randoop y scope 6 para BEAPI.

% En la tabla \ref{tab:CrossOverGA} se puede observar como se comporta el algoritmo cuando se utiliza diferente rate para el operador de CrossOver. 





% \input{tables/tableMutation}
\subsection{Uso de Builders para generar inputs de programas}
En esta parte de la evaluación nos referimos a cuán útiles son los métodos \emph{builders} identificados en el contexto de un análisis de programa, en particular, la generación automatizada de casos de prueba. Estos objetos podrían utilizarse, por ejemplo, como entradas en pruebas unitarias parametrizadas. Para los estudios de caso que proporcionan mecanismos para medir el tamaño de los objetos y comparar objetos por igualdad (es decir, los métodos size y equals de las estructuras de datos), generamos pruebas con Randoop utilizando todos los métodos disponibles en la API (API), y luego generamos pruebas con Randoop utilizando solo los métodos \emph{builders} (BLD) identificados por nuestro enfoque en el experimento anterior (Tabla \ref{tab:results-compute-bld}). Luego, comparamos el número de objetos diferentes (No. of Objs.) y el tamaño del objeto más grande (Max Obj. Size) creado por las pruebas generadas a partir de la API, en comparación con las pruebas generadas utilizando solo los métodos de BLD. Configuramos tres presupuestos diferentes para la generación de pruebas: 60, 120 y 180 segundos (Budget). Los resultados se resumen en la Tabla \ref{tab:results-obj}. Los resultados muestran que, en el mismo presupuesto de pruebas, BLD genera en promedio un 500 porciento más de objetos que API. En todos los casos, BLD también genera objetos significativamente más grandes que API. A la luz de estos resultados, queda claro que la identificación automatizada de \emph{builders} vale la pena para la generación automatizada de estructuras para clases con estado.
\input{tables/objectsBuilders}


\subsection{Uso de Builders en Verificación}

En el ultimo experimento sobre la identificación de \emph{builders}, utilicé Java PathFinder \cite{Visser:2005} (JPF) para realizar pruebas de generación de entradas de software para estructuras de datos de \emph{java.util}. JPF \footnote{https://github.com/javapathfinder/jpf-core}  es un verificador de modelos de estado explícito para programas escritos en Java. Para realizar la verificación, las técnicas de versificación de modelos de software se basan en la definición de controladores de métodos: combinaciones de métodos que permiten construir las entradas con las que se ejecutará el programa. Intuitivamente, es deseable seleccionar el menor conjunto de métodos posible, cuyas combinaciones permitan construir todas las estructuras acotadas para el módulo (para analizar el software con todas las entradas posibles). La dificultad de escribir controladores de pruebas es un obstáculo importante para el uso de un verificador de modelos. Esta selección de métodos, que generalmente se realiza manualmente, no es una tarea fácil: requiere un análisis exhaustivo de las rutinas disponibles en el módulo y una comprensión profunda de su semántica.
Es posible construir un método no determinista (harness de test) que genere todas las secuencias de llamadas a métodos de la API hasta un tamaño especificado por el usuario (scope). JPF se utiliza para enumerar todas estas secuencias. JPF almacena todos los estados explorados y retrocede cuando visita un estado previamente explorado.

JPF admite anotaciones de programa que se agregan a los programas a través de llamadas a métodos de una clase especial Verify.
Utilizamos los siguientes métodos de la biblioteca JPF \verb"Verify":
\vspace{5pt} 

\begin{itemize}
\item El método \verb"Verify.getInt(int lo, int hi)" devuelve un valor entre \verb"lo" y \verb"hi", inclusive. Crea un punto de elección no determinista: JPF necesita explorar las ejecuciones para todos los valores en el rango.
\item \verb"random(int n)" devuelve valores de 0 a \verb"n", de manera no determinista.
\end{itemize}

Si se desea verificar que el método \emph{put}  de TreeMap cumple con el repOK (predicado imperativo que verifica las invariantes de clase) de la estructura de datos, es necesario escribir algo como: 
\vspace{5pt} 


\begin{lstlisting}[caption={Probando el método put de TreeMap con JPF},label={lst:label},language=Java,captionpos=b]
public static void main(String[] args) {
   int scope = 3;
   TreeMap t = generateStructure(scope);
   t.put(Verify.getInt(0,scope),Verify.getInt(0,scope));
   assert t.repOK();
}
\end{lstlisting}

Para realizar el análisis de esta propiedad, es necesario proporcionar a JPF los mecanismos para generar todo el árbol de entrada (\textit{generateStructure}). 
En el siguiente ejemplo, mostramos un controlador de prueba construido con todos los métodos de la estructura de datos \textit{TreeMap}:
\vspace{5pt} 

\begin{lstlisting}[caption={Controlador con todos los métodos},label={lst:driverAPI},language=Java,captionpos=b]
private static TreeMap generateStructure(int scope) {
   int maxLength = Verify.getInt(0, scope);
   TreeMap t = new TreeMap();
   for (int i = 1; i <= maxLength; i++) {
      switch (Verify.random(n_methods)) {
         case 0:
            t.put(Verify.getInt(0,scope),Verify.getInt(0,scope));
            break;
         case 1:
            t.remove(Verify.getInt(0,scope));
            break;						
         case 2:
            t.clear();
            break;
         case 3:
            t.containsValue(Verify.getInt(0,scope));
            break;
         ...
         case 11: 
            t.putAll(l);
            break;
      }
   }
   return t;
}
\end{lstlisting}

El método controlador anterior, en primer lugar, selecciona el número de métodos a ejecutar, \textit{maxLength}, un número entre 0 y \textit{scope}. (Linea 2). Cada iteración del ciclo (Linea 4 a 23) corresponde a la ejecución de un solo método, seleccionado de manera no determinista entre todos los disponibles. En el caso de que el usuario no conozca el conjunto de métodos builders (y no quiera hacer el complicado trabajo de seleccionarlos manualmente), la solución más segura para evitar descartar métodos importantes es utilizar todos los métodos disponibles en el módulo, como se muestra en el método controlador descrito anteriormente. En el cuerpo del bucle, a cada método se le asigna un número entero entre 0 y \textit{n\_methods}. Se elige de manera no determinista el método a ejecutar en el ciclo actual. Por ejemplo, si \textit{n\_methods}=1, se ejecuta el método \textit{remove}. Es fácil ver que el número de ejecuciones posibles que se deben explorar por JPF crece exponencialmente con el número de métodos disponibles.

%(for every method that is executed in one iteration, there are \textit{n_methods} possible methods to execute in the next iteration).

Para evitar este crecimiento exponencial en este experimento, se propone utilizar únicamente los constructores detectados por nuestro enfoque explicado en la sección \ref{sec:builders}.
\vspace{5pt} 

\begin{figure}

\begin{lstlisting}[caption={Controlador con métodos constructores},label={lst:driverBLD},language=Java,captionpos=b]
private static TreeMap generateStructure(int scope) {
   int maxLength = Verify.getInt(0,scope);
   TreeMap t = new TreeMap();
   for (int i = 1; i <= maxLength; i++) {
      switch (Verify.random(11)) {
         case 0:
            t.put(Verify.getInt(0,scope),Verify.getInt(0,scope));
            break;
         case 1:
            t.remove(Verify.getInt(0,scope));
            break;						
      }
   }
   return t;
}
\end{lstlisting}
\end{figure}

Como se muestra en el controlador anterior, solo 2 métodos componen un conjunto mínimo y suficiente para construir controladores para TreeMap de \textit{java.util}. Utilizando solo esos métodos, generamos exactamente los mismos objetos TreeMap que antes (porque los constructores son suficientes y mínimos), y por lo tanto JPF explora las mismas ejecuciones de propiedades con ambos controladores.

Los resultados de la tabla \ref{tab:results-jpf1} y \ref{tab:results-jpf2} muestran que la construcción del controlador a partir de nuestro enfoque presentado permite aumentar la eficiencia y escalabilidad a estructuras más grandes en el análisis utilizando JPF. Esto se debe a la reducción de los métodos utilizados en los controladores (evitando métodos superfluos) y manteniendo la capacidad de construir todos los objetos acotados posibles (debido a la suficiencia de los métodos elegidos) en menos tiempo.

\input{tables/tableJPFBuilders}
\input{tables/tableJPFBuilders1}


\section{BEAPI}
En esta sección, evaluamos experimentalmente \textsf{BEAPI} en comparación con enfoques relacionados. La evaluación se organiza en torno a las siguientes preguntas de investigación:

\begin{description}
\item[RQ1] \emph{¿Se puede realizar una generación exhaustiva acotada de manera eficiente utilizando rutinas de la API?}
\item[RQ2] \emph{¿Cuánto impactan las optimizaciones propuestas en el rendimiento de la generación exhaustiva acotada a través de la API?}
\item[RQ3] \emph{¿Puede \textsf{BEAPI} ayudar a encontrar discrepancias entre las especificaciones \texttt{repOK} y la capacidad de generación de objetos de la API?}
\end{description}

Como estudios de caso, utilizamos implementaciones de estructuras de datos de cuatro bancos de pruebas: tres empleados en la evaluación de herramientas de prueba existentes (\textsf{Korat} \cite{Boyapati02}, \textsf{Kiasan} \cite{Deng06}, \textsf{FAJITA} \cite{Abad13}) y \textsf{ROOPS}. Estos bancos de pruebas cubren implementaciones diversas de estructuras de datos complejas, que son un buen objetivo para la generación exhaustiva acotada. Elegimos estos como estudios de caso porque las implementaciones vienen equipadas con \texttt{repOK}s escritos por los autores de los bancos de pruebas. Los experimentos se ejecutaron en una estación de trabajo con un procesador Intel Core i7-8700 (3.2 GHz) y 16 GB de RAM. Establecimos un tiempo de espera de 60 minutos para cada ejecución individual. Para replicar los experimentos, remitimos al lector al artefacto del artículo \cite{artifact}.
\subsection{Eficiencia}

En esta sección, evaluamos experimentalmente \textsf{BEAPI} en comparación con enfoques relacionados. La evaluación se organiza en torno a las siguientes preguntas de investigación:
\begin{description}
\item[RQ1] \emph{¿Puede realizarse BEG de manera eficiente utilizando rutinas de API?}
\item[RQ2] \emph{¿Cuánto impactan las optimizaciones propuestas en el rendimiento de BEG a partir de la API?}
\item[RQ3] \emph{¿Puede \textsf{BEAPI} ayudar a encontrar discrepancias entre las especificaciones \texttt{repOK} y la capacidad de generación de objetos de la API?}
\end{description}

Como casos de estudio, utilizamos implementaciones de estructuras de datos de cuatro bancos de pruebas: tres empleados en la evaluación de herramientas de prueba existentes (\textsf{Korat} \cite{Boyapati02}, \textsf{Kiasan} \cite{Deng06}, \textsf{FAJITA} \cite{Abad13}) y \textsf{ROOPS}. Estos bancos de pruebas cubren diversas implementaciones de estructuras de datos complejas, que son un buen objetivo para BEG. Elegimos estos casos de estudio porque las implementaciones vienen equipadas con \texttt{repOK}s escritos por los autores de los bancos de pruebas. Los experimentos se ejecutaron en una estación de trabajo con un procesador Intel Core i7-8700 (3.2 GHz) y 16 GB de RAM. Establecimos un tiempo límite de 60 minutos para cada ejecución individual. Para replicar los experimentos, remitimos al lector al artefacto del artículo \cite{artifact}.

\subsection{Eficiencia de la Generación Exhaustiva Acotada a partir de APIs}\label{sec:evaluation-vs-korat}
\input{tables/tableEficienciaBEAPI}

Para la RQ1, evaluamos si \emph{BEAPI} es lo suficientemente rápido como para ser un enfoque de BEG útil, comparándolo con el enfoque de BEG popularemente utilizado en el estado del arte, \textsf{Korat} \cite{Siddiqui09}. Los resultados de la comparación se resumen en la Tabla~\ref{table:korat-beapi}. Para cada técnica, informamos los tiempos de generación (en segundos), el número de estructuras generadas y exploradas, para distintos tipos de scopes. 
Aquí mostramos una muestra representativa de los resultados (tratamos de mantener la misma proporción de casos buenos y malos para cada técnica en los datos que informamos). 
Incluimos hasta el scope exitoso más grande para cada técnica; los tiempos de ejecución para los alcances más grandes se muestran en negrita en la tabla. De esta manera, si surgen problemas de escalabilidad, se pueden identificar fácilmente. Para obtener resultados de rendimiento adecuados para \textsf{BEAPI}, probamos exhaustivamente los métodos de la API de las clases para asegurarnos de que fueran correctos para este experimento. No intentamos cambiar los \texttt{repOK}s de ninguna manera, porque eso cambiaría el rendimiento de \textsf{Korat}, y uno de nuestros objetivos aquí es evaluar el rendimiento de \textsf{Korat} utilizando \texttt{repOK}s escritos por diferentes programadores. Se esperan diferencias en las estructuras exploradas, ya que los espacios de búsqueda correspondientes para \textsf{Korat} y \textsf{BEAPI} son diferentes. Sin embargo, para el mismo caso de estudio y alcance, se espera que ambos enfoques generen la misma cantidad de estructuras válidas. Esto es cierto en la mayoría de los experimentos, con excepciones notables de dos tipos diferentes. En primer lugar, hay casos en los que \texttt{repOK} tiene errores; estos casos están sombreados en las tablas. En segundo lugar, la noción ligeramente diferente de \emph{alcance} en cada técnica puede causar discrepancias. Esto solo ocurre para Árboles Rojo-Negro (\texttt{RBT}) y en los casos de Fibonacci (\texttt{FibHeap}), que se muestran en negrita. En estos casos, ciertas estructuras de tamaño $n$ solo se pueden generar a partir de estructuras más grandes, con inserciones seguidas de eliminaciones y luego inserciones nuevamente para activar reordenamientos de equilibrio específicos. \textsf{BEAPI} descarta las secuencias generadas tan pronto como exceden el tamaño máximo de la estructura, por lo que no puede generar estas estructuras.

En cuanto al rendimiento, tenemos resultados mixtos. En el banco de pruebas de \emph{Korat}, \emph{Korat} muestra un mejor rendimiento en 4 de 6 casos. En el banco de pruebas de \textsf{FAJITA}, \emph{BEAPI} es mejor en 3 de 4 casos. En el banco de pruebas de \textsf{ROOPS}, \emph{BEAPI} es mejor en 5 de 7 casos. En el banco de pruebas de \textsf{Kiasan}, \emph{Korat} es más rápido en 6 de los 7 casos. Observamos que \emph{BEAPI} muestra un mejor rendimiento en estructuras con restricciones más restrictivas, como \texttt{RBT} y Árboles de Búsqueda Binaria (\texttt{BST}); a menudo, estos casos tienen un número menor de estructuras válidas. Los casos en los que el número de estructuras válidas crece más rápido con respecto al alcance, como listas doblemente enlazadas (\texttt{DLList}), son más adecuados para \emph{Korat}. Más estructuras significa que \emph{BEAPI} tiene que crear más secuencias de prueba en cada iteración sucesiva, lo que hace que su rendimiento sufra más en tales casos. Como era de esperar, la forma en que se escriben los \texttt{repOK}s tiene un impacto significativo en el rendimiento de \emph{Korat}. Por ejemplo, para heaps binomiales (\texttt{BinHeap}), \emph{Korat} alcanza el alcance 8 con el \texttt{repOK} de \texttt{Roops}, el alcance 10 con el \texttt{repOK} de \texttt{FAJITA} y el alcance 11 con el \texttt{repOK} de \emph{Korat} (todos equivalentes en términos de estructuras generadas). En la mayoría de los casos, los \texttt{repOK}s del banco de pruebas de \emph{Korat} resultan en un mejor rendimiento, ya que están ajustados para su uso con \emph{Korat}. Los casos de estudio con errores en los \texttt{repOK}s están sombreados en la tabla y se discuten más en la Sección~\ref{sec:existing-specs-analysis}. Tenga en cuenta que los errores en los \texttt{repOK}s pueden afectar gravemente el rendimiento de \emph{Korat}.

\subsection{Impacto de las Optimizaciones realizas en BEAPI}
En la RQ2 evaluamos el impacto que cada una de las optimizaciones propuestas de \textsf{BEAPI} tiene en la generación exhaustiva acotada. Para esto, evaluamos el rendimiento de cuatro configuraciones diferentes de \emph{BEAPI}: \textsf{SM/BLD} es \emph{BEAPI} con coincidencia de estados (SM) y identificación de constructores (BLD) habilitados; \textsf{SM} es \emph{BEAPI} con solo coincidencia de estados (SM) habilitada; \textsf{BLD} es \emph{BEAPI} con solo identificación de constructores (BLD) habilitada; \textsf{NoOPT} tiene ambas optimizaciones deshabilitadas. En esta sección, solo mostraremos como afectan las optimizaciones de \emph{BEAPI} para el benchmark \texttt{Real World}; son cinco implementaciones "reales" de estructuras de datos: \texttt{LinkedList} (67 métodos de la API), \texttt{TreeSet} (34 métodos), \texttt{TreeMap} (61 métodos) y \texttt{HashMap} (45 métodos) de \texttt{java.util}, y \texttt{NCL} de Apache Collections (34 métodos).
La Tabla~\ref{tab:results-realWorld} resume los resultados de este experimento.
Vale aclarar, que hemos realizado el mismo experimento para el resto de los benchamarks utilizado en esta capitulo, pero por simplicidad para la lectura del lector, solo incluimos este benchmarks que es representativo para el analisis deseado.

El enfoque de fuerza bruta (\textsf{NoOPT}) tiene un rendimiento deficiente incluso para los estudios de caso más fáciles y con scopes muy pequeños. Estos scopes son demasiado pequeños y a menudo insuficientes si se desea generar conjuntos de pruebas de alta calidad. La coincidencia de estados es la optimización más impactante, mejorando considerablemente por sí sola el rendimiento y la escalabilidad en general (comparar los resultados de \textsf{NoOPT} y \textsf{SM}). Como se esperaba, la identificación de constructores es mucho más relevante en los casos en que el número de métodos en la API es grande (más de 10), y especialmente en las estructuras de datos del mundo real (con 34 o más métodos de la API). En las estructuras de datos del mundo real, el uso de metodos builders precalculados permitió que \textsf{SM/BLD} se escalara a ámbitos significativamente más grandes en todos los casos, excepto en \texttt{TreeMap} y \texttt{TreeSet}, donde mejoró significativamente los tiempos de ejecución. En general, las optimizaciones propuestas tienen un impacto crucial en el rendimiento y la escalabilidad de \emph{BEAPI}, y ambas deben habilitarse para obtener buenos resultados.

Aqui podemos concluir que es muy importante activar las optimizaciones que hemos desarollado, para que \emph{BEAPI} sea escalable a scopes mas altos.
Sin estas optimizaciones, (solo con fuerza bruta) se observa la problemática que tienen las técnicas de generación exhaustiva para poder escalar dada la gran cantidad de combinaciones de métodos para poder generar secuencias.

\emph{Sobre el costo de la identificación de métodos builders.} 

\input{tables/optimizaciones}

\subsubsection{Uso de Builders en BEAPI}
Para las conclusiones de esta sección, es suficiente decir que se empleó el ámbito 5 para la identificación de métodos builders en todos los casos, y que el tiempo de ejecución máximo del enfoque fue de 132 segundos en las estructuras de datos del mundo real (\texttt{TreeMap}, 61 métodos). Verificamos manualmente que los métodos identificados incluyeran un conjunto suficiente de constructores en todos los casos. Ten en cuenta que BEG se realiza a menudo para ámbitos cada vez más grandes, y los métodos builders identificados se pueden reutilizar en diferentes ejecuciones. Por lo tanto, los tiempos de identificación de constructores se amortizan en diferentes ejecuciones, lo que dificulta calcular cuánto tiempo de identificación de constructores se agrega a los tiempos de ejecución de \textsf{BEAPI} en cada caso. Por lo tanto, no incluimos los tiempos de identificación de metodos builders en los tiempos de ejecución de \textsf{BEAPI} en ninguno de los experimentos. Observa que, para los scopes más grandes, que son los más importantes, el tiempo de identificación de constructores es insignificante en relación con los tiempos de generación.
Estos resultados de identificación de métodos builders, se pueden observar con detalles en la sección \ref{sec:buildersExp}.



\subsection{Uso de BEAPI para analizar especificaciones}
\label{sec:existing-specs-analysis}
\input{tables/bugRepOk}
La RQ3 aborda si \textsf{BEAPI} puede ser útil para ayudar al usuario a encontrar fallas en \texttt{repOK}s, mediante la comparación del conjunto de objetos que se pueden generar utilizando la API y el conjunto de objetos generados a partir de utilizar un invariante, como es el caso del \texttt{repOK}. Diseñamos el siguiente procedimiento automatizado. Primero, ejecutamos \emph{BEAPI} para generar un conjunto, \texttt{SA}, de estructuras a partir de la API, y utilizamos \emph{Korat} para generar un conjunto, \texttt{SR}, a partir de \texttt{repOK}, utilizando el mismo ámbito para ambas herramientas. En segundo lugar, canonizamos las estructuras tanto en \texttt{SA} como en \texttt{SR} utilizando la linearización (Sección \ref{sec:stateMatching}). En tercer lugar, comparamos los conjuntos \texttt{SA} y \texttt{SR} en cuanto a igualdad. Las diferencias en esta comparación señalan una discrepancia entre \texttt{repOK} y la API. Existen tres posibles resultados para este procedimiento automatizado. Si \texttt{SA} $\subset$ \texttt{SR}, es posible que la API genere un subconjunto de las estructuras válidas, que \texttt{repOK} sufra de subespecificación (\texttt(under)) (restricciones faltantes), o ambos. En este caso, las estructuras en \texttt{SR} que no pertenecen a \texttt{SA} son evidencia del problema, y el usuario debe analizarlas manualmente para descubrir dónde está el error. Aquí, informamos los errores de subespecificación (confirmados manualmente) en \emph{repOK}s que son evidenciados por las estructuras mencionadas. En contraste, cuando \texttt{SR} $\subset$ \texttt{SA}, puede ser el caso de que la API genere un superconjunto de las estructuras válidas, que \texttt{repOK} sufra de sobreespecificación, \texttt(over), (\texttt{repOK} es demasiado restrictivo), o ambos. Las estructuras en \texttt{SA} que no pertenecen a \texttt{SR} podrían indicar la raíz del error, y nuevamente deben ser analizadas manualmente por el usuario. Informamos los errores de sobreespecificación (confirmados manualmente) en \texttt{repOK}s que son evidenciados por estas estructuras. Finalmente, puede darse el caso de que haya estructuras en \texttt{SR} que no pertenecen a \texttt{SA}, y que haya estructuras (distintas de las anteriores) en \texttt{SA} que no pertenecen a \texttt{SR}. Estos pueden ser debidos a fallos en la API, fallas en \texttt{repOK}, o ambos. Informamos las fallas confirmadas manualmente en \texttt{repOK}s que son evidenciadas por tales estructuras simplemente como errores (\texttt{repOK} describe un conjunto de estructuras diferente al que debería). Observa que las diferencias en las definiciones de ámbito de los enfoques pueden hacer que los conjuntos \texttt{SA} y \texttt{SR} difieran. Esto solo fue el caso en las estructuras \texttt{RBT} y \texttt{FibHeap}, donde \textsf{BEAPI} generó un conjunto más pequeño de estructuras para el mismo ámbito que \textsf{Korat} debido a restricciones de balance (como se explica en la Sección \ref{sec:evaluation-vs-korat}). Sin embargo, estos "falsos positivos" se pueden revelar fácilmente, ya que todas las estructuras generadas por \textsf{Korat} siempre estuvieron incluidas en las estructuras generadas por \textsf{BEAPI} si se utilizaba un ámbito más amplio para este último enfoque. Utilizando esta información, descartamos manualmente los "falsos positivos" debido a las diferencias de ámbito en \texttt{RBT} y \texttt{FibHeap}.

Los resultados de este experimento se resumen en la Tabla \ref{table:bugs}. Encontramos fallas en 9 de 26 \texttt{repOK}s utilizando el enfoque descrito anteriormente. El alto número de fallas descubiertas evidencia que los problemas en \texttt{repOK}s son difíciles de encontrar manualmente, y que \textsf{BEAPI} puede ser de gran ayuda para esta tarea.

\cacho{Agregar mas conclusiones}
\subsection{Comparativa de BEAPI con otras tecnicas de generacion de test}

\hspace{1cm}

En esta etapa de la evaluación, se llevó a cabo un análisis exhaustivo para determinar la utilidad de los métodos builders identificados en el contexto del análisis de programas, específicamente en la generación automatizada de casos de prueba. Estos builders se consideran objetos clave que pueden ser utilizados como entradas en test parametrizadas.

Los test parametrizados son una técnica utilizada en el campo de la generación automatizada de casos de prueba para aumentar la eficiencia y la cobertura de las pruebas. En lugar de escribir casos de prueba individuales para cada escenario posible, los test parametrizados permiten definir un conjunto de parámetros que se utilizan para generar automáticamente múltiples casos de prueba.

En el contexto de la evaluación experimental, se utilizó la técnica de test parametrizados para alimentar una test suite con objetos creados por diferentes técnicas. Esto significa que se definieron parámetros que representan diferentes características o propiedades de los objetos, y luego se generaron automáticamente casos de prueba utilizando estos parámetros.

Los test parametrizados son una técnica poderosa en la generación automatizada de casos de prueba, ya que permiten explorar diferentes combinaciones de parámetros y generar una variedad de casos de prueba de manera eficiente. En el contexto de la evaluación experimental, se utilizaron para evaluar y comparar el desempeño de diferentes técnicas en la generación de objetos y su impacto en la calidad de las pruebas.

Para llevar a cabo este experimento, se creó una test suite parametrizada que serviría como marco de prueba para evaluar los distintos enfoques. Esta test suite parametrizada fue, básicamente, crear un test por método que contiene la clase y ejercitarlos con los objetos creados por las diferentes técnicas. Se utilizaron varias técnicas y herramientas para generar los objetos necesarios. En primer lugar, se empleó la conocida herramienta \texttt{Randoop} utilizando como es su forma estander, con todos los métodos disponibles en su API, lo que da lugar a una suite de pruebas tradicional generada por Randoop. En este caso no se utiliza una test suite parametrizada, ya que \texttt{Randoop}  no genera objetos, sino que crea sus propias tests suite.

Ahora si, se utilizó una variante de Randoop llamada \texttt{R-Serialize} para serializar las secuencias de pruebas generadas anteriormente. Esto permitió generar objetos que, a su vez, se utilizaron para alimentar la test suite parametrizada, ampliando así el alcance de los casos de prueba. Estos objetos fueron generados utilizando todos los métodos de la API de Randoop, tal como se hizo en el enfoque anterior.

Otra herramienta utilizada en el experimento fue una versión modificada de \texttt{Randoop}, diseñada específicamente para utilizar únicamente los métodos builders identificados previamente mediante nuestro enfoque (\ref{cap:builders}). Los objetos generados por esta variante especial de Randoop también se serializaron y se incorporaron a la test suite parametrizada, permitiendo una comparación directa entre los objetos generados por los builders identificados y los generados sin utilizar la información de estos métodos builders.

Por último, se emplearon los objetos generados por la herramienta \texttt{BEAPI}. Estos objetos, creados utilizando su propia API, se integraron en la test suite parametrizada para evaluar su efectividad en la generación de casos de prueba. De esta manera, se obtuvo una visión completa y comparativa de las diferentes técnicas utilizadas en términos de generación de objetos y su impacto en la calidad de las pruebas.

Mediante este enfoque meticuloso y riguroso, se buscó determinar la capacidad de los builders identificados para mejorar la generación automatizada de casos de prueba y, en última instancia, contribuir a la mejora de la calidad del análisis de programas. Los resultados obtenidos en esta evaluación experimental proporcionaron información valiosa sobre la utilidad y efectividad de los builders en el contexto del análisis de programas, abriendo así nuevas oportunidades para futuras investigaciones y desarrollos en este campo.

A continuación, se realiza la comparativa de las diferentes herramientas utilizadas en el benchmark de \emph{java.util} previamente mencionado. Los casos de estudio son: \emph{HashSet}, \emph{HashMap}, \emph{TreeMap}, \emph{TreeSet} y \emph{LinkedList}. La tabla de resultados muestra varias métricas, como el tiempo en segundos (\texttt{GTime}) que es tiempo que lleva generar estos objectos/tests, la herramienta utilizada (\texttt{Tool}), la cantidad de objetos válidos generados (\texttt{Valid}) y la cantidad de objetos inválidos generados (\texttt{Invalid}).
Es importante destacar que se definió un invariante para cada clase bajo evaluación, el cual establece qué estructuras son válidas e inválidas. Todos los objetos generados fueron sometidos a la prueba de estos invariantes para determinar su validez. Sin embargo, es importante mencionar que la técnica \texttt{Randoop} genera directamente una test suite en lugar de objetos individuales, por lo que no se aplica directamente el concepto de validez e invalidez a los casos generados por esta herramienta.

Además, se realiza una comparación en función de la cantidad de tests generados por cada técnica. Esta medida se refleja en las columnas \texttt{Test}, \texttt{T(Seg)}, que indica la cantidad de tests que se ejecutan y sus respectivos segundos que son necesarios para ejecutar la test suite parametrizada, o no (en el caso de \texttt{Ranndop}), correspondiente.

Finalmente, se evalúa la calidad de las test suites generadas mediante la comparación de la cobertura de ramas y la cantidad de mutantes eliminados. Estas métricas permiten determinar qué tan efectivas son las test suites en términos de su capacidad para cubrir diferentes ramas del código y eliminar mutantes generados para introducir fallas.

A continuación analizaremos caso por caso con sus respectivas tablas.

\input{tables/randoopvsBeapi}


% En esta etapa de la evaluación, se llevó a cabo un análisis exhaustivo para determinar la utilidad de los metodos builders identificados en el contexto del análisis de programas, específicamente en la generación automatizada de casos de prueba. Estos builders se consideran objetos clave que pueden ser utilizados como entradas en test parametrizadas. 
% Los test parametrizados son una técnica utilizada en el campo de la generación automatizada de casos de prueba para aumentar la eficiencia y la cobertura de las pruebas. En lugar de escribir casos de prueba individuales para cada escenario posible, los test parametrizados permiten definir un conjunto de parámetros que se utilizan para generar automáticamente múltiples casos de prueba.
% En el contexto de la evaluación experimental, se utilizó la técnica de test parametrizados para alimentar una test suite con objetos creados por diferentes técnicas. Esto significa que se definieron parámetros que representan diferentes características o propiedades de los objetos, y luego se generaron automáticamente casos de prueba utilizando estos parámetros.
% Los test parametrizados son una técnica poderosa en la generación automatizada de casos de prueba, ya que permiten explorar diferentes combinaciones de parámetros y generar una variedad de casos de prueba de manera eficiente. En el contexto de la evaluación experimental, se utilizaron para evaluar y comparar el desempeño de diferentes técnicas en la generación de objetos y su impacto en la calidad de las pruebas.

% Para llevar a cabo este experimento, se creó una test suite parametrizada que serviría como marco de prueba para evaluar los distintos enfoques. Esta test suite parametrizada fue, basicamente, crear un test por metodos que contiene la clase y ejercitalos con los objectos creados por las distintas técnicas. Se utilizaron varias técnicas y herramientas para generar los objetos necesarios. En primer lugar, se empleó la conocida herramienta \texttt{Randoop}, utilizando todos los métodos disponibles en su API estándar, lo que dio lugar a una suite de pruebas tradicional generada por Randoop.

% Además, se utilizó una variante de Randoop llamada \texttt{R-Serialize} para serializar las secuencias de pruebas generadas anteriormente. Esto permitió generar objetos que, a su vez, se utilizaron para alimentar la test suite parametrizada, ampliando así el alcance de los casos de prueba. Estos objetos fueron generados utilizando todos los métodos de la API de Randoop, tal como se hizo en el enfoque anterior.

% Otra herramienta utilizada en el experimento fue una versión modificada de \texttt{Randoop}, diseñada específicamente para utilizar únicamente los métodos builders identificados previamente mediante nuestro enfoque (\ref{cap:builders}). Los objetos generados por esta variante especial de Randoop también se serializaron y se incorporaron a la test suite parametrizada, permitiendo una comparación directa entre los objetos generados por los builders identificados y los generados sin utilizar la informacion de estos metodos builders.

% Por último, se emplearon los objetos generados por la herramienta \texttt{BEAPI}. Estos objetos, creados utilizando su propia API, se integraron en la test suite parametrizada para evaluar su efectividad en la generación de casos de prueba. De esta manera, se obtuvo una visión completa y comparativa de las diferentes técnicas utilizadas en términos de generación de objetos y su impacto en la calidad de las pruebas.

% Mediante este enfoque meticuloso y riguroso, se buscó determinar la capacidad de los builders identificados para mejorar la generación automatizada de casos de prueba y, en última instancia, contribuir a la mejora de la calidad del análisis de programas. Los resultados obtenidos en esta evaluación experimental proporcionaron información valiosa sobre la utilidad y efectividad de los builders en el contexto del análisis de programas, abriendo así nuevas oportunidades para futuras investigaciones y desarrollos en este campo.


%  A continuacion hacemos la comparativa de las diferentes tools, explicada en el parrafo anterior, en el benchmarks de \emph{java.util} utilizado previamente. Los casos de estudios son; \emph{HashSet},\emph{HashMap},\emph{TreeMap},\emph{TreeSet}, \emph{LinkedList}. La comparacion se realiza sobre, \texttt{GTime}, que representa el tiempo en milisegundos, \texttt{Tool} indica la herramienta utilizada, \texttt{Valid} muestra la cantidad de objectos válidos, \texttt{Invalid} muestra la cantidad de objetos inválidos que son generados. Para saber que objetos son válidos e inválidos, escribimos un invariante para cada clase bajo evaluacion, y este nos dice que estructura es valida y que estructura es invalida. Puede observar que para la tecnica \texttt{Randoo} esto no cuenta, ya que recuerde que aqui utilizamos la tool \texttt{Randoop} que generan test suite directamnete y no objectos. Ademas, BEAPI siempre va a generar objectos validos, esto se debe a que como estos objectos son construidos desde la API, no hay posibilidad de generar objectos invalidos. Vale aclarar que estos objectos tambien fueron puesto bajo prueba del mismo invariante que escribimos para cada clase. 
%  Ademas, comparamos de acuerdo a la cantidad de test que cada tecnica genera. Para realizar esta medida, en las tools que utilizan la test suite parametrizada, indica la cantidad de test que se ejecutan y es un valor de que tiene relacion con los objectos que se alimentan a la test suite. En base a esto \texttt{T(Seg)} es la cantidad de segundo que lleva ejecutar esta test suite.
%  Por ultimo, comparamos que tan buena es estas test suite, comprando la cobertura de ramas y de mutantes que matan.

 \cacho{Mejorar esto:}
 
La tabla \ref{tab:hashSetTools} muestra una comparativa de diferentes técnicas aplicadas en el contexto del caso de estudio de \emph{java.util}, HashSet.
% Test indica el total de casos de prueba, T(Seg) representa el tiempo en segundos, Ramas muestra la cantidad de ramas y Mutacion indica el valor correspondiente a la mutación.

% En general, se realizaron pruebas utilizando diferentes técnicas, como Randoop, R-Serialize, R-Builders y BEAPI. Estas técnicas fueron evaluadas en diferentes intervalos de tiempo, como 5, 10, 20, 40 y 150 unidades.

Como se puede observar en la tabla \ref{tab:hashSetTools}, al utilizar los métodos builders como prioridad en Randoop, se generan significativamente más objetos (un aumento del 300%) en comparación con Randoop sin utilizar estos métodos. Por otro lado, BEAPI genera un total de 32 objetos para un scope de 5, y lo logra en menos de 5 segundos.

La cantidad de tests ejecutados es una medida de la cobertura de la test suite. En las técnicas que utilizan los objetos generados para alimentar tests parametrizados, hay una correlación directa entre estos dos números. Es importante destacar que la test suite para tests parametrizados es la misma para R-Serialize, R-Builders y BEAPI.

En cuanto a la calidad de las test suites generadas, podemos observar que Randoop no logra alcanzar altos valores de cobertura en un corto período de tiempo. Solo después de 150 segundos comienza a alcanzar coberturas similares a las otras técnicas. Esto se debe a que Randoop genera muchos tests que resultan irrelevantes o que están subsumidos en otros. Cuando utilizamos estas secuencias de tests para serializar y crear objetos que se utilizan como inputs en nuestros tests parametrizados, se puede observar una mejora en la cobertura, pero aún no alcanza los valores obtenidos con la identificación previa de los métodos builders y su posterior uso con Randoop (R-Builders). Esta técnica logra la misma cobertura que BEAPI, tanto en términos de ramas como de mutación. Aquí podemos apreciar la importancia de guiar a Randoop mediante la identificación de los métodos builders.



\cacho{Como resultados finales:}
Los resultados indican que la utilización de los métodos builders como prioridad en Randoop genera una mayor cantidad de objetos y mejora la calidad de las test suites en comparación con Randoop sin utilizar esta prioridad. Además, se destaca el desempeño de BEAPI, que logra generar un número significativo de objetos válidos en poco tiempo. Estos hallazgos respaldan la importancia de guiar la generación de casos de prueba utilizando información específica de la API, como la identificación de los métodos builders, para mejorar la efectividad y la cobertura de las pruebas en el contexto del software testing.





