%!TEX root = main.tex
\chapter[Evaluaci\'on]{Evaluaci\'on}
\label{cap:experimental}


En los capítulos \ref{cap:builders} y \ref{cap:beapi} presentamos enfoques para
identificar automáticamente un conjunto de métodos generadores de objetos, y una
técnica novedosa de generación exhaustiva acotada basada en la API. 
En este capítulo, realizamos una evaluación experimental de las técnicas
mencionadas. La sección \ref{sec:experimentalIdentificacion} analiza experimentalmente los
algoritmos de identificación de generadores de objetos, mientras que en la
sección \ref{sec:experimentalBeapi} se evalúa la generación exhaustiva acotada basada en la API.


\section{Algoritmos de identificación de métodos generadores de objetos}
\label{sec:experimentalIdentificacion}

En esta sección, evaluamos experimentalmente los enfoques presentados en el
Capítulo~\ref{cap:builders}. Analizaremos la eficiencia y precisión de cada
algoritmo utilizando las funciones objetivo introducidas en la
Sección~\ref{sec:fitness}. Además, mostraremos resultados preliminares sobre cómo los 
métodos generadores de objetos pueden ser aprovechados por algunas herramientas
automáticas de análisis de software.

Con respecto a estas técnicas, las siguientes preguntas de investigación guían esta experimentación:

\begin{itemize}
\item \emph{RQ1}: ¿Qué tan eficientes son los algoritmos propuestos para
    identificar conjuntos de métodos generadores de objetos?
\item \emph{RQ2}: ¿Qué tan precisos son los algoritmos presentados para identificar métodos generadores de objetos?
\item \emph{RQ3}: ¿Cuál es el impacto de utilizar métodos generadores de objetos
    en el contexto del análisis automático de software?
\end{itemize}

\subsection{Configuraci\'on experimental}

\subsubsection{Casos de estudio.}


La evaluación se llevó a cabo sobre un conjunto de clases Java 
 que manipulan estructuras dinámicas complejas,
incluyendo: \verb"NCL" de Apache Commons Collections~\cite{apache};
\verb"BinaryTree", \verb"BinomialHeap" y \verb"FibonacciHeap", extraídos
de~\cite{Visser:2006}; y \verb"UnionFind", una implementación de conjuntos
disjuntos tomada de JGrapht~\cite{jgrapht}.
También se incluyeron componentes de proyectos reales de software, como 
\verb"Lits" de la implementación de Sat4j~\cite{sat4j}, utilizada previamente 
en~\cite{Loncaric:2018}. \verb"Lits" maneja la representación interna de una
fórmula en el SAT solver (variables y literales).
%, y traduce literales desde DIMACS a la representación interna.
Por otro lado, se consideraron estructuras adicionales como \verb"Scheduler",
un planificador de procesos tomado del conjunto de benchmarks SIR~\cite{sir},
y varias colecciones del paquete estándar \verb"java.util" de Java%
\footnote{\url{https://docs.oracle.com/javase/8/docs/api/java/util/
package-summary.html}}, incluyendo \verb"TreeMap", \verb"TreeSet",
\verb"HashMap",\\
 \verb"HashSet" y \verb"LinkedList". 

\input{tables/groundTruth}

Para evaluar la precisión de nuestros algoritmos, se construyó manualmente un
conjunto de referencia o \emph{ground truth} que contiene los métodos
generadores de objetos (\emph{MGO}) considerados minimales y suficientes en cada caso de estudio. Esta
tarea implicó un análisis detallado y manual de cada API, lo que representó un
trabajo considerable, especialmente en los casos más complejos. La
Tabla~\ref{tab:groundTruth} resume este conjunto de referencia, indicando para
cada clase el número total de métodos en su API pública (\#API) y los métodos
identificados manualmente como generadores de objetos. 
%Cabe destacar que en algunos
%casos ciertos métodos pueden ser intercambiables (por ejemplo,
%\texttt{addFirst} y \texttt{addLast} en \texttt{NCL}).


\subsubsection{Algoritmos evaluados.}

\begin{itemize}
  \item \textbf{GA}: el algoritmo genético descrito en la Sección~\ref{alg:approachGA}.
  \item \textbf{HC}: el algoritmo \emph{hill-climbing} presentado en la Sección \ref{alg:approachHC}.
\end{itemize}

Propusimos dos funciones objetivo para evaluar cada conjunto candidato de
métodos: 

\begin{itemize}
    \item \textbf{FGE}: función objetivo basada en la cantidad de objetos
        producidos por un generador exhaustivo acotado, presentada en la Sección~\ref{sec:fitnessGE}.
\item \textbf{FRC}: función objetivo basada en la cobertura de código, descrita en la Sección~\ref{sec:fitnessRandoop}. 
\end{itemize}

Cuando un algoritmo se evalúa con una función objetivo específica, usamos la notación 
\textbf{Algoritmo–Fitness}. 
Esto da lugar a cuatro algoritmos a evaluar: \textbf{GA–FGE}, \textbf{GA–FRC}, \textbf{HC–FGE}, 
\textbf{HC–FRC}. Notar que el trabajo sobre cómputo de métodos generadores
de Politano et. al. \cite{Ponzio19} \pp{Tu paper de builders} presenta el algoritmo
\textbf{GA–FGE}. Las otras variantes fueron agregadas y evaluadas de forma
preliminar para esta tesis. 

Nuestros algoritmos requieren la configuración de diversos parámetros que influyen en su eficiencia y efectividad.
Estos valores se determinaron empíricamente mediante prueba y error, priorizando
la eficiencia y la precisión de las soluciones y se mantuvieron constantes en todos los experimentos.

En particular, para el algoritmo genético (\textbf{GA})
utilizamos una tasa de cruce de 0.40, una tasa de mutación de 0.05, 
selección por torneo con cuatro participantes, una población de 100 individuos y
un máximo de 20 generaciones por ejecución. 

Para las evaluaciones con la función objetivo basada en cobertura (\textbf{FRC}), 
fijamos un tiempo máximo de ejecución de Randoop de 30 segundos por candidato. 
%Este tiempo fue suficiente para generar suites de tests y lograr buena cobertura de código en nuestros experimentos.  
\pp{Esto de abajo no se entiende, tenemos que charlarlo:
\cacho{Lo cambie un poco pero no sé si está bien:}
Randoop introduce un componente de aleatoriedad en la generación de tests. 
Para controlar este factor y garantizar la reproducibilidad de los resultados de los experimentos con los mismo conjunto de parámetros, 
fijamos dos semillas distintas.
% y controlamos la aleatoriedad mediante semillas. Randoop utiliza esta semilla para controlar la generación de tests y
% permitir la reproducción de resultados. Dado que Randoop introduce un grado
% considerable de aleatoriedad, optamos por utilizar dos semillas distintas para capturar la variabilidad introducida por el 
% generador aleatorio, registrándolas para permitir la reproducción de resultados. 
% Esta configuración proporciona una medida más robusta del desempeño de los algoritmos en contextos con 
% elementos no determin\'isticos.}

En el caso de la función objetivo basada en generación exhaustiva (\textbf{FGE}),
el parámetro central es el \emph{scope}, entendido como la cantidad máxima de
objetos por clase que puede crear el generador exhaustivo, y rangos de
valores para los tipos primitivos. 
En nuestros experimentos fijamos el scope en 5. Es decir, permitimos un máximo
de cinco objetos por clase, y 5 valores posibles para tipos primitivos (ej. para enteros
usamos valores entre 0 y 4). \pp{Acá tiene que haber un TO también. Sino en
algunos casos corre el riesgo de no terminar.}
\cacho{Además, establecimos un tiempo máximo de ejecución de 30 segundos por caso (\emph{timeout}), 
para evitar que las instancias con combinaciones más grandes queden en una ejecución infinita, 
debido a la explosión combinatoria del espacio de búsqueda.}
%Además, para evitar diferencias espurias que no reflejan cambios semánticos en la representación, 
%omitimos campos internos que no afectan la estructura lógica de los objetos generados (por ejemplo, \texttt{modCount}, común en colecciones de Java), 
%el cual sólo contabiliza modificaciones internas sin alterar la estructura observada del objeto. \pp{Yo no pondría esto último porque no recuerdo que lo
%hayamos explicado en ningún lado.} \cacho{Hay un apartado pequeño, lo podemos charlar}

Para cada caso de estudio y función objetivo, el algoritmo
genético y el de \emph{Hill Climbing} fue ejecutado cinco veces con los parámetros mencionados. Los
resultados reportados en las siguientes secciones corresponden al promedio de
estas ejecuciones. En el caso particular de la función objetivo basada en
generación exhaustiva, no fue necesario realizar múltiples repeticiones, dado
que dicha evaluación es determinística bajo una misma configuración. \pp{No estoy de acuerdo con esto. El algoritmo genético es aleatorio
independientemente de la fitness que use. Todos hay que ejecutarlos múltiples
veces. Hablémoslo.}
\cacho{Otra version, no se si estoy en lo correcto:}
\cacho{
En el caso particular de la función objetivo basada en generación exhaustiva, 
consideramos que la variabilidad es mínima bajo una misma configuración, 
por lo que realizamos una única ejecución por caso. No obstante, reconocemos que la generación exhaustiva
podría presentar pequeños efectos de no-determinismo (por ejemplo, diferencias en el orden de exploración o de poda), 
que podrían influir en el tiempo de ejecución. Sin embargo, estos efectos son marginales y no afectan la precisión de los resultados, 
dado que la generación exhaustiva siempre produce el mismo conjunto de objetos para una configuración dada. }

Todos los experimentos fueron ejecutados en una máquina con procesador Intel
Core i7-6700 a 3.4\,GHz y 8\,GB de RAM, y sistema operativo
GNU/Linux.

% En términos de analizar el peor caso, con \textbf{HC+RC} se alcanza 5162s en
% \emph{TreeMap} (aprox 86 minutos) y 3842s en \emph{HashSet} para
% para \textbf{HC+GA} (aprox. 64 minutos). 
% Con \textbf{GE+GA}, se observa que el peor caso tarda 1803s (aprox 30 minutos) en \emph{HashMap} con \textbf{GE+RC} alcanza 113s para \emph{TreeMap} (aprox 2 minutos).

%Esto se debe a que los algoritmos
%de Hill Climbing comienzan desde abajo hacia arriba, considerando menos métodos antes de considerar más métodos, a diferencia de los 
%algoritmos genéticos, que generan sucesores mediante operadores de cruce y mutación. Esta estrategia favorece al algoritmo Hill Climbing 
%para encontrar los métodos mínimos en tiempos más cortos. Cabe mencionar que en casos de estudio donde el algoritmo genético supera al 
%Hill Climbing, se debe a que la clase bajo prueba tiene pocos métodos, lo que permite que los algoritmos evolutivos converjan más rápidamente que el algoritmo Hill Climbing.
%

\subsection{RQ1: Eficiencia del cómputo de métodos generadores de objetos}

\input{tables/eficienciaBuilders}

Medimos la eficiencia como el tiempo promedio (en segundos) necesario para
finalizar la búsqueda de métodos generadores a partir de la API. 
%Evaluamos los dos
%algoritmos de búsqueda (\emph{GA} y \emph{HC}) combinados con dos funciones
%objetivo: \emph{FGE} (Función objetivo con generación exhaustiva acotada) y \emph{FRC} (Función objetivo con cobertura). 
Los resultados se muestran en la Tabla~\ref{tab:eficiencia}. Cada fila
corresponde a un caso de estudio; debajo del nombre se indica el tamaño de la API
(\#API) y la cantidad de métodos generadores del \emph{ground truth}
(\#MGO). Las columnas se agrupan por algoritmo (GA y HC) y, dentro de
cada uno, por función objetivo (FGE y FRC). Para cada configuración reportamos el promedio de 5 ejecuciones.

Para comparar la eficiencia de \texttt{GA} y \texttt{HC}, utilizamos la misma 
función objetivo (\texttt{FGE} o \texttt{FRC}) y medimos el tiempo de ejecución 
promedio sobre los 12 casos de estudio. Esto da los siguientes 
resultados:
A continuación, reportamos los tiempos promedio de ejecución (sobre los 12 casos
de estudio) de los algoritmos evaluados:
\pp{Hay algunos casos en los que se supera el
timeout! NCL y TreeMap con GA+FRC}
\cacho{Chequear los experimentos!}
\begin{itemize}
  \item \textbf{GA+FGE}: 259 segundos
  \item \textbf{GA+FRC}: 1809.5 segundos
  \item \textbf{HC+FGE}: 19.6 segundos
  \item \textbf{HC+FRC}: 622.8 segundos
\end{itemize}

Estos valores muestran con claridad que \texttt{HC} es sistemáticamente más rápido
que \texttt{GA}. En particular, con \texttt{FGE} el tiempo de \texttt{HC} es 
aproximadamente 13 veces menor que el de \texttt{GA}, mientras que usando 
\texttt{FRC} es casi 3 veces menor. 
A nivel de casos
individuales, \texttt{HC} supera a \texttt{GA} en todos los casos con \texttt{FGE} y en 11 de 12 casos con \emph{FRC},
siendo \texttt{HashSet} la única excepción donde \texttt{GA+FRC} fue más veloz.
\cacho{Revisar esto, no me acuerdo bien, ver el paper} \pp{No hay razón real
para esto, suena a bug. Además, el que está mal es sólo la versión con FRC. En
el otro gana HC}.

Esta diferencia se entiende por la propia naturaleza de los algoritmos.
El algoritmo genético (\texttt{GA}) mantiene y evoluciona poblaciones de
candidatos, lo que implica evaluar la función objetivo (que es costosa
computacionalmente) para muchos candidatos.
En cambio, \texttt{HC} sigue una estrategia \emph{greedy}: elige la mejor opción posible entre los sucesores del candidato actual. Esto lo vuelve más eficiente.
Sin embargo, en ciertos casos \texttt{HC} podría perder precisión en el cómputo de métodos
generadores. 

Comparando las funciones objetivo, \texttt{FGE}
es más rápida que \texttt{FRC}. En \texttt{GA}, FRC requiere en promedio
unas siete veces más tiempo que \texttt{FGE}; en \texttt{HC}, \texttt{FRC} requiere unas casi 32 veces
más tiempo que \texttt{FGE}. La función objetivo \texttt{FGE} debido a que para
computar MGO en nuestos casos de estudio es suficiente con generar objetos de tamaño 
relativamente pequeño (scope 5). Además, \texttt{FGE} utiliza BEAPI (\cacho{ver
capítulo \ref{cap:beapi}}), que implementa mecanismos que hacen a la
generación acotada más eficiente (generación guiada por el feedback de la
ejecución, state matching). 
Ambas funciones tienen un \emph{timeout} de 
30 segundos \pp{esto va antes, cuando se presentan los algoritmos}, pero muchas veces
la generación exhaustiva acotada termina la generación de objetos antes del llegar al timeout. 
En cambio, la función objetivo FRC debe generar una gran cantidad de tests en
muchos casos para lograr una buena cobertura (notar que FRC no está guiada por
cobertura sino que es aleatoria). \pp{Hay que ver el tema seeds, porque debe ser
eso lo que la hace tan lenta.}
Esto explica porque FGE es más rápida en comparación con la función objetivo FRC
en estos experimentos.

Por otra parte, la función objetivo \texttt{FRC} tiene la ventaja de que no requiere 
que el usuario provea la definición del scope para la generación exhaustiva acotada. 
Además, \texttt{FRC} podría ser más escalable que \texttt{FGE} en casos donde
la cantidad de objetos sea demasiado grande para ser generados de manera
exhuastiva (por ejemplo, en clases con invariantes de representación más débiles
que los evaluados aquí).
De cualquier modo, los resultados preliminares muestran que los algoritmos 
son mucho más lentos con la función objetivo \texttt{FRC}. Se deja como trabajo 
futuro estudiar formas más eficientes de implementar la función \texttt{FRC}, y
realizar más experimentos para evaluar mejor su utilidad en la práctica 
\pp{No olvidarse de incluir esto en trabajos futuros}.

En conclusión, \texttt{HC+FGE} computa generadores de objetos muy rápidamente en
nuestros experimentos, en 20 segundos en promedio. \texttt{GA+FGE} y
\texttt{HC+FRC} son razonablemente rápidos para esta tarea, 
tomando en promedio poco más de 4 minutos y 10 minutos, respectivamente.
La combinación de \texttt{GA+FRC} hace que el cómputo se vuelva mucho más lento.
Como el cómputo de generadores de objetos se realiza una sola vez sobre el
código de la clase bajo análisis, previo a los análisis de código que se quieran
llevar adelante (por ejemplo, generación de
objetos para testing, etc.), creemos que \texttt{HC+FGE}, \texttt{GA+FGE}, y
\texttt{HC+FRC} son algoritmos viables para su uso en la práctica. 

\subsection{RQ2: Precisión de los algoritmos de cómputo de métodos generadores de objetos}
\label{sec:experimentalIdentificacionPrecision}

\input{tables/eficaciaBuilders.tex}

Medimos la efectividad como la capacidad de cada algoritmo para identificar
un \emph{conjunto minimal} de métodos generadores a partir de la API. 
La Tabla~\ref{tab:efectividad} resume los tamaños promedios (sobre 5
corridas) del conjunto de métodos identificado por cada algoritmo evaluado. 
Los resultados se analizaron manualmente con respecto al \emph{ground truth} 
( el número de MGOs minimales se indica como \#MGO en la tabla) y 
en todos los casos un conjunto de métodos generadores de objetos suficiente 
fue generado por todos los algoritmos. Sin embargo, como puede verse en la
tabla, algunos algoritmos incluyen algunos métodos irrelevantes en algunos casos.

Se observan cuatro patrones claros. Primero, \texttt{HC+FRC} encuentra el conjunto
\emph{minimal exacto en 12/12} sujetos. Segundo, \texttt{HC+GE} es exacto en
\emph{11/12} sujetos y sólo sobredimensiona en \texttt{Lits} (retorna 7 en lugar de
6).\cacho{Revisar esto, no me acuerdo bien, ver el paper}
Tercero, \texttt{GA+GE} es exacto en \emph{10/12} sujetos; sobredimensiona en
\emph{Lits} (7 vs.\ 6) y \texttt{HashMap} (2.60 vs.\ 2). Cuarto, \texttt{GA+FGE} es
exacto en \emph{8/12} sujetos y tiende a incluir métodos adicionales en
\texttt{NCL} (3.80 vs.\ 3), \texttt{TreeMap} (5.60 vs.\ 3), \texttt{TreeSet}
(3.20 vs.\ 3) y \texttt{HashSet} (3 vs.\ 2). \pp{Las conclusiones deberían ser
    bien simples: HC+FRC y HC+FGE dan 10 puntos en todos los casos, mientras que
    GA+FGE da razonablemente bien en la mayoría de los casos, y GA+FRC no importa 
demasiado porque es muy lento.}
\cacho{
Se observan tres patrones claros. Primero, \texttt{HC+FRC} y \texttt{HC+FGE} encuentra el conjunto de métodos generadores,
\emph{minimal}, para las doce clases analizadas.
Segundo, \texttt{GA+FGE} obtiene resultados muy cercanos al \emph{minimal} en la mayoría de los casos, 
con algun método de mas en algunas clases (\texttt{NCL}, \texttt{TreeMap}, \texttt{TreeSet}, \texttt{HashSet}).
Por último, \texttt{GA+FRC} muestra un comportamiento correcto pero con un costo computacional considerablemente mayor, 
por lo que no resulta competitivo en términos de eficiencia.
}

Estas desviaciones tienen dos causas principales. Por un lado, en \texttt{GE} la
evaluación se realiza por enumeración acotada; bajo scope fijo y valores
iniciales concretos, ciertas combinaciones pueden no ser alcanzables sin
incorporar alguna rutina adicional (como ocurre en \emph{Lits}), lo que hace que
el conjunto devuelto sea suficiente pero no minimal \pp{Es difícil de justificar 
    esto, deberíamos charlarlo. Pero para mi si hacés generación exhaustiva no
te pueden faltar cosas}. \cacho{Revisar esto, no me acuerdo bien, ver experimentos URGENTE!}
%Por otro lado, en
%\textbf{FGE} la métrica de cobertura puede favorecer métodos que
%aumentan rápido la cobertura de ramas/líneas aunque no sean estrictamente
%necesarios para construir todas las instancias, por lo que el algoritmo puede estabilizarse en soluciones
%ligeramente sobredimensionadas. PABLO: Esto no es cierto.

En el caso particular del algoritmo genético (\texttt{GA}), además de la
estocasticidad propia de la búsqueda (selección, cruce, mutación), empleamos un
criterio de corte del algoritmo que detiene la evolución cuando el mejor
fitness no mejora durante 5 generaciones consecutivas (ver
Sección~\ref{alg:approachGA}). Este criterio es eficaz para acotar el presupuesto,
pero puede congelar soluciones suficientes levemente mayores que el
mínimo cuando la presión selectiva ya no impulsa mejoras adicionales. \pp{Hay
que arreglar Lits, y TreeMap con GA+FRC que da demasiado horrible. Puede ser por
Timeout?}. \cacho{Revisar esto, no me acuerdo bien, ver experimentos URGENTE!}

En síntesis, todas las configuraciones producen conjuntos suficientes, y
\textbf{HC+FGE} además logra el mínimo exacto de forma sistemática. Cuando
el objetivo primario es minimizar el costo de generación/análisis, recomendamos
emplear el conjunto devuelto por \textbf{HC+FGE} (minimal y suficiente).


\subsection{RQ3: Impacto de los métodos generadores de objetos en el análisis de
software}

\subsubsection{Generación de objetos para testing}
\label{sec:object_builders_randoop}

\input{tables/objectsBuilders.tex}

\cacho{En esta sección evaluamos la utilidad práctica de los métodos generadores de objetos 
identificados previamente con el algoritmo de \emph{Hill Climbing} (\ref{alg:approachHC}}) 
con la función objetivo basada en la cantidad de objetos producidos por un generador exhaustivo acotado (\ref{sec:fitnessGE}), en el contexto de la
generación automática de objetos para las clases evaluadas}. 
Estos objetos pueden ser utilizadas luego como 
entradas en tests unitarios o tests basadas en propiedades.
 
Para este experimento, modificamos la herramienta \emph{Randoop} (Ver
preliminares, \ref{sec:feedback-directed-test-gen}) para que almacene los
objetos producidos por la ejecución de los tests. La evaluación consiste en
utilizar la variante mencionada de Randoop con toda la API de la clase bajo análisis 
(denominamos \texttt{API} a esta técnica de generación de objetos), y compararla
con otra versión que instruye a Randoop para que utilice sólo los MGOs \pp{Cuáles?} para
generar tests (llamamos \texttt{MGO} a este enfoque). 

Para estos enfoques, definimos tres valores de presupuesto de tiempo para la generación de 
test: 60, 120 y 180 segundos. En este experimento, medimos la cantidad de objetos distintos
generados por los tests (\emph{No. of Objs.}), usando los métodos
\texttt{equals()} de las clases evaluadas para descartar objetos iguales.
\cacho{Además, reportamos el tamaño del objeto más grande generado por las técnicas
\pp{Por qué faltan los tamaños de los objetos acá? En el paper los habíamos
puesto, o no?} (calculamos los tamaños de objetos con el método
\texttt{size()} de las clases bajo análisis). }
\cacho{Agregar table, URGENTE!. escribir esa parte}

Los resultados obtenidos se resumen en la Tabla~\ref{tab:results-obj}.

En promedio para todos los presupuestos, \texttt{MGO} genera 6355 objetos 
frente a 1597 de \texttt{API}. Es decir, el uso de métodos generadores resulta
en un incremento de 3.98 veces en la cantidad de objetos distintos producidos. 

Desagregando por presupuesto, para \textbf{60 segundos} con métodos generadores 
se obtienen, aproximadamente, tres veces más objetos, que con toda la API. 
Con un presupuesto de \textbf{120 segundos}, se generan aproximadamente cuatro 
veces más objetos con los \texttt{MGO} en comparación con la \texttt{API},
Finalmente, para \textbf{180 segundos}, la configuración con métodos 
generadores se construyen casi cinco veces más objetos con los \texttt{MGO} en
comparación con la \texttt{API}. Notablemente, la diferencia en la cantidad de
objetos generados se incrementa con el presupuesto. 

\pp{Falta lo de size}
\cacho{Agregar table, URGENTE!. escribir esa parte}

%La ventaja es consistente todos los casos. Si
%promediamos por clase (sobre los tres presupuestos), \texttt{MGO} es mejor
%en todas las clases, con ganancias especialmentes marcadas en \emph{NCL} (casi 15 veces mas en promedio por clase) y \emph{LinkedList}
%(casi 10 veces). 

En resumen, el uso de métodos generadores de objetos mejora significativamente la
capacidad de generación de objetos con Randoop, para un mismo presupuesto de
tiempo.
Es esperable que al generar objetos más diversos y más grandes, los objetos producidos por
\texttt{MGO} ejerciten mejor el código que los generados por
\texttt{API} si se los utiliza para testing. 
La calidad de los objetos generados se evalúa más adelante en la Sección~\ref{sec:parametrizedTesting}, 
donde los utilizamos como entradas de tests parametrizados sobre las estructuras de datos. 

\subsubsection{Verificación acotada usando JPF}
\label{sec:experiments-jpf-driver}

En un segundo experimento, evaluamos el impacto del uso de métodos generadores
de objetos en la eficiencia de la verificación acotada usando \emph{Java PathFinder}
(JPF)~\cite{Visser:2005} (explicado en la sección \ref{sec:jpf}).
Para este experimento preliminar, usamos como casos de estudio algunas clases
del paquete \texttt{java.util}. 
Los experimentos consisten en verificar de manera exhaustiva acotada que, dada
una estructura de datos, luego ejecución de un método de la estructura de datos
se satisface el invariante de representación (\texttt{repOK}) de la estructura. 
Por ejemplo, para el método \texttt{put} de \texttt{TreeMap} escribimos el
programa de la Figura~\ref{lst:propiedadJPF} para realizar la verificación con
JPF. \pp{Fijate que los captions de las figuras están en inglés y dicen Figure.
Hay que configurar algo para traducirlos a español.}

\begin{lstlisting}[caption={Verificando el método put de TreeMap con JPF},label={lst:propiedadJPF},language=Java,captionpos=b]
    public static void main(String[] args) {
       int scope = 3;
       TreeMap t = generateStructure(scope);
       t.put(Verify.getInt(0,scope),Verify.getInt(0,scope));
       assert t.repOK();
    }
\end{lstlisting}

El programa primero construye todos los árboles de tamaño acotado usando un
driver en la línea 3 (método \texttt{generateStructure}). Luego, ejecuta el
método bajo análisis en la línea 4 (\texttt{put}), y por último verifica que se
satisfaga el \texttt{repOK} luego de la ejecución del método en la línea 5.

Para la evaluación consideramos los dos enfoques presentados en la Sección
\ref{sec:jpf-builders} para construir los drivers. El enfoque \texttt{API} consiste en
utilizar toda la API. Recordar que un ejemplo de driver generado con este enfoque se 
presenta en la Figura \ref{lst:driverAPI}. Por otro lado, el enfoque
\texttt{MGO} consiste en usar sólo los métodos generadores de objetos para
construir los drivers, como se muestra en el ejemplo de la Figura \ref{lst:driverBLD}.
\pp{Los métodos generadores de objetos fueron precomputados usando el algoritmo ???}.

\input{tables/tableJPFBuilders.tex}
\input{tables/tableJPFBuilders1.tex}

Los resultados de los experimentos, presentados en las
Tablas~\ref{tab:results-jpf} y \ref{tab:results-jpf1}, muestran que los
controladores construidos a partir de los métodos generadores objetos (MGO) permiten
una mejora significativa en la eficiencia y la escalabilidad de JPF. 
Se observa una reducción considerable en el tiempo de análisis y en la cantidad
de estados explorados por JPF \pp{Dice states y time en las tablas. Cambiar.}, sin 
comprometer la cobertura de las estructuras objetivo. \pp{Qué cobertura?
tenemos números de cobertura para mostrar que no se reduce? O algún experimento
por el estilo?}

%Para focalizar la comparación en los distintos casos, 
%consideramos en cada clase–método todos los niveles de \emph{scope} que finalizan sin \emph{timeout}
%De esta manera, los resultados reflejan el comportamiento completo de cada técnica en el rango alcanzable por \emph{JPF}. 
%Cuando \textsf{API} alcanza \emph{timeout} en un determinado \emph{scope}, ese resultado indica un límite práctico 
%de escalabilidad para dicha variante
% , mientras que \textsf{MGO} (Métodos generadores de Objetos) en general logra completar en el mismo \emph{scope}

En las tablas se muestran los resultados para los scopes en los que
alguna de las técnicas termina antes de llegar al timeout.
En el análisis global, considerando los scopes para los que ambas técnicas terminan
antes del timeout, estas 30 combinaciones \pp{De donde sale el 30? Yo
cuento muchas más}, el \emph{driver} con \textsf{MGO} explora 14070756 estados
en 1591s, mientras que \textsf{API} visita 198130649 estados en 17,240 segundos.
En promedio por casos, \textsf{MGO} explora 234513 estados en 26.5 segundos, mientras que \textsf{API} 
explora 3302177 estados en 297.3 segundos. Esto implica que \textsf{MGO} reduce el espacio de estados en 
92.9\%\ y el tiempo en 91\%\ aproximadamente respecto de \textsf{API}. 
Se observa tambien que \textsf{API} alcanza el timeout en 9/30 combinaciones
(30\%) \pp{son más que 9, hay que recalcular los números?}.

%Bajo el mismo \emph{scope}, \textsf{API} explora aproximadamente 1308\%\ mas de estados y tarda 984\%\ mas de tiempo que \textsf{MGO}.
%En cuanto a la interpretación, 

En resumen, los resultados muestran evidencia preliminar de que construir \emph{drivers}
exclusivamente con los métodos generadores de objetos identificados por nuestro
enfoque \pp{cuál?}, reduce significativamente la cantidad de estados que debe
explorar JPF, y consecuentemente reduce el tiempo de la verificación. Esto le
permite a nuestro enfoque realizar la verificación para scopes más grandes
en varios de los casos analizados. 
Al mismo tiempo, la suficiencia del conjunto de \textsf{MGO}
garantiza que se preserva la capacidad del driver \textsf{MGO} de producir todas 
las instancias que genera el driver API. \pp{Hay que hacer un análisis de por
    qué hay más states en API, calculo que es porque se construye la misma
instancia muchas veces, pero lo tendremos que charlar.}


%poda de manera efectiva el factor de ramificación en los puntos de elección
%no deterministas del verificador (ej., \texttt{Verify.random}), lo que reduce
%tanto el número de estados alcanzables como el costo de retroceso durante la
%exploración. Al mismo tiempo, la suficiencia del conjunto de \textsf{MGO}
%garantiza que se preserva la capacidad de construir todas las instancias
%(acotadas por el \emph{scope}) y de poder escalar a \emph{scope} mas altos que pueden ser 
%relevantes para la verificación de las propiedades,
%sin sacrificar cobertura estructural dentro del alcance definido.
%
%Así, los casos con \emph{timeout} no se eliminan del análisis, 
%sino que se interpretan como evidencia de la explosión combinatoria que introduce la API completa, 
%en contraste con la reducción sustancial que se obtiene al restringirse al conjunto suficiente de métodos generadores.






%Esta evidencia refuerza la utilidad práctica 
%de nuestra técnica en entornos de verificación formal, donde el costo computacional de la exploración 
%del espacio de estados es un factor crítico.



% Para ilustrar este mecanismo, ponemos bajo análisis el método \texttt{put} de \texttt{TreeMap}.
% La generación de entradas para probar este método puede realizarse mediante un controlador como el mostrado en la immplentación mostrada en ~\ref{lst:driverBLD}.
% Para mitigar el problema de la explosión combinatoria, propusimos utilizar únicamente los métodos generadores 
% detectados automáticamente por nuestro enfoque con el algoritmo de \emph{HC} y la funcion objetivo de \emph{FRC} (ver Sección~\ref{sec:experimentalIdentificacionPrecision}). 
% Como se muestra en el framento de codigo ~\ref{lst:driverBLD}, el controlador basado exclusivamente en los métodos generadores de objectos de 
% \texttt{TreeMap} selecciona sólo dos métodos, suficientes y minimal, para generar todas las instancias 
% válidas dentro del alcance definido. En consecuencia, JPF puede explorar las mismas configuraciones 
% de objetos que con la API completa, pero con un espacio de búsqueda significativamente más reducido.

% El patrón se mantiene de forma consistente a través de múltiples estructuras de datos de 
% \texttt{java.util}, incluyendo \texttt{HashMap}, \texttt{HashSet}, \texttt{LinkedList}, 
% \texttt{TreeMap} y \texttt{TreeSet}. Por ejemplo, en el análisis del método \texttt{remove} 
% en \texttt{HashMap}, al aumentar el \textit{scope} de 4 a 6, el número de estados explorados 
% con el controlador basado en la API completa crece de 627361 a más de 22 millones, con tiempos 
% de ejecución que alcanzan los 3011 segundos (aproximadamente 50 minutos). En cambio, utilizando únicamente los métodos generadores de objetos, 
% se recorren menos de 2.3 millones de estados, en un tiempo de análisis significativamente menor.

% Este mismo comportamiento se replica en otras estructuras. En el caso de \texttt{LinkedList}, 
% el uso de todos los métodos de la API lleva a una explosión de estados (más de 39 millones en 
% \textit{scope} 6), mientras que con los métodos generadores de objectos, el número de estados se reduce a aproximadamente 
% 1.1 millones para el mismo scope. En términos de tiempo, esta diferencia se traduce en 2771 segundos contra apenas 221. 
% Asimismo, para \texttt{TreeMap} con el método \texttt{put}, el uso de los métodos generadores de objectos en 
% \textit{scope} 5 permite mantener el análisis por debajo de los 3100 segundos, mientras que 
% la versión con todos los métodos alcanza el límite de tiempo (TO que es igual a 5000 segundos).

% Además de la reducción en el tiempo de verificación y en los estados explorados, la comparación 
% entre ambos controladores confirma que el conjunto reducido de métodos generadores de objectos es suficiente 
% para generar todas las configuraciones relevantes de entrada que permiten evaluar correctamente 
% las propiedades del programa. Esto implica que, en la práctica, los métodos adicionales disponibles 
% en la API no contribuyen a la diversidad estructural de las entradas, sino que solo introducen 
% complejidad innecesaria en el espacio de búsqueda.

% En resumen, estos resultados evidencian que la identificación automática de métodos generadores no 
% solo mejora la calidad de las entradas en contextos como la generación de tests, sino que también 
% es una estrategia efectiva para escalar técnicas de verificación formal a estructuras de datos más 
% complejas. Al eliminar métodos superfluos y reducir el espacio de búsqueda, se mejora significativamente 
% el rendimiento de herramientas como JPF, sin sacrificar precisión ni completitud en el análisis.




\section{Generación exhaustiva acotada basada en la API}
\label{sec:experimentalBeapi}

En esta sección evaluamos empíricamente la técnica de generación exhaustiva
acotada BEAPI. 
Examinamos su eficiencia (tiempo de generación y escalabilidad respecto de los
\emph{scopes}) con respecto a enfoques existentes, y el impacto de las optimizaciones de BEAPI sobre su desempeño.
También realizamos una evaluación preliminar que muestra que BEAPI puede ser útil para chequear la
consistencia entre los objetos que puede generar la API y las restricciones que
imponen las especificaciones formales de invariantes de representación de estructuras (\texttt{repOK}). 
Por último, evaluamos preliminarmente que tan buenos son los objetos producidos por BEAPI en el
contexto del \emph{testing} parametrizado.

Así, en la evaluación de BEAPI buscaremos responder a las siguientes preguntas de
investigación:

\begin{itemize}
\item \emph{RQ4}: ¿Qué tan eficiente es BEAPI para la generación exhaustiva acotada de objetos?
\item\emph{RQ5}: ¿Cuál es el impacto de las optimizaciones propuestas en el rendimiento de BEAPI?
\item\emph{RQ6}: ¿Puede BEAPI ayudar a encontrar discrepancias entre especificaciones de invariantes de clase (repOK) y la capacidad de generación de objetos de la API?
\item\emph{RQ7}: ¿Qué tan efectivos son los objetos producidos por BEAPI para el testing parametrizado?  
\end{itemize}

\subsection{Configuración experimental}

Como casos de estudio, utilizamos implementaciones de estructuras de datos
tomadas de tres benchmarks existentes: \textsf{Kiasan}~\cite{Deng06}, 
\textsf{FAJITA}~\cite{Abad13}, \textsf{ROOPS} \cacho{VER CITA}. También
incluimos las clases utilizadas en la evaluación de
\textsf{Korat}~\cite{Boyapati02}, y un conjunto de clases de proyectos reales de
software que la denominamos \textsf{Real World}, donde se incluyen clases de
\emph{java.util} (TreeMap, TreeSet, LinkedList, Hasmap), de \emph{Apache Commons
Collections} (NodeCachingLinkedList) y un Scheduler de SIR.

Estos casos de estudio cubren una amplia variedad de estructuras de datos complejas, tales como listas doblemente enlazadas, árboles binarios de búsqueda, 
árboles rojo-negro, heaps binomiales, heaps de Fibonacci, entre otras. Una característica clave 
de estos casos de estudio es que incluyen especificaciones formales de invariante de clase (métodos 
\texttt{repOK}) escritas por sus propios autores, lo que resulta fundamental
para realizar las comparaciones con Korat, que requiere invariantes para
realizar la generación, y para los experimentos para responder a RQ6.

Para la evaluación se utilizó el prototipo de la herramienta BEAPI, que se
describe en la literatura en los trabajos de Politano et. al. \cite{}
\pp{Incluir acá los dos trabajos publicados de BEAPI}. 

En la comparación de eficiencia con técnicas existentes (RQ4) se utilizó Korat,
que es la herramienta existente de generación exhaustiva acotada más eficiente
\cite{} \pp{Buscar la cita de esto en el paper de beapi}. Tener en cuenta, como
Korat requiere de repOKs, se emplearon los \texttt{repOK}s provistos por los autores 
de cada caso de estudio. 
En algunos casos se identificaron errores o especificaciones incompletas en los \texttt{repOK}s; 
estos se señalan de forma explícita en los resultados y se analizan en más
detalle en la Sección~\ref{sec:existing-specs-analysis} (RQ6).

Para RQ7 se comparan los objetos generados por BEAPI, una técnica de generación
aleatoria (Randoop), y una variante de Randoop que sólo usa métodos generadores de objetos
(R-Builders). 

Los experimentos se llevaron a cabo en una computadora con procesador Intel Core i7-8700 
(3.2 GHz) y 16 GB de RAM, ejecutando Ubuntu Linux 20.04. Para cada ejecución individual se estableció 
un tiempo máximo de 60 minutos. Todos los experimentos fueron diseñados para ser reproducibles, 
y el artefacto experimental con el código fuente se encuentra disponible en:
\cacho{Poner pagina?} \pp{Si}.

\cacho{los parametros usados en GA} 
\pp{Qué GA?}


\subsection{RQ4: Eficiencia de BEAPI}

En esta sección comparamos BEAPI y Korat en términos de tiempo 
de ejecución, número de estructuras generadas y exploradas, y escalabilidad con
respecto a los \emph{scopes}.
Los resultados experimentales se muestran en la Tabla~\ref{table:korat-beapi}.
\pp{No entra la tabla en una página. Hay que dividirla en 2. Además tiene muchos
    errores, se corrieron varias líneas como FibHeap ROOPS s4, o SLList Fajita
s 8,9 y 10. Hay que revisar todo.}
La Tabla presenta una muestra representativa de los benchmarks
\textsf{Korat}, \textsf{FAJITA}, \textsf{ROOPS} y \textsf{Kiasan}. \pp{Los casos
restantes se pueden encontrar online \cite{}}.
Para cada clase evaluada, informamos los tiempos de generación 
(en segundos, columna \emph{Time}), la cantidad de estructuras válidas generadas (\emph{Generated}) y el total de estructuras exploradas (\emph{Explored}) por cada técnica. 
Incluimos, para cada enfoque, los \emph{scopes} más grandes (Columna \emph{S})
alcanzados por las técnicas antes de llegar al tiempo límite de 60 minutos. Vale
aclarar que no se utilizó el benchamark \textsf{Real World} en esta sección, ya que
no se dispone de \texttt{repOK}s para estas clases, lo que impide una comparación justa con \textsf{Korat}.

Agrupando por banco, se observa que en el conjunto \emph{KORAT} la herramienta \emph{Korat} es más veloz en 4/6 clases (el 67\% mas veloz que \textsf{BEAPI}); 
en \textsf{FAJITA}, \emph{BEAPI} supera a \emph{Korat} en 3/4 (75\%); en \emph{ROOPS}, \emph{BEAPI} lo hace en 5/7 (71\%); 
y en \textsf{Kiasan}, \textsf{Korat} es más rápido en 6/7 (86\%). 
En términos cualitativos, \emph{BEAPI} tiende a rendir mejor en estructuras fuertemente restringidas (pocas instancias válidas), 
mientras que \textsf{Korat} conserva ventaja cuando el dominio válido crece con rapidez.

A modo de ejemplo, cuando \emph{BEAPI} prevalece lo hace con reducciones de tiempo muy pronunciadas. Veamos un ejemplo,
en \textsf{FAJITA} con el caso de \emph{AVL}:
Para scope 10, Korat demora 163.50 segundos mientras que BEAPI tarda 1.92 segundos. 
Dicho de otro modo, \emph{BEAPI} es aproximadamente 85 veces más rápido. 
Para scope 11 en el mismo caso, los tiempos son 1271.23 frente a 5.80 segundos, \emph{BEAPI}
es unas 219 veces más rapido. 

A la inversa, cuando \textsf{Korat} domina (típico en su propio banco de pruebas), las ventajas también son claras. 
Por ejemplo, \textsf{KORAT} para el caso \textsf{DLList}:
Para scope 6 \emph{Korat} tarda 0.24 segundos mientras que \emph{BEAPI} demora 7.11 segundos. 
Dicho de otro modo, \textsf{Korat} es aproximadamente 30 veces más rápido. 

Los resultados muestran un comportamiento mixto. En estructuras con restricciones fuertes —como los 
árboles rojo-negro (\texttt{RBT}) y los árboles binarios de búsqueda (\texttt{BST})— \emph{BEAPI} 
supera a \textsf{Korat}, al explorar menos estados y terminar en menos tiempo. Esto ocurre porque 
en estas estructuras el número de configuraciones válidas es más bajo, lo que permite a \emph{BEAPI} 
recorrer el espacio de soluciones de forma eficiente. Por el contrario, en estructuras más permisivas, 
como listas doblemente enlazadas (\texttt{DLList}) o montículos binomiales (\texttt{BinHeap}), 
\textsf{Korat} logra un mejor rendimiento gracias a su estrategia de poda temprana basada en 
\texttt{repOK}.

En estos experimentos, se aseguraron condiciones comparables: los \texttt{repOK}s utilizados con 
\textsf{Korat} fueron los provistos originalmente por los autores de cada benchmark y no se 
modificaron para este estudio, evitando así alterar la eficiencia inherente a la técnica. 
Para \emph{BEAPI}, se probó exhaustivamente que los métodos de la API empleados fueran correctos 
y suficientes para el análisis. Como era de esperarse, pueden existir diferencias en los conjuntos 
de estructuras exploradas por cada técnica, debido a los distintos espacios de búsqueda que inducen. 
Sin embargo, para un mismo caso y alcance, ambas herramientas deberían generar la misma cantidad 
de estructuras válidas, salvo en casos donde hay errores en los \texttt{repOK}s o diferencias sutiles 
en la definición de \emph{scope}.

Cabe destacar dos tipos de excepciones en los resultados. En primer lugar, algunos casos incluyen 
\texttt{repOK}s con errores, que impactan negativamente en la eficiencia de \textsf{Korat}; estos 
casos están sombreados en la tabla y se analizan más en detalle en la Sección~\ref{sec:existing-specs-analysis}. 
Un caso ilustrativo es el de \texttt{BinHeap}, donde \textsf{Korat} alcanza el \emph{scope} 8 utilizando 
el \texttt{repOK} de \texttt{ROOPS}, el \emph{scope} 10 con el de \texttt{FAJITA}, y el \emph{scope} 11 
con su propia especificación, todas ellas equivalentes en términos de estructuras generadas. Esto evidencia 
la sensibilidad de \textsf{Korat} a la forma en que se escriben los \texttt{repOK}s, los cuales, si están 
ajustados específicamente para su motor de búsqueda, pueden mejorar significativamente su rendimiento. 
No obstante, cuando los \texttt{repOK}s presentan errores o están mal diseñados, su impacto negativo 
puede ser severo, como se analiza en profundidad en la Sección~\ref{sec:existing-specs-analysis}.

En segundo lugar, en estructuras como \texttt{RBT} y \texttt{FibHeap}, la diferencia semántica en la 
noción de \emph{scope} entre ambas técnicas produce discrepancias. Por ejemplo, hay estructuras de 
tamaño $n$ que solo pueden obtenerse mediante secuencias que superan temporalmente dicho tamaño 
(inserciones seguidas de eliminaciones y nuevas inserciones para provocar reordenamientos de balanceo). 
Como \emph{BEAPI} descarta automáticamente las secuencias que exceden el \emph{scope} máximo, no puede 
llegar a estas configuraciones, mientras que \textsf{Korat} sí lo logra.

Un aspecto clave que influye en la eficiencia de \emph{BEAPI} es la cantidad de estructuras válidas 
generables para un determinado alcance. A medida que crece esta cardinalidad, aumenta también el 
número de secuencias que \emph{BEAPI} debe evaluar, lo cual impacta directamente en su tiempo de 
ejecución. Sin embargo, un punto a favor de \emph{BEAPI} es que no depende de la eficiencia del 
\texttt{repOK}; esto lo hace más robusto ante especificaciones incorrectas o mal escritas, mientras 
que \textsf{Korat} puede verse severamente afectado por ellas.

% En cuanto al rendimiento observado, \textsf{Korat} muestra mejor desempeño en 4 de los 6 casos 
% pertenecientes a su propio banco de pruebas. En el benchmark \textsf{FAJITA}, \emph{BEAPI} supera 
% a \textsf{Korat} en 3 de 4 casos, mientras que en \textsf{ROOPS} lo hace en 5 de 7. Finalmente, 
% en el banco \textsf{Kiasan}, \textsf{Korat} resulta más rápido en 6 de 7 casos. Esta tendencia confirma 
% que \emph{BEAPI} ofrece un mejor rendimiento en estructuras altamente restringidas —donde el número 
% de configuraciones válidas es reducido— mientras que \textsf{Korat} mantiene la ventaja en dominios 
% más amplios, donde la cantidad de estructuras válidas crece de forma acelerada. Esto se debe a que 
% en tales escenarios, \emph{BEAPI} debe generar y evaluar muchas más secuencias de entrada en cada 
% iteración, lo cual penaliza su desempeño.

En conclusión, \emph{BEAPI} es una herramienta eficiente para la generación exhaustiva acotada. 
Ofrece tiempos competitivos y, en varios casos, mejores que \textsf{Korat}, especialmente cuando 
se trata de estructuras con invariantes complejas o bajo número de soluciones. Su independencia 
respecto al \texttt{repOK} lo vuelve una alternativa robusta y complementaria para contextos donde 
la calidad de la especificación no está garantizada.
\input{tables/tableEficienciaBEAPI.tex}

\subsection{RQ5: Impacto de las optimizaciones propuestas}
\label{sec:optimizations}

En esta pregunta de investigación evaluamos el impacto que tienen las optimizaciones propuestas 
de \emph{BEAPI} en su rendimiento general frente al enfoque de generación exhaustiva tradicional. 
Concretamente, analizamos cuatro configuraciones de la herramienta: \textsf{SM/BLD}, que habilita tanto 
la coincidencia de estados (\emph{State Matching}) como la identificación de métodos generadores de objetos (\emph{builders}); 
\textsf{SM}, que activa únicamente la coincidencia de estados; \textsf{BLD}, que habilita únicamente 
la identificación de métodos generadores de objetos; y \textsf{NoOPT}, que desactiva ambas optimizaciones, funcionando 
como un enfoque de fuerza bruta.

% Para este análisis, nos enfocamos en el benchmark \texttt{Real World}, que incluye seis 
% implementaciones reales de estructuras de datos ampliamente utilizadas: \texttt{LinkedList} 
% (67 métodos en la API), \texttt{TreeSet} (34), \texttt{TreeMap} (61), \texttt{HashMap} (45), \texttt{NCL} (34) y 
% \texttt{Schedule} (12). La Tabla~\ref{tab:results-realWorld} 
% resume los resultados obtenidos. Cabe mencionar que se realizaron experimentos equivalentes para 
% el resto de los benchmarks utilizados en este capítulo, pero se omiten en las tablas en esta tesis por motivos de espacio.
% El conjunto \texttt{Real World} representa adecuadamente el comportamiento observado en los demás casos, 
% y resulta suficiente para sustentar las conclusiones extraídas en esta sección.

La Tabla~\ref{tab:results-realWorld} \cacho{Ver como referencias, hacer una sola tabla} resume los resultados para el conjunto \textsf{Real World} 
(seis implementaciones ampliamente usadas: \texttt{LinkedList} con 67 métodos, \texttt{TreeSet} con 34 métodos, \texttt{TreeMap} con 61 métodos, \texttt{HashMap} con 45 métodos, \texttt{NCL} con 34 métodos y \texttt{Schedule} con 12 métodos). 
Resultados análogos se reportan para los demás benchmarks en las tablas correspondientes de esta sección; en conjunto, todos ellos muestran el mismo patrón y fundamentan las conclusiones presentadas.

Los resultados muestran de forma contundente que la configuración sin optimizaciones (\textsf{NoOPT}) 
tiene un rendimiento deficiente incluso en los estudios de caso más simples y con alcances bajos. 
Estos \emph{scopes}, si bien pequeños, ya resultan inadecuados para generar conjuntos de test de 
calidad y revelan de inmediato la falta de escalabilidad del enfoque de fuerza bruta. Esta limitación 
es representativa de la problemática general que enfrentan las técnicas de generación exhaustiva en 
presencia de APIs amplias y estructuras complejas.

Entre las optimizaciones propuestas, la coincidencia de estados (\textsf{SM}) demuestra ser la más 
impactante a nivel general. Por sí sola, permite escalar a alcances considerablemente mayores y reduce 
los tiempos de ejecución en varios órdenes de magnitud respecto a \textsf{NoOPT}. Su efecto se vuelve 
crítico en aquellas estructuras donde múltiples secuencias de llamadas a métodos pueden generar el mismo estado 
intermedio, lo que genera redundancia innecesaria si no se cuenta con un mecanismo de detección y poda 
de estados previamente explorados.

La segunda optimización, la identificación previa de métodos \emph{builders} (\textsf{BLD}), adquiere 
mayor relevancia a medida que se incrementa la cantidad de métodos disponibles en la API. En 
estructuras reales como \texttt{TreeMap}, \texttt{LinkedList} o \texttt{HashMap}, donde existen 
docenas de métodos públicos con efectos variados, limitar la generación de secuencias a combinaciones 
de constructores válidos tiene un impacto significativo en la eficiencia del proceso de generación. 
Aunque por sí sola esta optimización no siempre permite escalar a scopes altos, su combinación con la 
coincidencia de estados resulta esencial para lograr tiempos razonables de ejecución.

El mayor beneficio se observa en la configuración \textsf{SM/BLD}, donde ambas optimizaciones están 
habilitadas. En esta configuración, \emph{BEAPI} escala significativamente mejor, alcanzando scopes 
mucho mayores en todos los casos analizados, con tiempos de ejecución hasta cientos de veces menores 
que con las demás configuraciones.

Donde la tabla lo permite comparar directamente, se observan reducciones de tiempo muy pronunciadas frente a \textsf{SM} solo: 
por ejemplo, en \texttt{TreeSet} (scope 13) \textsf{SM/BLD} tarda 226.66 segundos versus 887.83 segundos con \textsf{SM}  (casi 4 veces más rapido con las optimizaciones).
En \texttt{TreeMap} (scope 6) \textsf{SM/BLD} tarda 839.87 segundos versus 2901.37,s segundos con \textsf{SM} (casi 3.5 veces más rapido);
En \texttt{Linkedlist} (scope 6) \textsf{SM/BLD} tarda 0.96 segundos versus 258.85,s segundos con \textsf{SM} (casi 270 veces más rapido); y en 
\texttt{NCL} (scope 4) \textsf{SM/BLD} tarda 0.41 segundos versus 3.48,s segundos con \textsf{SM} (casi 8 veces más rapido).

% Por ejemplo, para \texttt{NCL}, se logra alcanzar \textit{scope} 6 
% en 73.78 segundos con \textsf{SM/BLD}, mientras que con \textsf{SM} apenas se alcanza \textit{scope} 4 
% en más de 3 segundos, y \textsf{NoOPT} no logra ejecutar siquiera scopes bajos. Para \texttt{TreeSet}, 
% el contraste es aún más pronunciado: \textsf{SM/BLD} alcanza \textit{scope} 13 en 226 segundos, mientras 
% que \textsf{SM} requiere casi 900 segundos para el mismo alcance.

Estos resultados evidencian que las optimizaciones propuestas no solo reducen tiempos de ejecución, 
sino que amplían sustancialmente los límites de escalabilidad de la herramienta. Sin estas mejoras, 
\emph{BEAPI} se comporta como un generador exhaustivo tradicional, enfrentando rápidamente la explosión 
combinatoria del espacio de búsqueda al aumentar el número de métodos o el tamaño de las secuencias. 
En cambio, con las optimizaciones activadas, la técnica logra mantener el proceso dentro de márgenes 
computacionalmente viables incluso en contextos realistas y exigentes.

En conclusión, la activación conjunta de coincidencia de estados e identificación de \emph{builders} 
resulta crucial para obtener un rendimiento competitivo. Estas optimizaciones transforman a \emph{BEAPI} 
en una herramienta capaz de escalar a scopes altos sin sacrificar exhaustividad ni precisión, 
permitiendo su aplicación efectiva en casos de uso reales.

% Cabe destacar, que para todos los experimentos presentados en esta sección, se empleó un \emph{scope} fijo de 5 durante la 
% identificación de métodos generadores de objectos, y el tiempo máximo requerido por esta etapa fue de 132 segundos, 
% correspondiente a la estructura \emph{TreeMap}, que posee 61 métodos públicos. Verificamos manualmente que, 
% en todos los casos, el conjunto de métodos identificados fuera suficiente para construir los objetos válidos.

Cabe señalar que la identificación de métodos generadores se realiza típicamente una única vez por clase, 
y puede reutilizarse en múltiples ejecuciones posteriores de \emph{BEAPI}. En consecuencia, su costo se 
amortiza a lo largo del tiempo, para los scopes más grandes, que son los más importantes, el tiempo de identificación de 
métodos generadores es insignificante en relación con los tiempos de generación.Por esta razón, decidimos no incluir los tiempos de identificación 
en los tiempos de ejecución reportados para \textsf{BEAPI} en los distintos experimentos.
No obstante, es importante destacar que este preprocesamiento introduce una optimización significativa en 
términos de eficiencia, con un costo computacional bajo en relación con el proceso completo de generación. 

\cacho{Puse todos los casos, todos los scope como habiamos dicho de otra RQ, cualquier cosa saco info si es mucho.
En el paper estaba una sola por tema de espacio.}

\input{tables/optimizaciones}


\subsection{RQ6: BEAPI para analizar invariantes de representación}
\label{sec:existing-specs-analysis}

Esta pregunta de investigación evalúa si \textsf{BEAPI} puede ser útil para asistir a los usuarios 
en la detección de fallas en los métodos \texttt{repOK}, a través de la comparación entre el conjunto 
de objetos que se pueden generar mediante la API y aquellos que se pueden generar a partir del 
invariante de representación.

Para ello, diseñamos un procedimiento automatizado en tres pasos. En primer lugar, ejecutamos 
\emph{BEAPI} para generar un conjunto de estructuras, denominado \texttt{SBEAPI}, utilizando únicamente 
la API de la clase, y ejecutamos \emph{Korat} para generar un segundo conjunto, \texttt{SKORAT}, 
utilizando únicamente el método \texttt{repOK}. Ambas ejecuciones se realizaron con el mismo 
\emph{scope}. En segundo lugar, canonizamos todas las estructuras de ambos conjuntos utilizando el 
proceso de linearización (Sección~\ref{sec:stateMatching}), de modo que estructuras isomorfas puedan 
compararse de forma directa. Finalmente, comparamos los conjuntos \texttt{SBEAPI} y \texttt{SKORAT} para 
identificar diferencias que indiquen discrepancias semánticas entre la definición operativa de la 
clase (la API) y su especificación declarativa (\texttt{repOK}).

El procedimiento puede producir tres resultados distintos. Cuando \texttt{SBEAPI} $\subset$ \texttt{SKORAT}, 
puede deberse a que la API genera un subconjunto de las estructuras válidas, que \texttt{repOK} 
sufre de subespecificación (es decir, permite estructuras que no deberían ser válidas), o ambos. 
En estos casos, las estructuras que están en \texttt{SKORAT} pero no en \texttt{SBEAPI} son evidencia potencial 
de errores, y fueron inspeccionadas manualmente para confirmar si se trataba efectivamente de fallas 
en \texttt{repOK}. Clasificamos estos errores como \texttt{under}, ya que el invariante permite más 
de lo que debería.

Por el contrario, cuando \texttt{SKORAT} $\subset$ \texttt{SBEAPI}, puede indicar que \texttt{repOK} es demasiado 
restrictivo (sobreespecificado), que la API permite estructuras no válidas, o ambas cosas. En este 
caso, se analizan las estructuras que aparecen en \texttt{SBEAPI} pero no en \texttt{SKORAT}. Los errores 
confirmados manualmente en esta categoría se etiquetan como \texttt{over}.

En algunos casos, se encontraron diferencias en ambas direcciones: estructuras válidas generadas por 
\textsf{Korat} que no están presentes en \texttt{SBEAPI}, y estructuras generadas por \emph{BEAPI} que 
no son aceptadas por \texttt{repOK}. Estas diferencias simultáneas pueden deberse a fallas en ambas 
fuentes (API y \texttt{repOK}), o a errores más profundos de especificación. En estos casos, cuando se 
confirma una falla manualmente, la clasificamos como \texttt{error}.

Cabe destacar que diferencias menores en la definición de \emph{scope} entre los enfoques pueden 
producir "falsos positivos" en esta comparación. Esto ocurrió solamente en las estructuras 
\texttt{RBT} y \texttt{FibHeap}, donde \emph{BEAPI} no generó ciertas estructuras válidas dentro del 
alcance debido a restricciones de balanceo interno que requieren secuencias más largas que el 
\emph{scope} establecido. Estos casos se identificaron y descartaron manualmente, observando que al 
aumentar el \emph{scope}, las estructuras de \textsf{Korat} sí aparecen en la salida de \emph{BEAPI}.

Los resultados del experimento se resumen en la Tabla~\ref{table:bugs}. Encontramos errores en 9 de 
los 26 métodos \texttt{repOK} analizados. Esta proporción evidencia la dificultad de escribir 
invariantes de representación correctos, incluso en bibliotecas diseñadas por expertos, y pone de 
manifiesto el valor de \textsf{BEAPI} como herramienta de apoyo para detectar inconsistencias.

En particular, se detectaron errores de subespecificación en clases como \texttt{RBTree}, \texttt{AVL}, 
\texttt{BinTree}, \texttt{FibHeap} y \texttt{NCL}, donde \texttt{repOK} permite estructuras 
incompletas, con campos nulos o claves no válidas, que no pueden ser generadas mediante la API 
original. También se identificaron casos de sobreespecificación, como en \texttt{FibHeap}, donde 
el método \texttt{repOK} rechaza heaps válidos cuyo nodo mínimo es nulo de forma transitoria. 
Finalmente, se confirmaron errores estructurales en las definiciones de altura y ordenamiento en 
clases como \texttt{AVL}, donde las hojas están mal inicializadas o los cálculos son inconsistentes 
con la implementación.

En conclusión, \emph{BEAPI} no solo es útil para generar estructuras exhaustivamente, sino que 
también actúa como un verificador complementario de especificaciones de invariante, permitiendo 
descubrir errores difíciles de detectar mediante revisión manual o prueba dinámica convencional. 
Este enfoque ofrece una vía práctica y automatizada para mejorar la calidad de las especificaciones 
de clase, lo que redunda en mayor confiabilidad del software verificado.

\input{tables/bugRepOk}

\subsection{RQ7: Usando BEAPI para el testing parametrizado}
\label{sec:parametrizedTesting}

En esta etapa de la evaluación, se llevó a cabo un análisis exhaustivo para determinar la 
utilidad de los métodos generadores identificados mediante nuestro enfoque, particularmente 
en el contexto de la generación automatizada de casos de prueba. Estos métodos permiten construir 
objetos que pueden utilizarse como entradas en test suites parametrizadas.
\cacho{Poner algo sobre el experimento de builders que generamos objetos con BEAPI y los usamos en tests parametrizados.}

Los tests parametrizados constituyen una técnica eficiente para mejorar la cobertura en tests 
automatizados. En lugar de definir casos de prueba específicos para cada configuración, se 
establece una única prueba que se ejecuta múltiples veces con distintos parámetros. En nuestro 
contexto, dichos parámetros son objetos generados por distintas técnicas, los cuales sirven como 
entrada a los métodos de la clase bajo prueba.

Para llevar a cabo este experimento, diseñamos una test suite parametrizada en la que cada método 
público de la clase bajo análisis es ejercitado con objetos generados por distintas técnicas. Las 
herramientas utilizadas para generar estos objetos fueron: la versión estándar de \emph{Randoop}, que 
genera directamente una suite de tests sin reutilizar objetos previos; una variante de \emph{Randoop} 
denominada \emph{R-Serialize}, que serializa los objetos construidos durante la generación de 
tests, permitiendo su reutilización como entrada en tests parametrizados; una versión modificada 
de \emph{Randoop} que prioriza el uso de métodos generadores de objetos identificados previamente mediante 
nuestro enfoque (denominada \emph{R-Builders}, ver sección \ref{sec:object_builders_randoop}); y por último, la herramienta \emph{BEAPI}, que 
produce directamente un conjunto reducido pero válido de objetos a partir de su propia lógica de 
exploración exhaustiva acotada.

A fin de evaluar y comparar el impacto de cada enfoque, se utilizó el benchmark basado en clases 
del paquete \emph{java.util} debido a su rica API que tienen: \emph{HashSet}, \emph{HashMap}, \emph{TreeMap}, \emph{TreeSet} y \emph{LinkedList}.
Para cada clase se definió un invariante que permite distinguir objetos válidos de inválidos, permitiendo así evaluar la calidad de los generadores.

Las métricas consideradas fueron: la cantidad de objetos válidos e inválidos generados, el tiempo 
de generación (\emph{GTime}), este tiempo es el que se le da cada herramienta para ejecutar. Vale aclarar,
que en el caso de BEAPI, este tiempo es el equivalente a cierto scope. Vale recordar al lector, que BEAPI, al ser un generador exhaustivo,
tiene limite de scope y no de tiempo para generar. 
Ademas se tuvo en cuenta las metricas de la cantidad de casos de prueba ejecutados (\emph{Test}), el 
tiempo total de ejecución de la test suite (\emph{T(Seg)}), la cobertura de ramas alcanzada 
(\emph{Ramas}) y la cantidad de mutantes eliminados (\emph{Mutacion}). Es importante destacar 
que la test suite parametrizada es común a los enfoques \emph{R-Serialize}, \emph{R-Builders} 
y \emph{BEAPI}, permitiendo así una comparación justa sobre los objetos generados. En el caso de 
\emph{Randoop}, la herramienta genera su propia test suite, por lo que los resultados deben 
interpretarse con cautela.

\input{tables/randoopvsBeapi}

La Tabla~\ref{tab:hashSetTools} muestra una comparativa de diferentes técnicas aplicadas sobre 
\texttt{HashSet}. Se observa que al priorizar métodos generadores de objectos en Randoop, \emph{R-Builders}, la cantidad de 
objetos generados se incrementa significativamente (más de 300\% en comparación con la versión 
estándar, \emph{R-Serialize}), lo cual impacta positivamente en la cobertura, principalmente en tiempos de generacion más cortos.

En contraste, \textsf{BEAPI} genera sólo 32 objetos válidos, pero logra una cobertura de ramas y 
mutación equivalente a \texttt{R-Builders}, y superior a la alcanzada por \texttt{R-Serialize} y 
\texttt{Randoop} en lapsos de tiempo cortos. Esta eficiencia evidencia que BEAPI puede generar un 
conjunto reducido de objetos, pero de alta calidad para testing, en muy poco tiempo (menos de cinco 
segundos).

\cacho{No hablo de porcentaje porq es poco, diria en algunos casos el 1\% mas, no me parece relevante ese dato}
En términos de ejecución, mientras \texttt{R-Builders} y \texttt{R-Serialize} requieren ejecutar 
millones de tests para alcanzar altos niveles de cobertura, \texttt{BEAPI} logra el mismo nivel 
ejecutando apenas unos pocos miles. Esto indica que los objetos generados por \textsf{BEAPI} son 
mas efectivos como entradas, permitiendo detectar fallas y explorar el espacio de estados 
de manera eficiente.

La Tabla~\ref{tab:treeSetTools} presenta los resultados obtenidos para \texttt{TreeSet}. En 
este caso, las diferencias comienzan a notarse con mayor claridad. \texttt{R-Builders} logra 
una cobertura y mutación ligeramente superior a \texttt{BEAPI}. 
Una explicación para este comportamiento radica en que ciertas configuraciones válidas de \texttt{TreeSet} pueden 
requerir secuencias de inserciones y eliminaciones que BEAPI no alcanza debido a su 
limitación por scope. Las dos variantes con métodos generadores de objetos son claramente más eficientes en tiempo hasta cobertura que las demás. 
\texttt{BEAPI} alcanza de manera estable 150 ramas y 109 mutación en solo 2 segundos (en todos los presupuestos). 
\texttt{R-Builders} logra la mejor cobertura absoluta, pero necesita más tiempo, supera las 150 ramas recién a los 10 segundos (151 ramas, 111 mutación). 
En contraste, \texttt{R-Serialize} tarda 40 segundos para generarar y 11 segundos para ejecutar para quedarse en 125 ramas y 109 mutación, y \textbf{Randoop} llega a menos cobertura (105 ramas) incluso con 150 y 33 segundos.
Si comparamos, cobertura por segundo, \texttt{BEAPI} llega primero a una alta cobertura y \texttt{R-Builders} puede llegar a cubrir algun mutante mas, pero pagando un costo de tiempo mayor.

El comportamiento observado se repite en la Tabla~\ref{tab:treeMapTools}, donde se evalúa la 
clase \texttt{TreeMap}. 
En este caso, \texttt{R-Builders} obtiene más mutación que \textbf{BEAPI} (152 vs 140), es decir, mata, aproximadamente un 6\% mas de mutantes;
en cambio, \texttt{BEAPI} logra mayor cobertura de ramas en todos los presupuestos (alrededor del 8\% a partir de 20 segundos de tiempo de generación).
Nuevamente, la explicación podría estar relacionada con estructuras internas que BEAPI no puede construir directamente bajo un scope fijo, como 
árboles parcialmente desbalanceados que luego se ajustan mediante rotaciones o reordenamientos. 
A pesar de estas diferencias, \texttt{BEAPI} mantiene una cobertura muy competitiva y logra 
resultados consistentes con una fracción del esfuerzo computacional, lo que refuerza su valor 
como técnica efectiva, especialmente en contextos donde se requiere control y exhaustividad 
estructural.

En cuanto a la Tabla~\ref{tab:linkedListTools} que presenta los resultados obtenidos para la clase 
\texttt{LinkedList}. En este caso, se observa nuevamente que \texttt{BEAPI}, a pesar de 
generar un número reducido de objetos, mantiene niveles constantes de cobertura y mutación. 
Las técnicas \texttt{R-Serialize} y \texttt{R-Builders} logran cubrir las mismas ramas y casi los mismos 
mutantes que \texttt{BEAPI}, esto se debe a que las listas enlazadas son estructuras más simples,
donde la mayoría de las configuraciones válidas pueden alcanzarse mediante secuencias cortas
de inserciones y eliminaciones..

Con respecto a la tabla ~\ref{tab:hashMapTools}, 
correspondiente a \texttt{HashMap}, se aprecia un caso 
similar. \texttt{BEAPI} logra altos niveles de cobertura desde tiempos de generación muy 
reducidos, mientras que las otras técnicas necesitan un volumen considerable de objetos y 
tests para alcanzar resultados comparables. En este caso \texttt{R-Builders} obtiene un puntaje de mutación 6–7\% mayor que \texttt{BEAPI} (127–128 vs 119–120). 
Una explicación es que varios mutantes exitosos están asociados a rutas de código y estructuras intermedias que \texttt{BEAPI} no utiliza:
por ejemplo, secuencias largas con inserciones/eliminaciones que pasan por estados transitorios (similar a los casos de \texttt{TreeMap} y texttt{Treeset}). 
\texttt{R-Builders}, al explorar más operaciones y estados no canónicos, tiende a tocar esos caminos y matar esos mutantes extra, aunque al costo de mucha más ejecución. 
De hecho, en ramas \texttt{BEAPI} iguala o supera a \texttt{R-Builders} ( 122 vs 119 a partir de 
GTime=10), reforzando la idea de que la ventaja de \texttt{R-Builders} en mutación proviene de probar más tipos de operaciones y estados que por cubrir mejor el nucleo de la estructura.

Los resultados obtenidos permiten concluir que los objetos generados por \textsf{BEAPI} son 
efectivos y eficientes para ser utilizados en tests parametrizados. Aunque la cantidad total de 
objetos generados es menor, su calidad y representatividad permiten alcanzar los máximos niveles 
de cobertura con una mínima ejecución. Además, se observa que guiar a generadores aleatorios como 
Randoop mediante la identificación previa de métodos generadores de objectos también mejora significativamente 
la cobertura. Esto hace que se reafirme el valor del otro aporte de esta tesis: la identificación automática de métodos generadores.
Este conocimiento no solo permite mejorar herramientas existentes como Randoop (caso de \texttt{R-Builders}), 
sino que también da lugar a técnicas completamente nuevas como \textsf{BEAPI}, 

En contextos donde el tiempo de ejecución es un factor crítico, o donde se necesita eficiencia sin 
sacrificar cobertura, \textsf{BEAPI} se posiciona como una herramienta valiosa para la generación 
de objetos de prueba. Estos resultados también reafirman la importancia de identificar y utilizar 
adecuadamente los métodos generadores para mejorar la calidad de los tests automatizados.


% \subsubsection{Variacion de acuerdo a los Parametros}

% Ademas, examinamos la sensibilidad de los parámetros del Algoritmo Genético y exploramos los usos de los builders generados. Los resultados obtenidos proporcionaron información valiosa sobre el rendimiento y la utilidad de nuestra técnica en la generación de builders y su aplicación en diversas tareas.

% Nuestra comparacion se basa en medir la cantidad de tiempo que le lleva a cada Algortimo terminar la ejecucio, en la cantidad de candidatos que evalua la funcion de valoracion y cuan bueno es en eficacia para encontrar el minimo y suficiente subconjunto de metodos que pudimos observar en nuestro ground truth \cacho{Agregar seccion}. Ejecutamos el algoritmo 10 veces con el resto de los parametros que no esta en evaluacion con un valor promedio (Crossover=0.5, mutation 0.1, Tournanament 4).
% Tambien utilizamos 30 segundos para la fitness con randoop y scope 6 para BEAPI.

% En la tabla \ref{tab:CrossOverGA} se puede observar como se comporta el algoritmo cuando se utiliza diferente rate para el operador de CrossOver. 



% \subsection{Uso de BEAPI para analizar especificaciones}
% \label{sec:existing-specs-analysis}
% \input{tables/bugRepOk}
% La RQ3 aborda si \textsf{BEAPI} puede ser útil para ayudar al usuario a encontrar fallas en \texttt{repOK}s, mediante la comparación del conjunto de objetos que se pueden generar utilizando la API y el conjunto de objetos generados a partir de utilizar un invariante, como es el caso del \texttt{repOK}. Diseñamos el siguiente procedimiento automatizado. Primero, ejecutamos \emph{BEAPI} para generar un conjunto, \texttt{SA}, 
% de estructuras a partir de la API, y utilizamos \emph{Korat} para generar un conjunto, \texttt{SR}, a partir de \texttt{repOK}, 
% utilizando el mismo ámbito para ambas herramientas. En segundo lugar,  . En tercer lugar, comparamos 
%  los conjuntos \texttt{SA} y \texttt{SR} en cuanto a igualdad. Las diferencias en esta comparación 
%  señalan una discrepancia entre \texttt{repOK} y la API. Existen tres posibles resultados para este 
%  procedimiento automatizado. Si \texttt{SA} $\subset$ \texttt{SR}, es posible que la API genere un 
%  subconjunto de las estructuras válidas, que \texttt{repOK} sufra de subespecificación (\texttt(under)) 
%  (restricciones faltantes), o ambos. En este caso, las estructuras en \texttt{SR} que no pertenecen a
%   \texttt{SA} son evidencia del problema, y el usuario debe analizarlas manualmente para descubrir 
%   dónde está el error. Aquí, informamos los errores de subespecificación (confirmados manualmente) 
%   en \emph{repOK}s que son evidenciados por las estructuras mencionadas. En contraste, cuando \texttt{SR}
%    $\subset$ \texttt{SA}, puede ser el caso de que la API genere un superconjunto de las estructuras válidas
%    , que \texttt{repOK} sufra de sobreespecificación, \texttt(over), (\texttt{repOK} es demasiado restrictivo)
%    , o ambos. Las estructuras en \texttt{SA} que no pertenecen a \texttt{SR} podrían indicar la raíz del error,
%     y nuevamente deben ser analizadas manualmente por el usuario. Informamos los errores de sobreespecificación 
%     (confirmados manualmente) en \texttt{repOK}s que son evidenciados por estas estructuras. 
%     Finalmente, puede darse el caso de que haya estructuras en \texttt{SR} que no pertenecen 
%     a \texttt{SA}, y que haya estructuras (distintas de las anteriores) en \texttt{SA} que no
%      pertenecen a \texttt{SR}. Estos pueden ser debidos a fallos en la API, fallas en \texttt{repOK}, 
%      o ambos. Informamos las fallas confirmadas manualmente en \texttt{repOK}s que son evidenciadas por
%       tales estructuras simplemente como errores (\texttt{repOK} describe un conjunto de estructuras 
%       diferente al que debería). Observa que las diferencias en las definiciones de ámbito de los enfoques 
%       pueden hacer que los conjuntos \texttt{SA} y \texttt{SR} difieran. Esto solo fue el caso en las 
%       estructuras \texttt{RBT} y \texttt{FibHeap}, donde \textsf{BEAPI} generó un conjunto más pequeño de 
%       estructuras para el mismo ámbito que \textsf{Korat} debido a restricciones de balance (como se explica
%        en la Sección \ref{sec:evaluation-vs-korat}). Sin embargo, estos "falsos positivos" se pueden revelar 
%        fácilmente, ya que todas las estructuras generadas por \textsf{Korat} siempre estuvieron incluidas en
%         las estructuras generadas por \textsf{BEAPI} si se utilizaba un ámbito más amplio para este último
%          enfoque. Utilizando esta información, descartamos manualmente los "falsos positivos" debido a las 
%          diferencias de ámbito en \texttt{RBT} y \texttt{FibHeap}.

% Los resultados de este experimento se resumen en la Tabla \ref{table:bugs}. Encontramos fallas en 9 de 26 \texttt{repOK}s utilizando el enfoque descrito anteriormente. El alto número de fallas descubiertas evidencia que los problemas en \texttt{repOK}s son difíciles de encontrar manualmente, y que \textsf{BEAPI} puede ser de gran ayuda para esta tarea.

% \subsection{Comparativa de BEAPI con otras tecnicas de generacion de test}

% \hspace{1cm}

% En esta etapa de la evaluación, se llevó a cabo un análisis exhaustivo para determinar la utilidad de los métodos builders identificados en el contexto del análisis de programas, específicamente en la generación automatizada de casos de prueba. Estos builders se consideran objetos clave que pueden ser utilizados como entradas en test parametrizadas.

% Los test parametrizados son una técnica utilizada en el campo de la generación automatizada de casos de prueba para aumentar la eficiencia y la cobertura de las pruebas. En lugar de escribir casos de prueba individuales para cada escenario posible, los test parametrizados permiten definir un conjunto de parámetros que se utilizan para generar automáticamente múltiples casos de prueba.

% En el contexto de la evaluación experimental, se utilizó la técnica de test parametrizados para alimentar una test suite con objetos creados por diferentes técnicas. Esto significa que se definieron parámetros que representan diferentes características o propiedades de los objetos, y luego se generaron automáticamente casos de prueba utilizando estos parámetros.

% Los test parametrizados son una técnica poderosa en la generación automatizada de casos de prueba, ya que permiten explorar diferentes combinaciones de parámetros y generar una variedad de casos de prueba de manera eficiente. En el contexto de la evaluación experimental, se utilizaron para evaluar y comparar el desempeño de diferentes técnicas en la generación de objetos y su impacto en la calidad de las pruebas.

% Para llevar a cabo este experimento, se creó una test suite parametrizada que serviría como marco de prueba para evaluar los distintos enfoques. Esta test suite parametrizada fue, básicamente, crear un test por método que contiene la clase y ejercitarlos con los objetos creados por las diferentes técnicas. Se utilizaron varias técnicas y herramientas para generar los objetos necesarios. En primer lugar, se empleó la conocida herramienta \texttt{Randoop} utilizando como es su forma estander, con todos los métodos disponibles en su API, lo que da lugar a una suite de pruebas tradicional generada por Randoop. En este caso no se utiliza una test suite parametrizada, ya que \texttt{Randoop}  no genera objetos, sino que crea sus propias tests suite.

% Ahora si, se utilizó una variante de Randoop llamada \texttt{R-Serialize} para serializar las secuencias de pruebas generadas anteriormente. Esto permitió generar objetos que, a su vez, se utilizaron para alimentar la test suite parametrizada, ampliando así el alcance de los casos de prueba. Estos objetos fueron generados utilizando todos los métodos de la API de Randoop, tal como se hizo en el enfoque anterior.

% Otra herramienta utilizada en el experimento fue una versión modificada de \texttt{Randoop}, diseñada específicamente para utilizar únicamente los métodos builders identificados previamente mediante nuestro enfoque (\ref{cap:builders}). Los objetos generados por esta variante especial de Randoop también se serializaron y se incorporaron a la test suite parametrizada, permitiendo una comparación directa entre los objetos generados por los builders identificados y los generados sin utilizar la información de estos métodos builders.

% Por último, se emplearon los objetos generados por la herramienta \texttt{BEAPI}. Estos objetos, creados utilizando su propia API, se integraron en la test suite parametrizada para evaluar su efectividad en la generación de casos de prueba. De esta manera, se obtuvo una visión completa y comparativa de las diferentes técnicas utilizadas en términos de generación de objetos y su impacto en la calidad de las pruebas.

% Mediante este enfoque meticuloso y riguroso, se buscó determinar la capacidad de los builders identificados para mejorar la generación automatizada de casos de prueba y, en última instancia, contribuir a la mejora de la calidad del análisis de programas. Los resultados obtenidos en esta evaluación experimental proporcionaron información valiosa sobre la utilidad y efectividad de los builders en el contexto del análisis de programas, abriendo así nuevas oportunidades para futuras investigaciones y desarrollos en este campo.

% A continuación, se realiza la comparativa de las diferentes herramientas utilizadas en el benchmark de \emph{java.util} previamente mencionado. Los casos de estudio son: \emph{HashSet}, \emph{HashMap}, \emph{TreeMap}, \emph{TreeSet} y \emph{LinkedList}. La tabla de resultados muestra varias métricas, como el tiempo en segundos (\texttt{GTime}) que es tiempo que lleva generar estos objectos/tests, la herramienta utilizada (\texttt{Tool}), la cantidad de objetos válidos generados (\texttt{Valid}) y la cantidad de objetos inválidos generados (\texttt{Invalid}).
% Es importante destacar que se definió un invariante para cada clase bajo evaluación, el cual establece qué estructuras son válidas e inválidas. Todos los objetos generados fueron sometidos a la prueba de estos invariantes para determinar su validez. Sin embargo, es importante mencionar que la técnica \texttt{Randoop} genera directamente una test suite en lugar de objetos individuales, por lo que no se aplica directamente el concepto de validez e invalidez a los casos generados por esta herramienta.

% Además, se realiza una comparación en función de la cantidad de tests generados por cada técnica. Esta medida se refleja en las columnas \texttt{Test}, \texttt{T(Seg)}, que indica la cantidad de tests que se ejecutan y sus respectivos segundos que son necesarios para ejecutar la test suite parametrizada, o no (en el caso de \texttt{Ranndop}), correspondiente.

% Finalmente, se evalúa la calidad de las test suites generadas mediante la comparación de la cobertura de ramas y la cantidad de mutantes eliminados. Estas métricas permiten determinar qué tan efectivas son las test suites en términos de su capacidad para cubrir diferentes ramas del código y eliminar mutantes generados para introducir fallas.

% A continuación analizaremos caso por caso con sus respectivas tablas.

% \input{tables/randoopvsBeapi}


% En esta etapa de la evaluación, se llevó a cabo un análisis exhaustivo para determinar la utilidad de los metodos builders identificados en el contexto del análisis de programas, específicamente en la generación automatizada de casos de prueba. Estos builders se consideran objetos clave que pueden ser utilizados como entradas en test parametrizadas. 
% Los test parametrizados son una técnica utilizada en el campo de la generación automatizada de casos de prueba para aumentar la eficiencia y la cobertura de las pruebas. En lugar de escribir casos de prueba individuales para cada escenario posible, los test parametrizados permiten definir un conjunto de parámetros que se utilizan para generar automáticamente múltiples casos de prueba.
% En el contexto de la evaluación experimental, se utilizó la técnica de test parametrizados para alimentar una test suite con objetos creados por diferentes técnicas. Esto significa que se definieron parámetros que representan diferentes características o propiedades de los objetos, y luego se generaron automáticamente casos de prueba utilizando estos parámetros.
% Los test parametrizados son una técnica poderosa en la generación automatizada de casos de prueba, ya que permiten explorar diferentes combinaciones de parámetros y generar una variedad de casos de prueba de manera eficiente. En el contexto de la evaluación experimental, se utilizaron para evaluar y comparar el desempeño de diferentes técnicas en la generación de objetos y su impacto en la calidad de las pruebas.

% Para llevar a cabo este experimento, se creó una test suite parametrizada que serviría como marco de prueba para evaluar los distintos enfoques. Esta test suite parametrizada fue, basicamente, crear un test por metodos que contiene la clase y ejercitalos con los objectos creados por las distintas técnicas. Se utilizaron varias técnicas y herramientas para generar los objetos necesarios. En primer lugar, se empleó la conocida herramienta \texttt{Randoop}, utilizando todos los métodos disponibles en su API estándar, lo que dio lugar a una suite de pruebas tradicional generada por Randoop.

% Además, se utilizó una variante de Randoop llamada \texttt{R-Serialize} para serializar las secuencias de pruebas generadas anteriormente. Esto permitió generar objetos que, a su vez, se utilizaron para alimentar la test suite parametrizada, ampliando así el alcance de los casos de prueba. Estos objetos fueron generados utilizando todos los métodos de la API de Randoop, tal como se hizo en el enfoque anterior.

% Otra herramienta utilizada en el experimento fue una versión modificada de \texttt{Randoop}, diseñada específicamente para utilizar únicamente los métodos builders identificados previamente mediante nuestro enfoque (\ref{cap:builders}). Los objetos generados por esta variante especial de Randoop también se serializaron y se incorporaron a la test suite parametrizada, permitiendo una comparación directa entre los objetos generados por los builders identificados y los generados sin utilizar la informacion de estos metodos builders.

% Por último, se emplearon los objetos generados por la herramienta \texttt{BEAPI}. Estos objetos, creados utilizando su propia API, se integraron en la test suite parametrizada para evaluar su efectividad en la generación de casos de prueba. De esta manera, se obtuvo una visión completa y comparativa de las diferentes técnicas utilizadas en términos de generación de objetos y su impacto en la calidad de las pruebas.

% Mediante este enfoque meticuloso y riguroso, se buscó determinar la capacidad de los builders identificados para mejorar la generación automatizada de casos de prueba y, en última instancia, contribuir a la mejora de la calidad del análisis de programas. Los resultados obtenidos en esta evaluación experimental proporcionaron información valiosa sobre la utilidad y efectividad de los builders en el contexto del análisis de programas, abriendo así nuevas oportunidades para futuras investigaciones y desarrollos en este campo.


%  A continuacion hacemos la comparativa de las diferentes tools, explicada en el parrafo anterior, en el benchmarks de \emph{java.util} utilizado previamente. Los casos de estudios son; \emph{HashSet},\emph{HashMap},\emph{TreeMap},\emph{TreeSet}, \emph{LinkedList}. La comparacion se realiza sobre, \texttt{GTime}, que representa el tiempo en milisegundos, \texttt{Tool} indica la herramienta utilizada, \texttt{Valid} muestra la cantidad de objectos válidos, \texttt{Invalid} muestra la cantidad de objetos inválidos que son generados. Para saber que objetos son válidos e inválidos, escribimos un invariante para cada clase bajo evaluacion, y este nos dice que estructura es valida y que estructura es invalida. Puede observar que para la tecnica \texttt{Randoo} esto no cuenta, ya que recuerde que aqui utilizamos la tool \texttt{Randoop} que generan test suite directamnete y no objectos. Ademas, BEAPI siempre va a generar objectos validos, esto se debe a que como estos objectos son construidos desde la API, no hay posibilidad de generar objectos invalidos. Vale aclarar que estos objectos tambien fueron puesto bajo prueba del mismo invariante que escribimos para cada clase. 
%  Ademas, comparamos de acuerdo a la cantidad de test que cada tecnica genera. Para realizar esta medida, en las tools que utilizan la test suite parametrizada, indica la cantidad de test que se ejecutan y es un valor de que tiene relacion con los objectos que se alimentan a la test suite. En base a esto \texttt{T(Seg)} es la cantidad de segundo que lleva ejecutar esta test suite.
%  Por ultimo, comparamos que tan buena es estas test suite, comprando la cobertura de ramas y de mutantes que matan.


