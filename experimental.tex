%!TEX root = main.tex
\chapter[Evaluaci\'on]{Evaluaci\'on}
\label{cap:experimental}

En los capítulos \ref{cap:builders} y \ref{cap:beapi} presentamos enfoques para
identificar automáticamente un conjunto de métodos generadores de objetos, y una
técnica novedosa de generación exhaustiva acotada basada en la API. 
En este capítulo, realizamos una evaluación experimental de las técnicas
mencionadas. La sección \ref{sec:experimentalIdentificacion} analiza experimentalmente los
algoritmos de identificación de generadores de objetos, mientras que en la
sección \ref{sec:experimentalBeapi} se evalúa la generación exhaustiva acotada basada en la API.


\section{Algoritmos de identificación de métodos generadores de objetos}
\label{sec:experimentalIdentificacion}

En esta sección, evaluamos experimentalmente los enfoques presentados en el
Capítulo \ref{cap:builders}. Analizaremos la eficiencia y precisión de cada
algoritmo utilizando las funciones objetivo introducidas en la
Sección \ref{sec:fitness}. Además, mostraremos resultados preliminares sobre cómo los 
métodos generadores de objetos pueden ser aprovechados por algunas herramientas
automáticas de análisis de software.

Con respecto a estas técnicas, las siguientes preguntas de investigación guían esta experimentación:

\begin{itemize}
\item \emph{RQ1}: ¿Qué tan eficientes son los algoritmos propuestos para
    identificar conjuntos de métodos generadores de objetos?
\item \emph{RQ2}: ¿Qué tan precisos son los algoritmos presentados para identificar métodos generadores de objetos?
\item \emph{RQ3}: ¿Cuál es el impacto de utilizar métodos generadores de objetos
    en el contexto del análisis automático de software?
\end{itemize}

\subsection{Configuraci\'on experimental}

\subsubsection{Casos de estudio.}


La evaluación se llevó a cabo sobre un conjunto de clases Java 
 que manipulan estructuras dinámicas complejas,
incluyendo: \verb"NCL" de Apache Commons Collections \cite{apache};
\verb"BinaryTree", \verb"BinomialHeap" y \verb"FibonacciHeap", extraídos
de \cite{Visser:2006}; y \verb"UnionFind", una implementación de conjuntos
disjuntos tomada de JGrapht \cite{jgrapht}.
También se incluyeron componentes de proyectos reales de software, como 
\verb"Lits" de la implementación de Sat4j \cite{sat4j}, utilizada previamente 
en \cite{Loncaric:2018}. \verb"Lits" maneja la representación interna de una
fórmula en el SAT solver (variables y literales).
%, y traduce literales desde DIMACS a la representación interna.
Por otro lado, se consideraron estructuras adicionales como \verb"Scheduler",
un planificador de procesos tomado del conjunto de benchmarks SIR \cite{sir},
y varias colecciones del paquete estándar \verb"java.util" de Java%
\footnote{\url{https://docs.oracle.com/javase/8/docs/api/java/util/
package-summary.html}}, incluyendo \verb"TreeMap", \verb"TreeSet",
\verb"HashMap", \verb"HashSet" y \verb"LinkedList". 

\input{tables/groundTruth}

Para evaluar la precisión de nuestros algoritmos, se analizaron los resultados 
manualmente. 
%conjunto de referencia o \emph{ground truth} que contiene los métodos
%generadores de objetos (\emph{MGO}) considerados minimales y suficientes en cada caso de estudio. 
Esta tarea implicó un análisis detallado de cada API, lo que representó un
trabajo considerable, especialmente en los casos más complejos. La
Tabla \ref{tab:groundTruth} provee ejemplos de métodos generadores de objetos
(\emph{MGO}) minimales y suficientes para cada caso de estudio (recordar que puede
haber más de un conjunto de \emph{MGO}s suficientes de tamaño minimal). 
%este conjunto de referencia, 
En la tabla también indicamos para cada clase el número total de métodos en su
API pública (\#API). 
%y los métodos identificados manualmente como generadores de objetos. 
%Cabe destacar que en algunos
%casos ciertos métodos pueden ser intercambiables (por ejemplo,
%\texttt{addFirst} y \texttt{addLast} en \texttt{NCL}).


\subsubsection{Algoritmos evaluados.}

\begin{itemize}
  \item \textbf{GA}: el algoritmo genético descrito en la Sección \ref{alg:approachGA}.
  \item \textbf{HC}: el algoritmo \emph{hill-climbing} presentado en la Sección \ref{alg:approachHC}.
\end{itemize}

Propusimos dos funciones objetivo para evaluar cada conjunto candidato de
métodos: 

\begin{itemize}
    \item \textbf{FGE}: función objetivo basada en la cantidad de objetos
        producidos por un generador exhaustivo acotado, presentada en la Sección \ref{sec:fitnessGE}.
\item \textbf{FRC}: función objetivo basada en la cobertura de código, descrita en la Sección \ref{sec:fitnessRandoop}. 
\end{itemize}

Cuando un algoritmo se evalúa con una función objetivo específica, usamos la notación 
\textbf{Algoritmo–Fitness}. 
Esto da lugar a cuatro algoritmos a evaluar: \textbf{GA–FGE}, \textbf{GA–FRC}, \textbf{HC–FGE}, 
\textbf{HC–FRC}. Notar que el trabajo sobre cómputo de métodos generadores
de Politano et. al. \cite{Ponzio19} presenta el algoritmo
\textbf{GA–FGE}. Las otras variantes fueron agregadas y evaluadas de forma
preliminar para esta tesis. 

Nuestros algoritmos requieren la configuración de diversos parámetros que influyen en su eficiencia y efectividad.
Estos valores se determinaron empíricamente, priorizando la eficiencia y la
precisión de las soluciones en experimentos realizados previamente.
En particular, para el algoritmo genético (\textbf{GA})
utilizamos una tasa de cruce de 0.40, una tasa de mutación de 0.05, 
selección por torneo con cuatro participantes, una población de 100 individuos y
un máximo de 20 generaciones por ejecución. 

Para las evaluaciones con la función objetivo basada en cobertura (\textbf{FRC}), 
por cada candidato lanzamos dos hilos ejecutando Randoop con distintas semillas. 
Además, fijamos un tiempo máximo de ejecución de Randoop de 30 segundos (en
cada hilo).
\pp{Decir algo de esto en la sección de la técnica FRC: Randoop introduce un componente 
    de aleatoriedad en la generación de tests. Para controlar este
    factor y garantizar la reproducibilidad de los resultados de los
    experimentos con los mismo conjunto de parámetros, fijamos dos semillas
distintas.}
% y controlamos la aleatoriedad mediante semillas. Randoop utiliza esta semilla para controlar la generación de tests y
% permitir la reproducción de resultados. Dado que Randoop introduce un grado
% considerable de aleatoriedad, optamos por utilizar dos semillas distintas para capturar la variabilidad introducida por el 
% generador aleatorio, registrándolas para permitir la reproducción de resultados. 
% Esta configuración proporciona una medida más robusta del desempeño de los algoritmos en contextos con 
% elementos no determin\'isticos.}

En el caso de la función objetivo basada en generación exhaustiva (\textbf{FGE}),
el parámetro central es el \emph{scope}, entendido como la cantidad máxima de
objetos por clase que puede crear el generador exhaustivo, y rangos de
valores para los tipos primitivos. 
En nuestros experimentos fijamos el scope en 5. Es decir, permitimos un máximo
de cinco objetos por clase, y 5 valores posibles para tipos primitivos (por ejemplo, 
para enteros usamos valores entre 0 y 4). Además, establecimos un tiempo máximo de ejecución 
de 30 segundos para la función objetivo.
%Además, para evitar diferencias espurias que no reflejan cambios semánticos en la representación, 
%omitimos campos internos que no afectan la estructura lógica de los objetos generados (por ejemplo, \texttt{modCount}, común en colecciones de Java), 
%el cual sólo contabiliza modificaciones internas sin alterar la estructura observada del objeto. \pp{Yo no pondría esto último porque no recuerdo que lo
%hayamos explicado en ningún lado.} \cacho{Hay un apartado pequeño, lo podemos charlar}

Para tratar con la aleatoriedad en los enfoques, cada algoritmo fue ejecutado
cinco veces con los parámetros mencionados anteriormente. Los resultados reportados en las siguientes secciones corresponden al promedio de las cinco ejecuciones. 

Cuando se desea evaluar la utilidad práctica de los métodos generadores de objetos en
RQ3, se usan siempre los métodos identificados automáticamente por el algoritmo
\textbf{HC–FGE}, que fue el más preciso en nuestros experimentos (ver Sección \ref{sec:experimentalIdentificacionPrecision}).
 
Todos los experimentos fueron ejecutados en una máquina con procesador Intel Core i7-6700
(4 núcleos, 8 hilos, frecuencia de hasta 4 GHz, 8 MB de caché, sexta generación),
8,GB de RAM y sistema operativo GNU/Linux.

% En términos de analizar el peor caso, con \textbf{HC+RC} se alcanza 5162s en
% \emph{TreeMap} (aprox 86 minutos) y 3842s en \emph{HashSet} para
% para \textbf{HC+GA} (aprox. 64 minutos). 
% Con \textbf{GE+GA}, se observa que el peor caso tarda 1803s (aprox 30 minutos) en \emph{HashMap} con \textbf{GE+RC} alcanza 113s para \emph{TreeMap} (aprox 2 minutos).

%Esto se debe a que los algoritmos
%de Hill Climbing comienzan desde abajo hacia arriba, considerando menos métodos antes de considerar más métodos, a diferencia de los 
%algoritmos genéticos, que generan sucesores mediante operadores de cruce y mutación. Esta estrategia favorece al algoritmo Hill Climbing 
%para encontrar los métodos mínimos en tiempos más cortos. Cabe mencionar que en casos de estudio donde el algoritmo genético supera al 
%Hill Climbing, se debe a que la clase bajo prueba tiene pocos métodos, lo que permite que los algoritmos evolutivos converjan más rápidamente que el algoritmo Hill Climbing.
%

\subsection{RQ1: Eficiencia del cómputo de métodos generadores de objetos}
\label{sec:eficienciaBuilders}

\input{tables/eficienciaBuilders}

Medimos la eficiencia como el tiempo promedio (en segundos) necesario para
finalizar la búsqueda de métodos generadores a partir de la API. 
%Evaluamos los dos
%algoritmos de búsqueda (\emph{GA} y \emph{HC}) combinados con dos funciones
%objetivo: \emph{FGE} (Función objetivo con generación exhaustiva acotada) y \emph{FRC} (Función objetivo con cobertura). 
Los resultados se muestran en la Tabla \ref{tab:eficiencia}. Cada fila
corresponde a un caso de estudio; debajo del nombre se indica el tamaño de la API
(\#API) y el número de métodos generadores del conjunto minimal (\#MGO). 
Las columnas se agrupan por algoritmo (GA y HC) y, dentro de
cada uno, por función objetivo (FGE y FRC). Para cada configuración reportamos el promedio de 5 ejecuciones.

A continuación, reportamos los tiempos promedio de ejecución (sobre los 12 casos
de estudio) de los algoritmos evaluados:
\begin{itemize}
  \item \textbf{GA+FGE}: 258.9 segundos
  \item \textbf{GA+FRC}: 1895.5 segundos
  \item \textbf{HC+FGE}: 19.7 segundos
  \item \textbf{HC+FRC}: 434.2 segundos
\end{itemize}

Estos valores muestran con claridad que \texttt{HC} es sistemáticamente más rápido
que \texttt{GA}. En particular, con \texttt{FGE} el tiempo de \texttt{HC} es 
aproximadamente 13 veces menor que el de \texttt{GA}, mientras que usando 
\texttt{FRC} es casi 3 veces menor. 
A nivel de casos
individuales, \texttt{HC} supera a \texttt{GA} en todos los casos tanto con
\texttt{FGE} como con \emph{FRC}. 
Esta diferencia se entiende por la propia naturaleza de los algoritmos.
El algoritmo genético (\textsf{GA}) mantiene y evoluciona poblaciones de
candidatos, lo que implica evaluar la función objetivo (que es costosa
computacionalmente) para muchos candidatos.
En cambio, \textsf{HC} sigue una estrategia \emph{greedy}: elige la mejor opción posible entre los sucesores del candidato actual. Esto lo vuelve más eficiente.
Sin embargo, en ciertos casos \textsf{HC} podría perder precisión en el cómputo de métodos
generadores. 

Comparando las funciones objetivo, \texttt{FGE}
es más rápida que \texttt{FRC}. En \texttt{GA}, FRC requiere en promedio
unas siete veces más tiempo que \texttt{FGE}; en \texttt{HC}, \texttt{FRC} requiere unas casi 32 veces
más tiempo que \texttt{FGE}. La función objetivo \texttt{FGE} es más rápida 
debido a que para computar MGO en nuestos casos de estudio es suficiente con generar objetos 
de tamaño relativamente pequeño (scope 5). Además, \texttt{FGE} utiliza BEAPI (ver
Capítulo \ref{cap:beapi}), que implementa la generación exhaustiva acotada
basada en la API de manera eficiente. 
En cambio, la función objetivo FRC debe generar una gran cantidad de tests en
muchos casos para lograr una buena cobertura (notar que FRC es aleatoria). 
Además, FRC requiere correr múltiples ejecuciones de Randoop con diferentes semillas de 
aleatoriedad (dos en estos experimentos), lo que la hace más costosa.
Todo esto explica porque FGE es más rápida en comparación con la función objetivo FRC
en estos experimentos.

Por otra parte, la función objetivo \textsf{FRC} tiene la ventaja de que no requiere 
que el usuario provea la definición del scope para la generación exhaustiva acotada. 
Además, \textsf{FRC} podría ser más escalable que \textsf{FGE} en casos donde
la cantidad de objetos sea demasiado grande para ser generados de manera
exhaustiva (por ejemplo, en clases con invariantes de representación más débiles
que los evaluados aquí).
De cualquier modo, los resultados preliminares muestran que los algoritmos 
son mucho más lentos con la función objetivo \textsf{FRC}. Se deja como trabajo 
futuro estudiar formas más eficientes de implementar la función \textsf{FRC}, y
realizar más experimentos para evaluar mejor su utilidad en la práctica 
\pp{No olvidarse de incluir esto en trabajos futuros}.

En conclusión, \texttt{HC+FGE} computa generadores de objetos muy rápidamente en
nuestros experimentos, en 20 segundos en promedio. \texttt{GA+FGE} y
\texttt{HC+FRC} son razonablemente rápidos para esta tarea, 
tomando en promedio poco más de 4 minutos, y 10 minutos, respectivamente.
La combinación de \texttt{GA+FRC} hace que el cómputo se vuelva mucho más lento.
Como el cómputo de generadores de objetos se realiza una sola vez sobre el
código de la clase bajo análisis, previo a los análisis de código que se quieran
llevar adelante (por ejemplo, generación de
objetos para testing, etc.), creemos que \textsf{HC+FGE}, \textsf{GA+FGE}, y
\textsf{HC+FRC} son algoritmos viables para su uso en la práctica. 

\subsection{RQ2: Precisión de los algoritmos de cómputo de métodos generadores de objetos}
\label{sec:experimentalIdentificacionPrecision}

\input{tables/eficaciaBuilders.tex}

Medimos la precisión como la capacidad de cada algoritmo para identificar
un conjunto suficiente y minimal de métodos generadores a partir de la API. 
La Tabla \ref{tab:efectividad} resume los tamaños promedios (sobre 5
ejecuciones) del conjunto de métodos identificado por cada algoritmo evaluado. 
Los resultados se analizaron manualmente 
(el número de MGOs minimales se indica como \#MGO en la tabla) y 
en todos los casos un conjunto de métodos generadores de objetos suficiente 
fue generado por todos los algoritmos. Sin embargo, como puede verse en la
tabla, algunos algoritmos incluyen algunos métodos irrelevantes en algunos casos.

Se observan tres patrones claros. Primero, \texttt{HC+FRC} y \texttt{HC+FGE}
encuentran un conjunto de métodos generadores suficiente y minimal para las doce clases analizadas.
Segundo, \texttt{GA+FGE} identifica conjuntos minimales de generadores de objetos
en todos los casos menos en \texttt{HashMap}, donde en algunas ejecuciones incluye
un método irrelevante.
Por último, \texttt{GA+FRC} obtiene resultados muy cercanos a un conjunto minimal 
en la mayoría de los casos, 
incluyendo algún método irrelevante en alguna de las ejecuciones para algunas clases (\texttt{NCL}, \texttt{TreeMap}, \texttt{TreeSet}, \texttt{HashSet}),
pero tiene un costo computacional considerablemente mayor, por lo que no resulta 
competitivo en términos de eficiencia.

La pérdida de precisión del algoritmo genético (\texttt{GA}) puede deberse a la
estocasticidad propia de la búsqueda (selección, cruce, mutación). Otro factor
que podría influir es el criterio de corte del algoritmo que detiene la evolución 
cuando el mejor fitness no mejora durante cinco generaciones consecutivas (ver
Sección \ref{alg:approachGA}). Este criterio es eficaz para acelerar la
terminación del algoritmo, pero puede resultar en soluciones subóptimas (como
se ve en algunos casos). Se deja como trabajo futuro la optimización de los
parámetros del algoritmo genético en busca de mejorar su precisión \pp{Poner en
trabajos futuros}. 

En síntesis, los algoritmos presentados producen conjuntos suficientes
de métodos generadores de objetos en los casos considerados. Además, 
\textbf{HC} genera conjuntos minimales en todos estos casos, con ambas funciones
objetivo. Además, \textbf{HC+FGE} es el algoritmo más eficiente, lo que lo
convierte en una buena opción para computar métodos generadores de objetos
rápidamente.

\subsection{RQ3: Impacto de los métodos generadores de objetos en el análisis de
software}

\subsubsection{Generación de objetos para testing}
\label{sec:object_builders_randoop}

\input{tables/objectsBuilders.tex}

En esta sección evaluamos la utilidad práctica de los métodos generadores 
de objetos en el contexto de la generación automática de objetos usando Randoop. 
Para esto utilizamos los generadores de objetos identificados con el 
algoritmo de \emph{Hill Climbing} (Sección \ref{alg:approachHC}) y la función objetivo 
basada en la cantidad de objetos producidos por un generador exhaustivo acotado 
(Sección \ref{sec:fitnessGE}). En la sección siguiente veremos que estos objetos 
pueden ser utilizadas luego como entradas en tests basados en propiedades.
 
Para este experimento, modificamos la herramienta \emph{Randoop} (ver
Sección \ref{sec:feedback-directed-test-gen}) para que almacene los
objetos producidos por la ejecución de los tests. La evaluación consiste en
utilizar la variante mencionada de Randoop con toda la API de la clase bajo
análisis. Denominamos \texttt{RSer-API} a esta técnica de generación de objetos 
(una versión de Randoop que usa la API completa y serializa objetos). Por otro
lado, se utiliza otra versión que instruye a Randoop para que utilice sólo los MGOs para
generar tests, denominada \texttt{RSer-MGO} (una versión de Randoop que usa solo MGOs 
y serializa objetos). 

Para estos enfoques, definimos tres valores de presupuesto de tiempo para la generación de 
objetos: 60, 120 y 180 segundos. En este experimento, medimos la cantidad de objetos distintos
generados por los tests (\emph{Num. Objetos}) (usamos los métodos \texttt{equals()} de las 
clases evaluadas para descartar objetos iguales).
% \cacho{Además, reportamos el tamaño del objeto más grande generado por las técnicas
% \pp{Por qué faltan los tamaños de los objetos acá? En el paper los habíamos
% puesto, o no?} (calculamos los tamaños de objetos con el método
% \texttt{size()} de las clases bajo análisis). }
% \cacho{Agregar table, URGENTE!. escribir esa parte}

Los resultados obtenidos se resumen en la Tabla \ref{tab:results-obj}.
En promedio, para un mismo presupuesto de generación \texttt{RSer-MGO} 
produce 1.53 veces más objetos que \texttt{RSer-API}. 
Desagregando por presupuesto, para 60 segundos con métodos generadores 
se obtienen, aproximadamente, dos veces más objetos, que con toda la API. 
Con un presupuesto de 120 segundos, se generan aproximadamente 1.5 
veces más objetos con \texttt{RSer-MGO} en comparación con \texttt{RSer-API},
Finalmente, para 180 segundos, se construyen casi 1.37 veces más objetos 
con \texttt{RSer-MGO} en comparación con \texttt{RSer-API}. 
%Esta diferencias es debido a que, al aumentar el presupuesto, Randoop tiene más tiempo para explorar la API completa y generar más objetos,
%reduciendo la ventaja relativa de usar sólo los métodos generadores de objetos.
%

% \cacho{Todo esto es nuevo}
% Utilizamos los objectos que produce Randoop con los dos enfoques para medir la diversidad de los objectos generados.

% \cacho{Para observar la calidad de los objectos, se mide en la seccion \ref{sec:parametrizedTesting}, donde consideramos no solo la tecnica de Randoop y Randoop con MGO sino que, tambien, consideramos BEAPI.}

%La ventaja es consistente todos los casos. Si
%promediamos por clase (sobre los tres presupuestos), \texttt{MGO} es mejor
%en todas las clases, con ganancias especialmentes marcadas en \emph{NCL} (casi 15 veces mas en promedio por clase) y \emph{LinkedList}
%(casi 10 veces). 

En resumen, el uso de métodos generadores de objetos mejora significativamente la
capacidad de generación de objetos distintos de Randoop.
%, para un mismo presupuesto de tiempo.
Es esperable que al generar objetos más diversos, los objetos producidos por
\texttt{RSer-MGO} ejerciten mejor el código bajo test que los generados por
\texttt{RSer-API} si se los utiliza para testing. 
La calidad de los objetos generados se evalúa en más detalle en la sección siguiente,
donde los utilizamos como entradas de tests basados en propiedades. 

\subsubsection{Testing basado en propiedades}
\label{sec:experiments-builders-parameterized}

\begin{lstlisting}[caption={Tests basados en propiedades para \texttt{remove} de
\texttt{LinkedList}},label={lst:parametrizadoLinkedList},language=Java,captionpos=b,float]
public void removeTest1(List l, int i) {
    int oldSize = l.size();
    if (l.contains(i)) {
        boolean res = l.remove(i);
        assertTrue(res);
        assertEquals(l.size(), oldSize-1);
    }
}

public void removeTest2(List l, int i) {
    int oldSize = l.size();
    if (!l.contains(i)) {
        boolean res = l.remove(i);
        assertFalse(res);
        assertEquals(l.size(), oldSize);
    }
}
\end{lstlisting}


En esta sección, realizamos un experimento preliminar para determinar la 
utilidad de los métodos generadores de objetos en el contexto del testing basado 
en propiedades. En lugar de definir casos de tests específicos para testear el
software (como en el testing unitario), los tests basados en propiedades consisten en 
identificar propiedades que debe satisfacer el programa, y escribir tests
que verifiquen estas propiedades para cualquier entrada. Por ejemplo, para el
método \texttt{remove} de la clase \texttt{LinkedList} sabemos que, si eliminamos
un elemento que ya pertenece a la lista, \texttt{remove} retorna
\texttt{true} y la lista resultante tiene un elemento menos (el elemento
eliminado). Además, si intentamos eliminar un elemento que no está en la lista, 
\texttt{remove} retorna \texttt{false}, y el tamaño de la lista no cambia. La
Figura \ref{lst:parametrizadoLinkedList} muestra dos tests que verifican estas
propiedades.

Para este experimento, se utilizaron las clases más complejas de nuestra
evaluación experimental, esto es, las clases del paquete \texttt{java.util},
debido a su API rica en cantidad de métodos: \texttt{HashSet}, \texttt{HashMap},
\texttt{TreeMap}, \texttt{TreeSet} y \texttt{LinkedList}. Para estas clases, 
escribimos manualmente tests basados en
propiedades intentando maximizar cobertura y puntaje de mutación para los
métodos públicos de las clases. Como escribir estos tests manualmente es una
tarea laboriosa, nos limitamos sólo a estas clases para este experimento
preliminar. Se deja como trabajo futuro extender la evaluación a un conjunto más
amplio de casos de estudio. \pp{Más trabajos futuros}.

Entre las herramientas evaluadas incluimos la versión estándar de \textsf{Randoop}, que 
simplemente genera una suite de tests (y se evalúa directamente su cobertura y puntaje de
mutación; los tests parametrizados no se usan para esta herramienta). Además,
usamos las versiones modificadas de \textsf{Randoop} para generar objetos 
descritas en la sección anterior: \texttt{RSer-API} que utiliza toda la API para la 
generación y serializa los objetos producidos por la ejecución de los tests, y
\texttt{RSer-MGO} que utiliza sólo los MGOs para generar tests y serializa los
objetos. Los objetos generados por estas técnicas se usan como entradas para
instanciar los parámetros de los tests basados en propiedades (como los que se 
    muestran en la Figura \ref{lst:parametrizadoLinkedList}). 
Si los tests parametrizados requieren valores de tipos primitivos (como en la
Figura), para cada objeto de entrada (generado por la técnica considerada) 
se elige un valor aleatorio dentro de un rango pequeño de valores predefinidos. 

Las métricas consideradas fueron: el tiempo 
de generación (\textsf{GTime}), este es el tiempo máximo que se le da a cada
herramienta para ejecutarse, la cantidad de tests producidos y la cantidad de
objetos generados (\textsf{Test/Obj}), el 
tiempo de ejecución de la test suite (\textsf{Time}), la cobertura de ramas alcanzada 
(\textsf{Ramas}), y la cantidad de mutantes que mata la suite (\textsf{Mutación}). 
Es importante destacar que la test suite basada en propiedades es común a los enfoques 
\texttt{RSer-API}, \texttt{RSer-MGO}. En el caso de \textsf{Randoop}, la herramienta 
genera su propia test suite, por lo que los resultados no son directamente
comparables con las otras dos técnicas, pero sirve para tener una referencia de
los resultados que pueden obtener las herramientas automáticas de generación de
tests por sí solas.

\input{tables/randoopvsBuilders}

En \texttt{LinkedList} (\ref{tab:linkedListTools_builders}), la estructura más simple de todas las
consideradas, \texttt{RSer-MGO} y \texttt{RSer-API} logran la mayor cobertura en
60 segundos de generación. \texttt{RSer-MGO} obtiene el puntaje de mutación
más alto en 120 segundos, mientras que \texttt{RSer-API} lo hace en 180 segundos.

En los casos restantes, que implementan en estructuras de datos más complejas, 
la diferencia es significativamente más grande a favor de \texttt{RSer-MGO}.
En \texttt{HashSet} (\ref{tab:hashSetTools_builders}), \texttt{RSer-MGO} y
\texttt{RSer-API} obtienen la mayor cobertura en 120 segundos. 
\texttt{RSer-MGO} obtiene el puntaje de mutación más alto en 120 segundos, 
mientras que \texttt{RSer-API} nunca llega a lograr el puntaje más alto.
En \texttt{HashMap} (\ref{tab:hashMapTools_builders}),
\texttt{TreeMap} (\ref{tab:treeMapTools_builders}) y \texttt{TreeSet} (\ref{tab:treeSetTools_builders}), las mejoras en cobertura y mutantes muertos por parte de \texttt{RSer-MGO} son
significativas para todos los tiempos de generación. Además, \texttt{RSer-MGO}
siempre llegan a los puntajes más altos que obtienen las técnicas consideradas, 
mientras que \texttt{RSer-API} no logra los puntajes más altos ni siquiera para 
el máximo tiempo de generación (180 segundos).

Como se esperaba, las técnicas que usan los tests basados en
propiedades escritos manualmente superan significativmamente tanto en cobertura
de ramas como en puntaje de mutación a \textsf{Randoop}, que no requiere de ningún 
tipo de asistencia manual.

En conclusión, la identificación de métodos generadores de objetos permite la definición de 
la técnica \texttt{RSer-MGO}, que muestra ser más efectiva en este experimento
preliminar para producir entradas de calidad para tests basados en propiedades. 
En particular, debido a que típicamente \texttt{RSer-MGO} logra matar más mutantes, 
los objetos generados por esta técnica tienen el potencial de mejorar la capacidad de 
detección de fallas respecto de los objetos producidos por las otras técnicas evaluadas.


\subsubsection{Verificación acotada usando JPF}
\label{sec:experiments-jpf-driver}

En un tercer experimento, evaluamos el impacto del uso de métodos generadores
de objetos en la eficiencia de la verificación acotada usando \emph{Java PathFinder}
(JPF) \cite{Visser:2005} (explicado en la Sección \ref{sec:jpf}).
Para este experimento preliminar, usamos como casos de estudio algunas clases
del paquete \texttt{java.util}. 
Los experimentos consisten en verificar de manera exhaustiva acotada que, dada
una estructura de datos, luego ejecución de un método de la estructura de datos
se satisface el invariante de representación (\texttt{repOK}) de la estructura. 
Por ejemplo, para el método \texttt{put} de \texttt{TreeMap} escribimos el
programa de la Figura \ref{lst:propiedadJPF} para realizar la verificación con
JPF.


\begin{lstlisting}[caption={Verificando el método put de TreeMap con JPF},label={lst:propiedadJPF},language=Java,captionpos=b]
    public static void main(String[] args) {
       int scope = 3;
       TreeMap t = generateStructure(scope);
       t.put(Verify.getInt(0,scope),Verify.getInt(0,scope));
       assert t.repOK();
    }
\end{lstlisting}

El programa primero construye todos los árboles de tamaño acotado usando un
driver en la línea 3 (método \texttt{generateStructure}). Luego, ejecuta el
método bajo análisis en la línea 4 (\texttt{put}), y por último verifica que se
satisfaga el \texttt{repOK} luego de la ejecución del método en la línea 5.

Para la evaluación consideramos los dos enfoques presentados en la Sección \ref{sec:jpf-builders} para construir los drivers. El enfoque \textsf{API} consiste en
utilizar toda la API. Recordar que un ejemplo de driver generado con este enfoque se 
presenta en la Figura \ref{lst:driverAPI}. Por otro lado, el enfoque
\textsf{MGO} consiste en usar sólo los métodos generadores de objetos para
construir los drivers, como se muestra en el ejemplo de la Figura \ref{lst:driverBLD}.
Como en los experimentos anteriores, los métodos generadores de objetos fueron precomputados 
usando el algoritmo \emph{Hill Climbing} (Sección \ref{alg:approachHC}) 
con la función objetivo basada en generación exhaustiva acotada (Sección \ref{sec:fitnessGE}).

\input{tables/tableJPFBuilders.tex}
\input{tables/tableJPFBuilders1.tex}

Los resultados de los experimentos, presentados en las
Tablas \ref{tab:results-jpf} y \ref{tab:results-jpf1}, muestran que los
controladores construidos a partir de los métodos generadores objetos (MGO) permiten
una mejora significativa en la eficiencia y la escalabilidad de JPF. 
Se observa una reducción significativa en el tiempo de análisis y en la cantidad
de estados explorados por JPF. Además, si se cuenta con métodos
generadores de objetos suficientes (como sucede para las estructuras evaluadas), 
sólo se dejan de explorar caminos de ejecución que llevan a objetos redundantes. 
Es decir, no se compromete la exhaustividad del análisis acotado al usar MGOs.

%Para focalizar la comparación en los distintos casos, 
%consideramos en cada clase–método todos los niveles de \emph{scope} que finalizan sin \emph{timeout}
%De esta manera, los resultados reflejan el comportamiento completo de cada técnica en el rango alcanzable por \emph{JPF}. 
%Cuando \textsf{API} alcanza \emph{timeout} en un determinado \emph{scope}, ese resultado indica un límite práctico 
%de escalabilidad para dicha variante
% , mientras que \textsf{MGO} (Métodos generadores de Objetos) en general logra completar en el mismo \emph{scope}

En las tablas se muestran los resultados para los scopes en los que
alguna de las técnicas termina antes de llegar al timeout.
Considerando los \emph{scopes} para los que ambas técnicas terminan
antes del timeout, tenemos un total de 60 casos. 
En promedio para estos 60 casos, \textsf{MGO} explora 234513 estados en 29.5 segundos, mientras que \textsf{API} explora 3302177 estados en 287.3 segundos. Esto implica que \textsf{MGO} reduce
el espacio de estados a explorar en 
92.9\%\ y el tiempo en 89.7\%\ aproximadamente respecto de \textsf{API}. 
Se observa tambien que \textsf{API} alcanza el timeout en 4/60 combinaciones
(6\%).

%Bajo el mismo \emph{scope}, \textsf{API} explora aproximadamente 1308\%\ mas de estados y tarda 984\%\ mas de tiempo que \textsf{MGO}.
%En cuanto a la interpretación, 

En resumen, los resultados muestran evidencia preliminar de que construir \emph{drivers}
exclusivamente con los métodos generadores de objetos reduce significativamente la cantidad 
de estados que debe explorar JPF, y consecuentemente reduce el tiempo de la verificación. Esto le
permite a nuestro enfoque \textsf{MGO} realizar la verificación para scopes más grandes
en varios de los casos analizados. 
Al mismo tiempo, la suficiencia de los MGOs
garantiza que se preserva la capacidad del driver \textsf{MGO} de producir todos 
los objetos distintos que genera el driver API, es decir, no se compromete la
exhaustividad de la verificación acotada al usar \textsf{MGO}.


\section{Generación exhaustiva acotada basada en la API}
\label{sec:experimentalBeapi}

En esta sección evaluamos empíricamente la técnica de generación exhaustiva
acotada BEAPI. 
Examinamos su eficiencia (tiempo de generación y escalabilidad respecto de los
\emph{scopes}) con respecto a enfoques existentes, y el impacto de las optimizaciones de BEAPI sobre su desempeño.
También realizamos una evaluación preliminar que muestra que BEAPI puede ser útil para chequear la
consistencia entre los objetos que puede generar la API y las restricciones que
imponen las especificaciones formales de invariantes de representación de estructuras (\texttt{repOK}). 
Por último, evaluamos preliminarmente que tan buenos son los objetos producidos por BEAPI en el
contexto del \emph{testing} basado en propiedades.

Así, en la evaluación de BEAPI buscaremos responder las siguientes preguntas de
investigación:

\begin{itemize}
\item \emph{RQ4}: ¿Qué tan eficiente es BEAPI para la generación exhaustiva acotada de objetos?
\item\emph{RQ5}: ¿Cuál es el impacto de las optimizaciones propuestas en el rendimiento de BEAPI?
\item\emph{RQ6}: ¿Puede BEAPI ayudar a encontrar discrepancias entre especificaciones de invariantes de clase (repOK) y la capacidad de generación de objetos de la API?
\item\emph{RQ7}: ¿Qué tan efectivos son los objetos producidos por BEAPI para el testing basado en propiedades?  
\end{itemize}

\subsection{Configuración experimental}
\label{sec:confExperimentalBeapi}

Como casos de estudio, utilizamos implementaciones de estructuras de datos
tomadas de tres benchmarks existentes: \texttt{Kiasan} \cite{Deng06}, 
\texttt{FAJITA} \cite{Abad13}, \texttt{ROOPS} \cite{Roops}. También
incluimos las clases utilizadas en la evaluación de
\texttt{Korat} \cite{Boyapati02}, y un conjunto de clases de proyectos reales de
software que la denominamos \texttt{Real World}, donde se incluyen clases de
\texttt{java.util} (TreeMap, TreeSet, LinkedList, Hasmap), de \texttt{Apache Commons
Collections} (NodeCachingLinkedList) y un Scheduler de SIR.

Estos casos de estudio cubren una amplia variedad de estructuras de datos complejas, tales como listas doblemente enlazadas, árboles binarios de búsqueda, 
árboles rojo-negro, heaps binomiales, heaps de Fibonacci, entre otras. Una característica clave 
de estos casos de estudio es que incluyen especificaciones formales de invariante de clase (métodos 
\texttt{repOK}) escritas por sus propios autores, lo que resulta fundamental
para realizar las comparaciones con Korat, que requiere invariantes para
realizar la generación, y para los experimentos para responder a RQ6.

Para la evaluación se utilizó el prototipo de la herramienta BEAPI, que se
describe en la literatura en los trabajos de Politano et. al. \cite{Politano23,
Politano24}.

En BEAPI, la identificación de métodos generadores de objetos se realiza en una 
etapa previa a la generación exhaustiva, utilizando el algoritmo de 
\textsf{Hill Climbing} (Sección \ref{alg:approachHC}), usando la función
objetivo basada en la cantidad de objetos producida por un generador exhaustivo
acotado (Sección \ref{sec:fitnessGE}). Para la función objetivo se usa el mismo BEAPI 
como generador.
Para la identificación de métodos generadores de objetos, se empleó un
\emph{scope} fijo de 5 en todos los casos.
%; con este scope se obtuvieron 
%generadores de objetos suficientes en todos los casos. 
 El tiempo máximo requerido por el proceso de identificación fue de 68 segundos en el peor caso 
 (en el benchmarks \texttt{Real World}, el caso de estudio TreeMap, que tiene 32 métodos en su API).
% con 32 métodos, el más costoso). 
% El tiempo máximo requerido por el proceso de identificación fue de 
% 26 segundos en los benchmarks \textsf{Kiasan}, \textsf{FAJITA}, \textsf{ROOPS} y 
% \textsf{Korat} (siendo el caso más costoso SLL de ROOPS, con 11 métodos), 
% y de 68 segundos en los benchmarks \textsf{Real World} (siendo TreeMap, 
% con 32 métodos, el más costoso). 
En todos los casos verificamos manualmente que los métodos identificados 
constituyeran un conjunto suficiente de métodos generadores de objetos.

Nótese que la generación exhaustiva acotada suele realizarse de manera
incremental, para scopes cada vez más grandes, y que los generadores de objetos identificados 
pueden reutilizarse en las distintas ejecuciones. Por lo tanto, los 
tiempos de identificación de generadores de objetos se amortizan a través de las 
diferentes ejecuciones, lo que dificulta calcular cuánto agregan dichos tiempos 
a los tiempos totales de ejecución de BEAPI en cada caso. Por esta razón, no incluimos 
los tiempos de identificación de builders en los tiempos de ejecución de BEAPI en ninguno 
de los experimentos de las secciones a continuación. Además, para los scopes más
grandes y las estructuras más complejas —que son los más importantes para
evaluar la eficiencia de la generación exhaustiva acotada— el tiempo de identificación 
de métodos generadores de objetos es pequeño en relación con los 
tiempos de generación.

En la comparación de eficiencia con técnicas existentes (RQ4) se utilizó Korat,
que es la herramienta existente de generación exhaustiva acotada más eficiente
\cite{Siddiqui09}. Tener en cuenta que, como
Korat requiere de repOKs, se emplearon los \texttt{repOK}s provistos por los autores 
de cada caso de estudio. 
En algunos casos se identificaron errores o especificaciones incompletas en los \texttt{repOK}s; 
estos se señalan de forma explícita en los resultados y se analizan en más
detalle en la Sección \ref{sec:existing-specs-analysis} (RQ6).

Para RQ7 se comparan los objetos generados por \textsf{BEAPI} con una técnica de generación
aleatoria (\textsf{Randoop}), y las dos variantes de Randoop que serializan los objetos
generados por la ejecución de los tests descritas en la Sección
\ref{sec:object_builders_randoop}. Una de ellas sólo usa métodos generadores de
objetos (\textsf{RSer-MGO}), y otra que usa la API completa (\textsf{RSer-API}). 

Los experimentos se llevaron a cabo en una computadora con procesador Intel Core i7-8700
(6 núcleos, 12 threads, frecuencia de hasta 4.6 GHz, 12 MB de caché, octava generación),
16 GB de RAM y sistema operativo Ubuntu Linux 20.04.

Para cada ejecución individual se estableció 
un tiempo máximo de 60 minutos. Todos los experimentos fueron diseñados para ser reproducibles, 
y el artefacto experimental con el código fuente se encuentra disponible en: \url{https://github.com/mpolitano/bounded-exhaustive-api}

\subsection{RQ4: Eficiencia de BEAPI}
\label{sec:beapi-eficiencia}

En esta sección comparamos BEAPI y Korat en términos de tiempo 
de ejecución, número de estructuras generadas y exploradas, y escalabilidad con
respecto a los \emph{scopes}.
Los resultados experimentales se muestran en la Tabla \ref{table:korat-beapi} y \ref{table:korat-beapi1}.

%La Tabla presenta una muestra representativa de los resultados para los benchmarks
%\texttt{Korat}, \texttt{FAJITA}, \texttt{ROOPS} y \texttt{Kiasan} (intentamos mantener la misma proporción de casos buenos y malos para cada técnica en los datos que reportamos). \pp{Los casos
%restantes se pueden encontrar online \cite{}. Queda raro que esta tabla sea la
%única que esté recortada en la tesis. hay que ver si se puede hacer algo al
%respecto.}.
Para cada clase evaluada, informamos los tiempos de generación 
(en segundos, columna \textsf{Time}), la cantidad de estructuras válidas generadas (\textsf{Generated}) y el total de estructuras exploradas (\textsf{Explored}) por cada técnica. 
Incluimos, para cada enfoque, los \textsf{scopes} más grandes (columna \textsf{S})
alcanzados por las técnicas antes de llegar al tiempo límite de 60 minutos. De
esta manera, si surgieran problemas de escalabilidad, podrían identificarse fácilmente. Vale
aclarar que no se utilizó el benchamark \texttt{Real World} en esta sección, ya que
no se dispone de \texttt{repOK}s provistos por los autores de las clases, 
lo que impide una comparación justa con \texttt{Korat}.

Para obtener resultados de rendimiento adecuados para \textsf{BEAPI}, realizamos
testing exhaustivo de los métodos de la API de las clases para asegurar que
sean correctos. No intentamos modificar los \texttt{repOK} de ninguna manera, ya que eso alteraría el rendimiento de \textsf{Korat}, y uno de nuestros objetivos aquí es evaluar el rendimiento de \textsf{Korat} utilizando \texttt{repOK}s escritos por distintos programadores.

Es esperable que existan diferencias en las estructuras exploradas, dado que los
espacios de búsqueda de \textsf{Korat} y \textsf{BEAPI} son distintos. Sin
embargo, para el mismo caso de estudio y el mismo scope, cabría esperar que
ambos enfoques generen la misma cantidad de estructuras válidas. Esto ocurre en
la mayoría de los experimentos, con dos tipos notables de excepciones. Primero,
hay casos en los que el \texttt{repOK} contiene errores; estos casos aparecen
sombreados en las tablas (los errores de los invariantes se analizan en 
profundidad en la Sección \ref{sec:existing-specs-analysis}).

Segundo, la noción ligeramente diferente de scope
utilizada por cada técnica puede causar discrepancias. Esto solo sucede para
Árboles Rojo-Negro (\texttt{RBT}) y montículos de Fibonacci (\texttt{FibHeap}),
que se muestran en negrita. En estos casos, ciertas estructuras de tamaño $n$
solo pueden generarse a partir de estructuras más grandes, mediante inserciones
seguidas de eliminaciones y luego nuevas inserciones, para producir
configuraciones de balance específicas. \textsf{BEAPI} descarta secuencias generadas 
tan pronto como superan el tamaño máximo de la estructura, por lo que no puede generar 
estas estructuras.

En resumen, la Tabla \ref{table:korat-beapi} muestra que para el benchmark \texttt{KORAT} la
herramienta \textsf{Korat} es más veloz en 4 de 6 clases; 
en \texttt{FAJITA}, \textsf{BEAPI} es más rápido que \textsf{Korat} en 4 de 7
casos.
La Tabla \ref{table:korat-beapi1} indica que en
\texttt{ROOPS}, \textsf{BEAPI} es más rápido en 6 de 7 casos; 
en \texttt{Kiasan}, \textsf{Korat} es más rápido en 7 de 8 clases.

Observamos que \textsf{BEAPI} muestra un mejor rendimiento en estructuras con
invariantes de representación más fuertes, 
como \texttt{RBT} y árboles binarios de búsqueda (\texttt{BST}); a menudo estos
casos presentan una cantidad de relativamente más chica de estructuras válidas.
En los casos con invariantes más débiles, en los que la cantidad de estructuras
válidas crece más rápidamente con respecto al scope, como las listas doblemente
enlazadas (\texttt{DLList}), \textsf{Korat} suele ser más eficiente. Un mayor 
número de estructuras válidas implica que \textsf{BEAPI} debe crear un número
mayor de secuencias de test totales, lo que impacta negativamente en su rendimiento.

Como era de esperarse, la manera en que están escritos los \texttt{repOK} tiene
un impacto significativo en el rendimiento de \textsf{Korat}. Por ejemplo, para
montículos binomiales (\texttt{BinHeap}), \textsf{Korat} alcanza scope 8 usando
el \texttt{repOK} de \texttt{Roops}, scope 10 con el de \texttt{FAJITA} y scope
11 con el de \texttt{Korat} (todos equivalentes en cuanto a las estructuras
generadas). En la mayoría de los casos, los \texttt{repOK} del benchmark de
\texttt{Korat} resultan en un mejor rendimiento, ya que están afinados
específicamente para su uso con \textsf{Korat} (estos \texttt{repOK}s producen
    que la estrategia de poda de estructuras inválidas de \textsf{Korat} se
ejecute con más frecuencia, reduciendo significativamente la cantidad de
estructuras exploradas, lo que mejora su rendimiento). 

En conclusión, \textsf{BEAPI} presenta una eficiencia y una escalabilidad comparable a la de
\textsf{Korat} (gana en algunos casos y pierde en otros), sin requerir de un
\texttt{repOK} para la generación. Y por lo tanto, \textsf{BEAPI} no depende de que el
\texttt{repOK} esté optimizado para la generación exhaustiva acotada, que como se puede
ver en \texttt{ROOPS}, ciertos \texttt{repOK} pueden resultar muy
contraproducentes para la performance de \textsf{Korat}. 

\input{tables/tableEficienciaBEAPI.tex}
\input{tables/tableEficienciaBEAPI1.tex}

\subsection{RQ5: Impacto de las optimizaciones propuestas}
\label{sec:optimizations}

\input{tables/optimizaciones}

En esta pregunta de investigación evaluamos el impacto que tienen las optimizaciones propuestas 
de \textsf{BEAPI} en su rendimiento. 
Concretamente, analizamos cuatro configuraciones de la herramienta:
\textsf{CI/MGO}, que habilita tanto 
la coincidencia de estados (\textsf{CI}) como la identificación de métodos
generadores de objetos (\textsf{MGO}); 
\textsf{CI}, que activa únicamente la coincidencia de estados; \textsf{MGO}, que habilita únicamente 
la identificación de métodos generadores de objetos; y \textsf{NoOPT}, que desactiva ambas optimizaciones.

Las Tablas \ref{tab:results-fajita}, \ref{tab:results-roops}, \ref{tab:results-korat}, \ref{tab:results-kiasan}, \ref{tab:results-realWorld} resumen los resultados de este experimento.

Los resultados muestran de forma contundente que la configuración sin optimizaciones (\textsf{NoOPT}) 
tiene un rendimiento deficiente en general, inclusive en los estudios de caso más simples y
para los scopes más pequeños. 
Los \textsf{scopes} pequeños (3) resultan inadecuados para generar conjuntos de
objetos útiles para revelar errores. 
\textsf{NoOPT} evidencia los problemas de escalabilidad que enfrentan 
las técnicas de generación exhaustiva en presencia de APIs ricas y estructuras complejas.

De las optimizaciones propuestas, la coincidencia de estados (\textsf{CI})
es la que mayor impacto tiene en la eficiencia de BEAPI (comparar los resultados
de NoOPT y CI). Por sí sola, permite escalar a scopes considerablemente más
grandes y reduce los tiempos de ejecución en varios órdenes de magnitud con respecto a \textsf{NoOPT}. 
Esto muestra que eliminar secuencias de test redundantes, es decir, que producen
los mismos objetos, es crucial para la eficiencia de BEAPI.

La segunda optimización, el uso de métodos generadores de objetos
(\textsf{MGO}), es mucho más relevante en casos donde el número de métodos en la API es grande (más de 10), y de manera notable en las estructuras de datos del mundo real (con 20 o más métodos). En 
estructuras del benchmark \texttt{Real World} como \texttt{TreeMap}, \texttt{LinkedList} o \texttt{HashMap}, donde existen 
decenas de métodos públicos, limitar la generación de secuencias a combinaciones 
de MGOs tiene un impacto significativo en la eficiencia del proceso de
generación. Como era esperable, en las clases que tienen pocos métodos, el impacto de utilizar MGOs es 
relativamente pequeño.

El mayor beneficio se observa en la configuración \textsf{CI/MGO}, donde ambas optimizaciones están 
habilitadas. En esta configuración, \textsf{BEAPI} es significativamente más
rápido, y alcanza scopes mucho mayores que \textsf{NoOPT} en todos los casos
analizados.
Por ejemplo, en \textsf{ROOPS}, \textsf{CI/MGO} es más de un orden de magnitud
más rápido que \textsf{CI} en AVL y RBT, y alcanza al menos un scope adicional
en NCL, LList, y RBT. 
En \texttt{Real World}, el uso de MGOs permitió que \textsf{CI/MGO} escalara a
scopes significativamente mayores en todos los casos excepto TMap y TSet, donde igualmente \textsf{CI/MGO} muestra
una reducción importante en los tiempos de ejecución. 
En general, las optimizaciones propuestas tienen un impacto crucial en el
rendimiento y la escalabilidad de BEAPI, y ambas deben habilitarse para obtener
buenos resultados en la práctica.


\subsection{RQ6: Análisis de invariantes de representación usando BEAPI}
\label{sec:existing-specs-analysis}

Esta pregunta de investigación evalúa si \textsf{BEAPI} puede ser útil para asistir a los usuarios 
en la detección de fallas en los métodos \texttt{repOK} (descripciones formales de los invariantes 
de representación de las clases), a través de la comparación entre el conjunto 
de objetos que se pueden generar mediante la API y aquellos que se pueden generar a partir del 
invariante de representación.

Para ello, diseñamos el siguiente procedimiento automatizado. En primer lugar, ejecutamos 
\textsf{BEAPI} para generar un conjunto de estructuras, denominado \texttt{SBEAPI}, utilizando únicamente 
la API de la clase, y ejecutamos \textsf{Korat} para generar un segundo conjunto, \texttt{SKORAT}, 
utilizando únicamente el método \texttt{repOK}. Ambas ejecuciones se realizan
para un mismo \emph{scope}. En segundo lugar, canonizamos todas las estructuras
en ambos conjuntos utilizando el proceso de linearización (Sección
\ref{sec:stateMatching}), de modo que las estructuras isomorfas puedan 
compararse de forma directa. Finalmente, verificamos si los conjuntos
\texttt{SBEAPI} y \texttt{SKORAT} son iguales o no.  

El procedimiento anterior puede producir tres resultados. Cuando \texttt{SBEAPI} $\subset$ \texttt{SKORAT}, 
puede deberse a que la API genera un subconjunto de las estructuras válidas, a que \texttt{repOK} 
sufre de subespecificación (es decir, permite estructuras que no deberían ser válidas), o ambos. 
En estos casos, las estructuras que están en \texttt{SKORAT} pero no en
\texttt{SBEAPI} son evidencia de potenciales errores, y el usuario debe
analizarlas manualmente para determinar el origen del problema. Como en este
experimento \textsf{BEAPI} se ejecutó con métodos generadores de objetos que son
correctos, el error debe estar presente en el \texttt{repOK}.
Clasificamos estos errores como \texttt{sub} 
ya que el invariante permite más estructuras de las que debería.

Por el contrario, cuando \texttt{SKORAT} $\subset$ \texttt{SBEAPI}, puede
indicar que el \texttt{repOK} es demasiado restrictivo (sobreespecificado), que 
la API genera estructuras inválidas, o ambas cosas a la vez. Las estructuras en
\texttt{SBEAPI} que no pertenecen a \texttt{SKORAT} son testigos del error, y
nuevamente deben ser analizadas manualmente por el usuario. Nuevamente, como la
los métodos utilizados por \textsf{BEAPI} son correctos, el error en este caso
debe estar en el \texttt{repOK}. Reportamos los
errores de sobreespecificación en los \texttt{repOK}s (confirmados manualmente)
que fueron evidenciados por estas estructuras como \texttt{sobre}.

En algunos casos, se encontraron diferencias en ambas direcciones: estructuras
en \texttt{SKORAT} que no están presentes en \texttt{SBEAPI}, y estructuras \texttt{SBEAPI} que no 
pertenecen a  \texttt{SKORAT}. Estas diferencias pueden deberse a fallas en la
API, fallas en el \texttt{repOK}, o ambas. Reportamos las fallas en los
\texttt{repOK}s confirmadas manualmente que fueron evidenciadas por dichas
estructuras simplemente como \emph{error} (el \texttt{repOK} describe un
conjunto de estructuras distinto del que debería, ya que la API es correcta).

Cabe destacar que las diferencias entre las definiciones de scope en los
enfoques pueden provocar discrepancias entre los conjuntos \texttt{SBEAPI} y
\texttt{SKORAT}. Esto sólo ocurrió en las estructuras \texttt{RBT} y
\texttt{FibHeap}, donde \textsf{BEAPI} generó un conjunto más chico de 
estructuras que \textsf{Korat} para el mismo scope, debido a restricciones de balance 
(como se explicó en la Sección \ref{sec:beapi-eficiencia}). Sin embargo, estos ``falsos positivos''
debido a diferencias de scope pueden revelarse fácilmente, dado que todas las estructuras generadas 
por \textsf{Korat} siempre estaban incluidas en las generadas por \textsf{BEAPI} 
cuando se utilizaba un scope mayor para este último. Usando este criterio, descartamos manualmente 
los ``falsos positivos'' en \texttt{RBT} y \texttt{FibHeap}.

Los resultados del experimento se resumen en la Tabla \ref{table:bugs}. Se
encontraron errores en 9 de los 26 \texttt{repOK} analizados usando el enfoque
descrito anteriormente.  En particular, se detectaron errores de
subespecificación en clases como \texttt{RBTree}, \texttt{AVL},
\texttt{BinTree}, \texttt{FibHeap} y \texttt{NCL}, en los que el \texttt{repOK}
admiten estructuras inválidas.  También se identificaron casos de
sobreespecificación, como en \texttt{FibHeap}, donde el método \texttt{repOK}
rechaza heaps válidos cuyo nodo mínimo es \texttt{null}. Finalmente, se
encontraron errores en las definiciones de altura en clases como \texttt{AVL},
donde las hojas están inicializadas con valores inconsistentes con la
implementación.

Este experimento muestra la dificultad de escribir invariantes de representación correctos y eficientes 
para la generación exhaustiva acotada.  
%Incluso, por autores de benchmarks de testing tomados de la literatura. 
Los errores descubiertos evidencian que los problemas en los \texttt{repOK}s son difíciles de 
detectar manualmente, y que \textsf{BEAPI} puede ser de gran ayuda para esta tarea.


\input{tables/bugRepOk}

\subsection{RQ7: BEAPI para el testing basado en propiedades}
\label{sec:parametrizedTesting}


%En el experimento de MGO (ver \ref{sec:object_builders_randoop}), generamos objetos utilizando los MGO y toda la API junto con la herramienta \texttt{Randoop} y los empleamos posteriormente en un conjunto de tests parametrizados, con el objetivo de evaluar su impacto en la cobertura.
%

En esta sección, evaluamos la calidad de los objetos generados por \textsf{BEAPI} en el
contexto del testing basado en propiedades.
Para ello, comparamos \textsf{BEAPI} con los enfoques de generación aleatoria utilizados
en la Sección \ref{sec:experiments-builders-parameterized}. Es decir,
consideramos la versión estándar de \textsf{Randoop},y las versiones modificadas de 
\textsf{Randoop} para serializar objetos: \textsf{RSer-API} que utiliza toda la
API, y \textsf{RSer-MGO} que utiliza sólo los métodos generadores de objetos.
Los objetos generados por estas técnicas se usan como entradas para
instanciar los parámetros de los tests basados en propiedades 
definidos en la Sección \ref{sec:experiments-builders-parameterized} (la 
Figura \ref{lst:parametrizadoLinkedList} provee un ejemplo).
Para esta evaluación preliminar se incluyeron las clases 
 del paquete\texttt{java.util} debido a la complejidad de las estructuras y a su 
 API rica en métodos: \texttt{HashSet}, \texttt{HashMap}, \texttt{TreeMap}, \texttt{TreeSet} y \texttt{LinkedList}.
Para más detalles, ver la descripción del experimento en la Sección
\ref{sec:experiments-builders-parameterized}; el experimento de esta sección es
similar, salvo que agregamos BEAPI como generador de objetos adicional.

Las métricas consideradas fueron: el tiempo 
de generación de tests (\textsf{GTime}), la cantidad de objetos generados y la cantidad 
de tests producidos (\textsf{Obj/Test}), el 
tiempo de ejecución de la test suite (\textsf{Time}), la cobertura de ramas alcanzada 
(\textsf{Ramas}), y la cantidad de mutantes que mata la suite (\textsf{Mutación}). 
Los enfoques \texttt{RSer-API}, \texttt{RSer-MGO} y \textsf{BEAPI} son usados para alimentar
la test suite basada en propiedades definida manualmente. \textsf{Randoop} 
genera su propia test suite, por lo que los resultados no son directamente
comparables con las otras técnicas. 
Como la generación con \textsf{BEAPI} se lleva a cabo por scope (y no por tiempo como en
los otros enfoques), incluimos en los resultados el scope para el que \textsf{BEAPI}
termina dentro de cada presupuesto de generación predefinido. Así,
\textsf{BEAPI-sxx} denota la técnica \textsf{BEAPI} ejecutada para un scope de
\textsf{xx}.
\input{tables/randoopvsBeapi}

A continuación se describen los resultados de este experimento.
Como se discutió en la Sección \ref{sec:experiments-builders-parameterized}, \textsf{RSer-MGO} presenta un rendimiento
significativamente mejor que \textsf{Randoop} y \textsf{RSer-API}. Por lo tanto,
en lo que sigue sólo compararemos \textsf{RSer-MGO} con \textsf{BEAPI}.

La Tabla \ref{tab:hashSetTools_beapi} muestra los resultados para \texttt{HashSet}. 
En este caso, \textsf{RSer-MGO} obtiene la
cobertura de ramas y el puntaje de mutación más grandes a partir de 120
segundos. \textsf{BEAPI} requiere un poco más de tiempo para lograr los mismos
resultados (167 segundos).

La Tabla \ref{tab:linkedListTools_beapi} presenta los resultados obtenidos para la clase 
\texttt{LinkedList}, que es la estructura más simple de este experimento. 
Se observa que \textsf{BEAPI} y \textsf{RSer-MGO} llegan a
los mayores valores de cobertura y mutación con el presupuesto más chico (30 segundos).

La Tabla \ref{tab:treeSetTools_beapi} presenta los resultados obtenidos para \texttt{TreeSet}. En 
este caso, las diferencias comienzan a notarse con mayor claridad.
\textsf{BEAPI} alcanza los mayores valores de cobertura (154) y mutación (116)
con el presupuesto más chico (30 segundos). \textsf{RSer-MGO} logra el máximo de
cobertura (154) para un presupuesto de 60 segundos, pero en 180 segundos sólo
logra matar 111 mutantes (menos que los 116 de \textsf{BEAPI}).

Con respecto a la Tabla \ref{tab:hashMapTools_beapi}, 
correspondiente a \texttt{HashMap}, se aprecia un caso 
similar. \textsf{BEAPI} logra los mayores valores de cobertura (125) y mutación
(131) con el presupuesto más chico (30 segundos). En contraste, con el máximo
presupuesto, \textsf{RSer-MGO} llega a cubrir una menor cantidad de ramas
(122) y a matar menos mutantes (127).

Por último, para la clase \texttt{TreeMap}, cuyos resultados se muestran en la 
Tabla \ref{tab:treeMapTools_beapi}, puede observarse que \textsf{BEAPI} alcanza 
los mayores valores de cobertura (194) y mutación (165) antes de los 60
segundos. \textsf{RSer-MGO}, en 180 segundos de generación, obtiene una menor cobertura
(162) y mata menos mutantes (156).

Los resultados obtenidos permiten concluir que los objetos generados por \textsf{BEAPI} son 
efectivos para el testing basado en propiedades. Si bien la generación para scopes grandes
pueden ser prohibitiva (debido a la exhaustividad de la técnica), usar scopes
relativamente pequeños es suficiente para que \textsf{BEAPI} logre los puntajes 
de cobertura y mutación más altos de todas las técnicas consideradas en este experimento. 



