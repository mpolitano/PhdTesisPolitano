%!TEX root = main.tex
\chapter[Evaluaci\'on]{Evaluaci\'on}
\label{cap:experimental}

En los capítulos \ref{cap:builders} y \ref{cap:beapi} presentamos enfoques para
identificar automáticamente un conjunto de métodos generadores de objetos, y una
técnica novedosa de generación exhaustiva acotada basada en la API. 
En este capítulo, realizamos una evaluación experimental de las técnicas
mencionadas. La sección \ref{sec:experimentalIdentificacion} analiza experimentalmente los
algoritmos de identificación de generadores de objetos, mientras que en la
sección \ref{sec:experimentalBeapi} se evalúa la generación exhaustiva acotada basada en la API.


\section{Algoritmos de identificación de métodos generadores de objetos}
\label{sec:experimentalIdentificacion}

En esta sección, evaluamos experimentalmente los enfoques presentados en el
Capítulo \ref{cap:builders}. Analizaremos la eficiencia y precisión de cada
algoritmo utilizando las funciones objetivo introducidas en la
Sección \ref{sec:fitness}. Además, mostraremos resultados preliminares sobre cómo los 
métodos generadores de objetos pueden ser aprovechados por algunas herramientas
automáticas de análisis de software.

Con respecto a estas técnicas, las siguientes preguntas de investigación guían esta experimentación:

\begin{itemize}
\item \emph{RQ1}: ¿Qué tan eficientes son los algoritmos propuestos para
    identificar conjuntos de métodos generadores de objetos?
\item \emph{RQ2}: ¿Qué tan precisos son los algoritmos presentados para identificar métodos generadores de objetos?
\item \emph{RQ3}: ¿Cuál es el impacto de utilizar métodos generadores de objetos
    en el contexto del análisis automático de software?
\end{itemize}

\subsection{Configuraci\'on experimental}

\subsubsection{Casos de estudio.}


La evaluación se llevó a cabo sobre un conjunto de clases Java 
 que manipulan estructuras dinámicas complejas,
incluyendo: \verb"NCL" de Apache Commons Collections \cite{apache};
\verb"BinaryTree", \verb"BinomialHeap" y \verb"FibonacciHeap", extraídos
de \cite{Visser:2006}; y \verb"UnionFind", una implementación de conjuntos
disjuntos tomada de JGrapht \cite{jgrapht}.
También se incluyeron componentes de proyectos reales de software, como 
\verb"Lits" de la implementación de Sat4j \cite{sat4j}, utilizada previamente 
en \cite{Loncaric:2018}. \verb"Lits" maneja la representación interna de una
fórmula en el SAT solver (variables y literales).
%, y traduce literales desde DIMACS a la representación interna.
Por otro lado, se consideraron estructuras adicionales como \verb"Scheduler",
un planificador de procesos tomado del conjunto de benchmarks SIR \cite{sir},
y varias colecciones del paquete estándar \verb"java.util" de Java%
\footnote{\url{https://docs.oracle.com/javase/8/docs/api/java/util/
package-summary.html}}, incluyendo \verb"TreeMap", \verb"TreeSet",
\verb"HashMap", \verb"HashSet" y \verb"LinkedList". 

\input{tables/groundTruth}

Para evaluar la precisión de nuestros algoritmos, se analizaron los resultados 
manualmente. 
%conjunto de referencia o \emph{ground truth} que contiene los métodos
%generadores de objetos (\emph{MGO}) considerados minimales y suficientes en cada caso de estudio. 
Esta tarea implicó un análisis detallado de cada API, lo que representó un
trabajo considerable, especialmente en los casos más complejos. La
Tabla \ref{tab:groundTruth} provee ejemplos de métodos generadores de objetos
(\emph{MGO}) minimales y suficientes para cada caso de estudio (recordar que puede
haber más de un conjunto de \emph{MGO}s suficientes de tamaño minimal). 
%este conjunto de referencia, 
En la tabla también indicamos para cada clase el número total de métodos en su
API pública (\#API). 
%y los métodos identificados manualmente como generadores de objetos. 
%Cabe destacar que en algunos
%casos ciertos métodos pueden ser intercambiables (por ejemplo,
%\texttt{addFirst} y \texttt{addLast} en \texttt{NCL}).


\subsubsection{Algoritmos evaluados.}

\begin{itemize}
  \item \textbf{GA}: el algoritmo genético descrito en la Sección \ref{alg:approachGA}.
  \item \textbf{HC}: el algoritmo \emph{hill-climbing} presentado en la Sección \ref{alg:approachHC}.
\end{itemize}

Propusimos dos funciones objetivo para evaluar cada conjunto candidato de
métodos: 

\begin{itemize}
    \item \textbf{FGE}: función objetivo basada en la cantidad de objetos
        producidos por un generador exhaustivo acotado, presentada en la Sección \ref{sec:fitnessGE}.
\item \textbf{FRC}: función objetivo basada en la cobertura de código, descrita en la Sección \ref{sec:fitnessRandoop}. 
\end{itemize}

Cuando un algoritmo se evalúa con una función objetivo específica, usamos la notación 
\textbf{Algoritmo–Fitness}. 
Esto da lugar a cuatro algoritmos a evaluar: \textbf{GA–FGE}, \textbf{GA–FRC}, \textbf{HC–FGE}, 
\textbf{HC–FRC}. Notar que el trabajo sobre cómputo de métodos generadores
de Politano et. al. \cite{Ponzio19} presenta el algoritmo
\textbf{GA–FGE}. Las otras variantes fueron agregadas y evaluadas de forma
preliminar para esta tesis. 

Nuestros algoritmos requieren la configuración de diversos parámetros que influyen en su eficiencia y efectividad.
Estos valores se determinaron empíricamente, priorizando la eficiencia y la
precisión de las soluciones en experimentos realizados previamente.
En particular, para el algoritmo genético (\textbf{GA})
utilizamos una tasa de cruce de 0.40, una tasa de mutación de 0.05, 
selección por torneo con cuatro participantes, una población de 100 individuos y
un máximo de 20 generaciones por ejecución. 

Para las evaluaciones con la función objetivo basada en cobertura (\textbf{FRC}), 
por cada candidato lanzamos dos hilos ejecutando Randoop con distintas semillas. 
Además, fijamos un tiempo máximo de ejecución de Randoop de 30 segundos (en
cada hilo).
\pp{Decir algo de esto en la sección de la técnica FRC: Randoop introduce un componente 
    de aleatoriedad en la generación de tests. Para controlar este
    factor y garantizar la reproducibilidad de los resultados de los
    experimentos con los mismo conjunto de parámetros, fijamos dos semillas
distintas.}
% y controlamos la aleatoriedad mediante semillas. Randoop utiliza esta semilla para controlar la generación de tests y
% permitir la reproducción de resultados. Dado que Randoop introduce un grado
% considerable de aleatoriedad, optamos por utilizar dos semillas distintas para capturar la variabilidad introducida por el 
% generador aleatorio, registrándolas para permitir la reproducción de resultados. 
% Esta configuración proporciona una medida más robusta del desempeño de los algoritmos en contextos con 
% elementos no determin\'isticos.}

En el caso de la función objetivo basada en generación exhaustiva (\textbf{FGE}),
el parámetro central es el \emph{scope}, entendido como la cantidad máxima de
objetos por clase que puede crear el generador exhaustivo, y rangos de
valores para los tipos primitivos. 
En nuestros experimentos fijamos el scope en 5. Es decir, permitimos un máximo
de cinco objetos por clase, y 5 valores posibles para tipos primitivos (por ejemplo, 
para enteros usamos valores entre 0 y 4). Además, establecimos un tiempo máximo de ejecución 
de 30 segundos para la función objetivo.
%Además, para evitar diferencias espurias que no reflejan cambios semánticos en la representación, 
%omitimos campos internos que no afectan la estructura lógica de los objetos generados (por ejemplo, \texttt{modCount}, común en colecciones de Java), 
%el cual sólo contabiliza modificaciones internas sin alterar la estructura observada del objeto. \pp{Yo no pondría esto último porque no recuerdo que lo
%hayamos explicado en ningún lado.} \cacho{Hay un apartado pequeño, lo podemos charlar}

Para tratar con la aleatoriedad en los enfoques, cada algoritmo fue ejecutado
cinco veces con los parámetros mencionados anteriormente. Los resultados reportados en las siguientes secciones corresponden al promedio de las cinco ejecuciones. 

Cuando se desea evaluar la utilidad práctica de los métodos generadores de objetos en
RQ3, se usan siempre los métodos identificados automáticamente por el algoritmo
\textbf{HC–FGE}, que fue el más preciso en nuestros experimentos (ver Sección \ref{sec:experimentalIdentificacionPrecision}).
 
Todos los experimentos fueron ejecutados en una máquina con procesador Intel Core i7-6700
(4 núcleos, 8 hilos, frecuencia de hasta 4 GHz, 8 MB de caché, sexta generación),
8,GB de RAM y sistema operativo GNU/Linux.

% En términos de analizar el peor caso, con \textbf{HC+RC} se alcanza 5162s en
% \emph{TreeMap} (aprox 86 minutos) y 3842s en \emph{HashSet} para
% para \textbf{HC+GA} (aprox. 64 minutos). 
% Con \textbf{GE+GA}, se observa que el peor caso tarda 1803s (aprox 30 minutos) en \emph{HashMap} con \textbf{GE+RC} alcanza 113s para \emph{TreeMap} (aprox 2 minutos).

%Esto se debe a que los algoritmos
%de Hill Climbing comienzan desde abajo hacia arriba, considerando menos métodos antes de considerar más métodos, a diferencia de los 
%algoritmos genéticos, que generan sucesores mediante operadores de cruce y mutación. Esta estrategia favorece al algoritmo Hill Climbing 
%para encontrar los métodos mínimos en tiempos más cortos. Cabe mencionar que en casos de estudio donde el algoritmo genético supera al 
%Hill Climbing, se debe a que la clase bajo prueba tiene pocos métodos, lo que permite que los algoritmos evolutivos converjan más rápidamente que el algoritmo Hill Climbing.
%

\subsection{RQ1: Eficiencia del cómputo de métodos generadores de objetos}
\label{sec:eficienciaBuilders}

\input{tables/eficienciaBuilders}

Medimos la eficiencia como el tiempo promedio (en segundos) necesario para
finalizar la búsqueda de métodos generadores a partir de la API. 
%Evaluamos los dos
%algoritmos de búsqueda (\emph{GA} y \emph{HC}) combinados con dos funciones
%objetivo: \emph{FGE} (Función objetivo con generación exhaustiva acotada) y \emph{FRC} (Función objetivo con cobertura). 
Los resultados se muestran en la Tabla \ref{tab:eficiencia}. Cada fila
corresponde a un caso de estudio; debajo del nombre se indica el tamaño de la API
(\#API) y el número de métodos generadores del conjunto minimal (\#MGO). 
Las columnas se agrupan por algoritmo (GA y HC) y, dentro de
cada uno, por función objetivo (FGE y FRC). Para cada configuración reportamos el promedio de 5 ejecuciones.

A continuación, reportamos los tiempos promedio de ejecución (sobre los 12 casos
de estudio) de los algoritmos evaluados:
\begin{itemize}
  \item \textbf{GA+FGE}: 258.9 segundos
  \item \textbf{GA+FRC}: 1895.5 segundos
  \item \textbf{HC+FGE}: 19.7 segundos
  \item \textbf{HC+FRC}: 434.2 segundos
\end{itemize}

Estos valores muestran con claridad que \texttt{HC} es sistemáticamente más rápido
que \texttt{GA}. En particular, con \texttt{FGE} el tiempo de \texttt{HC} es 
aproximadamente 13 veces menor que el de \texttt{GA}, mientras que usando 
\texttt{FRC} es casi 3 veces menor. 
A nivel de casos
individuales, \texttt{HC} supera a \texttt{GA} en todos los casos tanto con
\texttt{FGE} como con \emph{FRC}. 
Esta diferencia se entiende por la propia naturaleza de los algoritmos.
El algoritmo genético (\texttt{GA}) mantiene y evoluciona poblaciones de
candidatos, lo que implica evaluar la función objetivo (que es costosa
computacionalmente) para muchos candidatos.
En cambio, \texttt{HC} sigue una estrategia \emph{greedy}: elige la mejor opción posible entre los sucesores del candidato actual. Esto lo vuelve más eficiente.
Sin embargo, en ciertos casos \texttt{HC} podría perder precisión en el cómputo de métodos
generadores. 

Comparando las funciones objetivo, \texttt{FGE}
es más rápida que \texttt{FRC}. En \texttt{GA}, FRC requiere en promedio
unas siete veces más tiempo que \texttt{FGE}; en \texttt{HC}, \texttt{FRC} requiere unas casi 32 veces
más tiempo que \texttt{FGE}. La función objetivo \texttt{FGE} es más rápida 
debido a que para computar MGO en nuestos casos de estudio es suficiente con generar objetos 
de tamaño relativamente pequeño (scope 5). Además, \texttt{FGE} utiliza BEAPI (ver
Capítulo \ref{cap:beapi}), que implementa la generación exhaustiva acotada
basada en la API de manera eficiente. 
En cambio, la función objetivo FRC debe generar una gran cantidad de tests en
muchos casos para lograr una buena cobertura (notar que FRC es aleatoria). 
Además, FRC requiere correr múltiples ejecuciones de Randoop con diferentes semillas de 
aleatoriedad (dos en estos experimentos), lo que la hace más costosa.
Todo esto explica porque FGE es más rápida en comparación con la función objetivo FRC
en estos experimentos.

Por otra parte, la función objetivo \texttt{FRC} tiene la ventaja de que no requiere 
que el usuario provea la definición del scope para la generación exhaustiva acotada. 
Además, \texttt{FRC} podría ser más escalable que \texttt{FGE} en casos donde
la cantidad de objetos sea demasiado grande para ser generados de manera
exhuastiva (por ejemplo, en clases con invariantes de representación más débiles
que los evaluados aquí).
De cualquier modo, los resultados preliminares muestran que los algoritmos 
son mucho más lentos con la función objetivo \texttt{FRC}. Se deja como trabajo 
futuro estudiar formas más eficientes de implementar la función \texttt{FRC}, y
realizar más experimentos para evaluar mejor su utilidad en la práctica 
\pp{No olvidarse de incluir esto en trabajos futuros}.

En conclusión, \texttt{HC+FGE} computa generadores de objetos muy rápidamente en
nuestros experimentos, en 20 segundos en promedio. \texttt{GA+FGE} y
\texttt{HC+FRC} son razonablemente rápidos para esta tarea, 
tomando en promedio poco más de 4 minutos, y 10 minutos, respectivamente.
La combinación de \texttt{GA+FRC} hace que el cómputo se vuelva mucho más lento.
Como el cómputo de generadores de objetos se realiza una sola vez sobre el
código de la clase bajo análisis, previo a los análisis de código que se quieran
llevar adelante (por ejemplo, generación de
objetos para testing, etc.), creemos que \texttt{HC+FGE}, \texttt{GA+FGE}, y
\texttt{HC+FRC} son algoritmos viables para su uso en la práctica. 

\subsection{RQ2: Precisión de los algoritmos de cómputo de métodos generadores de objetos}
\label{sec:experimentalIdentificacionPrecision}

\input{tables/eficaciaBuilders.tex}

Medimos la precisión como la capacidad de cada algoritmo para identificar
un conjunto suficiente y minimal de métodos generadores a partir de la API. 
La Tabla \ref{tab:efectividad} resume los tamaños promedios (sobre 5
ejecuciones) del conjunto de métodos identificado por cada algoritmo evaluado. 
Los resultados se analizaron manualmente 
(el número de MGOs minimales se indica como \#MGO en la tabla) y 
en todos los casos un conjunto de métodos generadores de objetos suficiente 
fue generado por todos los algoritmos. Sin embargo, como puede verse en la
tabla, algunos algoritmos incluyen algunos métodos irrelevantes en algunos casos.

Se observan tres patrones claros. Primero, \texttt{HC+FRC} y \texttt{HC+FGE}
encuentran un conjunto de métodos generadores suficiente y minimal para las doce clases analizadas.
Segundo, \texttt{GA+FGE} identifica conjuntos minimales de generadores de objetos
en todos los casos menos en \texttt{HashMap}, donde en algunas ejecuciones incluye
un método irrelevante.
Por último, \texttt{GA+FRC} obtiene resultados muy cercanos a un conjunto minimal 
en la mayoría de los casos, 
incluyendo algún método irrelevante en alguna de las ejecuciones para algunas clases (\texttt{NCL}, \texttt{TreeMap}, \texttt{TreeSet}, \texttt{HashSet}),
pero tiene un costo computacional considerablemente mayor, por lo que no resulta 
competitivo en términos de eficiencia.

La pérdida de precisión del algoritmo genético (\texttt{GA}) puede deberse a la
estocasticidad propia de la búsqueda (selección, cruce, mutación). Otro factor
que podría influir es el criterio de corte del algoritmo que detiene la evolución 
cuando el mejor fitness no mejora durante cinco generaciones consecutivas (ver
Sección \ref{alg:approachGA}). Este criterio es eficaz para acelerar la
terminación del algoritmo, pero puede resultar en soluciones subóptimas (como
se ve en algunos casos). Se deja como trabajo futuro la optimización de los
parámetros del algoritmo genético en busca de mejorar su precisión \pp{Poner en
trabajos futuros}. 

En síntesis, los algoritmos presentados producen conjuntos suficientes
de métodos generadores de objetos en los casos considerados. Además, 
\textbf{HC} genera conjuntos minimales en todos estos casos, con ambas funciones
objetivo. Además, \textbf{HC+FGE} es el algoritmo más eficiente, lo que lo
convierte en una buena opción para computar métodos generadores de objetos
rápidamente.


\subsection{RQ3: Impacto de los métodos generadores de objetos en el análisis de
software}

\subsubsection{Generación de objetos para testing}
\label{sec:object_builders_randoop}

\input{tables/objectsBuilders.tex}

En esta sección evaluamos la utilidad práctica de los métodos generadores 
de objetos en el contexto de la generación automática de objetos usando Randoop. 
Para esto utilizamos los generadores de objetos identificados con el 
algoritmo de \emph{Hill Climbing} (Sección \ref{alg:approachHC}) y la función objetivo 
basada en la cantidad de objetos producidos por un generador exhaustivo acotado 
(Sección \ref{sec:fitnessGE}). En la sección siguiente veremos que estos objetos 
pueden ser utilizadas luego como entradas en tests basados en propiedades.
 
Para este experimento, modificamos la herramienta \emph{Randoop} (ver
Sección \ref{sec:feedback-directed-test-gen}) para que almacene los
objetos producidos por la ejecución de los tests. La evaluación consiste en
utilizar la variante mencionada de Randoop con toda la API de la clase bajo
análisis. Denominamos \texttt{RSer-API} a esta técnica de generación de objetos 
(una versión de Randoop que usa la API completa y serializa objetos). Por otro
lado, se utiliza otra versión que instruye a Randoop para que utilice sólo los MGOs para
generar tests, denominada \texttt{RSer-MGO} (una versión de Randoop que usa solo MGOs 
y serializa objetos). 

Para estos enfoques, definimos tres valores de presupuesto de tiempo para la generación de 
objetos: 60, 120 y 180 segundos. En este experimento, medimos la cantidad de objetos distintos
generados por los tests (\emph{Num. Objetos}) (usamos los métodos \texttt{equals()} de las 
clases evaluadas para descartar objetos iguales).
% \cacho{Además, reportamos el tamaño del objeto más grande generado por las técnicas
% \pp{Por qué faltan los tamaños de los objetos acá? En el paper los habíamos
% puesto, o no?} (calculamos los tamaños de objetos con el método
% \texttt{size()} de las clases bajo análisis). }
% \cacho{Agregar table, URGENTE!. escribir esa parte}

Los resultados obtenidos se resumen en la Tabla \ref{tab:results-obj}.
En promedio, para un mismo presupuesto de generación \texttt{RSer-MGO} 
produce 1.45 veces más objetos que \texttt{RSer-API}. 
Desagregando por presupuesto, para 60 segundos con métodos generadores 
se obtienen, aproximadamente, dos veces más objetos, que con toda la API. 
Con un presupuesto de 120 segundos, se generan aproximadamente 1.5 
veces más objetos con \texttt{RSer-MGO} en comparación con \texttt{RSer-API},
Finalmente, para 180 segundos, se construyen casi 1.37 veces más objetos 
con \texttt{RSer-MGO} en comparación con \texttt{RSer-API}. 
%Esta diferencias es debido a que, al aumentar el presupuesto, Randoop tiene más tiempo para explorar la API completa y generar más objetos,
%reduciendo la ventaja relativa de usar sólo los métodos generadores de objetos.
%

% \cacho{Todo esto es nuevo}
% Utilizamos los objectos que produce Randoop con los dos enfoques para medir la diversidad de los objectos generados.

% \cacho{Para observar la calidad de los objectos, se mide en la seccion \ref{sec:parametrizedTesting}, donde consideramos no solo la tecnica de Randoop y Randoop con MGO sino que, tambien, consideramos BEAPI.}

%La ventaja es consistente todos los casos. Si
%promediamos por clase (sobre los tres presupuestos), \texttt{MGO} es mejor
%en todas las clases, con ganancias especialmentes marcadas en \emph{NCL} (casi 15 veces mas en promedio por clase) y \emph{LinkedList}
%(casi 10 veces). 

En resumen, el uso de métodos generadores de objetos mejora significativamente la
capacidad de generación de objetos distintos de Randoop.
%, para un mismo presupuesto de tiempo.
Es esperable que al generar objetos más diversos, los objetos producidos por
\texttt{RSer-MGO} ejerciten mejor el código bajo test que los generados por
\texttt{RSer-API} si se los utiliza para testing. 
La calidad de los objetos generados se evalúa en más detalle en la sección siguiente,
donde los utilizamos como entradas de tests basados en propiedades. 

\subsubsection{Testing basado en propiedades}
\label{sec:experiments-builders-parameterized}

En esta etapa de la evaluación, se llevó a cabo un análisis exhaustivo para determinar la 
utilidad de los métodos generadores identificados mediante nuestro enfoque, particularmente 
en el contexto de la generación automatizada de casos de prueba. Estos métodos permiten construir 
objetos que pueden utilizarse como entradas en test suites parametrizadas.

Los tests parametrizados constituyen una técnica eficiente para mejorar la cobertura en tests 
automatizados. En lugar de definir casos de prueba específicos para cada configuración, se 
establece una única prueba que se ejecuta múltiples veces con distintos parámetros. En nuestro 
contexto, dichos parámetros son objetos generados por distintas técnicas, los cuales sirven como 
entrada a los métodos de la clase bajo prueba.

Para llevar a cabo este experimento, diseñamos una test suite parametrizada en la que cada método 
público de la clase bajo análisis es ejercitado con objetos generados por distintas técnicas. Las 
herramientas utilizadas para generar estos objetos fueron: la versión estándar de \emph{Randoop}, que 
genera directamente una suite de tests sin reutilizar objetos previos; una variante de \emph{Randoop} 
denominada \emph{R-Serialize-API}, que serializa los objetos construidos durante la generación de 
tests, permitiendo su reutilización como entrada en tests parametrizados; una versión modificada 
de \emph{Randoop} que prioriza el uso de métodos generadores de objetos identificados previamente mediante 
nuestro enfoque (denominada \emph{R-Serialize-MGO}, ver sección \ref{sec:object_builders_randoop}); 

% y por último, la herramienta \emph{BEAPI}, que 
% produce directamente un conjunto reducido pero válido de objetos a partir de su propia lógica de 
% exploración exhaustiva acotada.
\pp{Cacho: Agregar tablas de Randoop, R-Serialize-API, R-Serialize-MGO.}

\pp{Pablo: Describir estas técnicas en configuración experimental.}


A fin de evaluar y comparar el impacto de cada enfoque, se utilizó el benchmark basado en clases 
del paquete \emph{java.util} debido a su rica API que tienen: \emph{HashSet}, \emph{HashMap}, \emph{TreeMap}, \emph{TreeSet} y \emph{LinkedList}.
Ademas, para poder utilizar todos los \emph{benchamark} se necesita escribir test parametrizados para todos, siendo esta, una tarea laboriosa.
Las métricas consideradas fueron: el tiempo 
de generación (\emph{GTime}), este tiempo es el que se le da cada herramienta para ejecutar.
Ademas se tuvo en cuenta las metricas de la cantidad de casos de prueba ejecutados (\emph{Test}), el 
tiempo total de ejecución de la test suite (\emph{T(Seg)}), la cobertura de ramas alcanzada 
(\emph{Ramas}) y la cantidad de mutantes eliminados (\emph{Mutacion}). Es importante destacar 
que la test suite parametrizada es común a los enfoques \emph{R-Serialize}, \emph{R-Builders}, permitiendo así una comparación justa sobre los objetos generados. En el caso de 
\emph{Randoop}, la herramienta genera su propia test suite, por lo que los resultados deben 
interpretarse con cautela.


\input{tables/randoopvsBuilders}

Las tablas anteriores, demuestran que los resultados obtenidos con \texttt{R-Serialize-MGO} son consistentemente superiores a los alcanzados por \texttt{R-Serialize-API} y \texttt{Randoop} en todos los casos evaluados. 
\texttt{R-Serialize-MGO} mejora la cobertura de ramas y mutación con sus objetos generados a partir de los métodos generadores identificados previamente.
En particular, se obtiene una mejora en las clases \texttt{TreeSet}, \texttt{TreeMap} y \texttt{HashMap} y logrando empatar con \texttt{R-Serialize-API} en \texttt{LinkedList} y \texttt{HashSet}.
Estos resultados evidencian que los métodos generadores de objetos identificados son efectivos para producir entradas de alta calidad para testing, mejorando la capacidad de detección de fallas.
Ademas, podemos afirmar que los MGO son utiles para mejorar las herramientas de testing automatico como Randoop.


\cacho{Explico tabla por tabla?}

\subsubsection{Verificación acotada usando JPF}
\label{sec:experiments-jpf-driver}

En un segundo experimento, evaluamos el impacto del uso de métodos generadores
de objetos en la eficiencia de la verificación acotada usando \emph{Java PathFinder}
(JPF) \cite{Visser:2005} (explicado en la sección \ref{sec:jpf}).
Para este experimento preliminar, usamos como casos de estudio algunas clases
del paquete \texttt{java.util}. 
Los experimentos consisten en verificar de manera exhaustiva acotada que, dada
una estructura de datos, luego ejecución de un método de la estructura de datos
se satisface el invariante de representación (\texttt{repOK}) de la estructura. 
Por ejemplo, para el método \texttt{put} de \texttt{TreeMap} escribimos el
programa de la Figura \ref{lst:propiedadJPF} para realizar la verificación con
JPF. \pp{Fijate que los captions de las figuras están en inglés y dicen Figure.
Hay que configurar algo para traducirlos a español.} \cacho{Hecho}

\begin{lstlisting}[caption={Verificando el método put de TreeMap con JPF},label={lst:propiedadJPF},language=Java,captionpos=b]
    public static void main(String[] args) {
       int scope = 3;
       TreeMap t = generateStructure(scope);
       t.put(Verify.getInt(0,scope),Verify.getInt(0,scope));
       assert t.repOK();
    }
\end{lstlisting}

El programa primero construye todos los árboles de tamaño acotado usando un
driver en la línea 3 (método \texttt{generateStructure}). Luego, ejecuta el
método bajo análisis en la línea 4 (\texttt{put}), y por último verifica que se
satisfaga el \texttt{repOK} luego de la ejecución del método en la línea 5.

Para la evaluación consideramos los dos enfoques presentados en la Sección \ref{sec:jpf-builders} para construir los drivers. El enfoque \texttt{API} consiste en
utilizar toda la API. Recordar que un ejemplo de driver generado con este enfoque se 
presenta en la Figura \ref{lst:driverAPI}. Por otro lado, el enfoque
\texttt{MGO} consiste en usar sólo los métodos generadores de objetos para
construir los drivers, como se muestra en el ejemplo de la Figura \ref{lst:driverBLD}.
\cacho{Nuevo}Los métodos generadores de objetos fueron precomputados usando el algoritmo de \emph{Hill Climbing} (\ref{alg:approachHC}) 
con la función objetivo que utiliza los objectos que produce el generador exhaustivo acotado (\ref{sec:fitnessGE})

\input{tables/tableJPFBuilders.tex}
\input{tables/tableJPFBuilders1.tex}

Los resultados de los experimentos, presentados en las
Tablas \ref{tab:results-jpf} y \ref{tab:results-jpf1}, muestran que los
controladores construidos a partir de los métodos generadores objetos (MGO) permiten
una mejora significativa en la eficiencia y la escalabilidad de JPF. 
Se observa una reducción considerable en el tiempo de análisis y en la cantidad
de estados explorados por JPF, sin 
comprometer la cobertura de las estructuras objetivo. \pp{Qué cobertura?
tenemos números de cobertura para mostrar que no se reduce? O algún experimento
por el estilo?} \cacho{Lo sacamos, no hay nada de esto, la idea era como decir que no se pierden estructuras, que se aseguran que estan las mismas y mas}

%Para focalizar la comparación en los distintos casos, 
%consideramos en cada clase–método todos los niveles de \emph{scope} que finalizan sin \emph{timeout}
%De esta manera, los resultados reflejan el comportamiento completo de cada técnica en el rango alcanzable por \emph{JPF}. 
%Cuando \textsf{API} alcanza \emph{timeout} en un determinado \emph{scope}, ese resultado indica un límite práctico 
%de escalabilidad para dicha variante
% , mientras que \textsf{MGO} (Métodos generadores de Objetos) en general logra completar en el mismo \emph{scope}

\cacho{Estaba mal lo habia hecho solo para un caso, Charlarlo con Pablo como lo explico}

En las tablas se muestran los resultados para los scopes en los que
alguna de las técnicas termina antes de llegar al timeout.
\cacho{Calcular de nuevo el promedio de sumar todos los casos y todos los tiempos}
En el análisis global, considerando los \emph{scopes} para los que ambas técnicas terminan
antes del timeout, estas 61 combinaciones, el \emph{driver} con \textsf{MGO} explora 14070756 estados
en 1591s, mientras que \textsf{API} visita 198130649 estados en 17,240 segundos.
En promedio por casos, \textsf{MGO} explora 234513 estados en 26.5 segundos, mientras que \textsf{API} 
explora 3302177 estados en 297.3 segundos. Esto implica que \textsf{MGO} reduce el espacio de estados en 
92.9\%\ y el tiempo en 91\%\ aproximadamente respecto de \textsf{API}. 
Se observa tambien que \textsf{API} alcanza el timeout en 9/61 combinaciones
(14\%) \pp{son más que 9, hay que recalcular los números?}. 

%Bajo el mismo \emph{scope}, \textsf{API} explora aproximadamente 1308\%\ mas de estados y tarda 984\%\ mas de tiempo que \textsf{MGO}.
%En cuanto a la interpretación, 

En resumen, los resultados muestran evidencia preliminar de que construir \emph{drivers}
exclusivamente con los métodos generadores de objetos identificados por nuestro
enfoque \pp{cuál?} \cacho{nuevo}(usando el algoritmo de \emph{Hill Climbing} (\ref{alg:approachHC}) 
con la función objetivo del generador exhaustivo acotado (\ref{sec:fitnessGE})), reduce significativamente la cantidad de estados que debe
explorar JPF, y consecuentemente reduce el tiempo de la verificación. Esto le
permite a nuestro enfoque realizar la verificación para scopes más grandes
en varios de los casos analizados. 
Al mismo tiempo, la suficiencia del conjunto de \textsf{MGO}
garantiza que se preserva la capacidad del driver \textsf{MGO} de producir todas 
las instancias que genera el driver API. \pp{Hay que hacer un análisis de por
    qué hay más states en API, calculo que es porque se construye la misma
instancia muchas veces, pero lo tendremos que charlar.}


%poda de manera efectiva el factor de ramificación en los puntos de elección
%no deterministas del verificador (ej., \texttt{Verify.random}), lo que reduce
%tanto el número de estados alcanzables como el costo de retroceso durante la
%exploración. Al mismo tiempo, la suficiencia del conjunto de \textsf{MGO}
%garantiza que se preserva la capacidad de construir todas las instancias
%(acotadas por el \emph{scope}) y de poder escalar a \emph{scope} mas altos que pueden ser 
%relevantes para la verificación de las propiedades,
%sin sacrificar cobertura estructural dentro del alcance definido.
%
%Así, los casos con \emph{timeout} no se eliminan del análisis, 
%sino que se interpretan como evidencia de la explosión combinatoria que introduce la API completa, 
%en contraste con la reducción sustancial que se obtiene al restringirse al conjunto suficiente de métodos generadores.






%Esta evidencia refuerza la utilidad práctica 
%de nuestra técnica en entornos de verificación formal, donde el costo computacional de la exploración 
%del espacio de estados es un factor crítico.



% Para ilustrar este mecanismo, ponemos bajo análisis el método \texttt{put} de \texttt{TreeMap}.
% La generación de entradas para probar este método puede realizarse mediante un controlador como el mostrado en la immplentación mostrada en  \ref{lst:driverBLD}.
% Para mitigar el problema de la explosión combinatoria, propusimos utilizar únicamente los métodos generadores 
% detectados automáticamente por nuestro enfoque con el algoritmo de \emph{HC} y la funcion objetivo de \emph{FRC} (ver Sección \ref{sec:experimentalIdentificacionPrecision}). 
% Como se muestra en el framento de codigo  \ref{lst:driverBLD}, el controlador basado exclusivamente en los métodos generadores de objectos de 
% \texttt{TreeMap} selecciona sólo dos métodos, suficientes y minimal, para generar todas las instancias 
% válidas dentro del alcance definido. En consecuencia, JPF puede explorar las mismas configuraciones 
% de objetos que con la API completa, pero con un espacio de búsqueda significativamente más reducido.

% El patrón se mantiene de forma consistente a través de múltiples estructuras de datos de 
% \texttt{java.util}, incluyendo \texttt{HashMap}, \texttt{HashSet}, \texttt{LinkedList}, 
% \texttt{TreeMap} y \texttt{TreeSet}. Por ejemplo, en el análisis del método \texttt{remove} 
% en \texttt{HashMap}, al aumentar el \textit{scope} de 4 a 6, el número de estados explorados 
% con el controlador basado en la API completa crece de 627361 a más de 22 millones, con tiempos 
% de ejecución que alcanzan los 3011 segundos (aproximadamente 50 minutos). En cambio, utilizando únicamente los métodos generadores de objetos, 
% se recorren menos de 2.3 millones de estados, en un tiempo de análisis significativamente menor.

% Este mismo comportamiento se replica en otras estructuras. En el caso de \texttt{LinkedList}, 
% el uso de todos los métodos de la API lleva a una explosión de estados (más de 39 millones en 
% \textit{scope} 6), mientras que con los métodos generadores de objectos, el número de estados se reduce a aproximadamente 
% 1.1 millones para el mismo scope. En términos de tiempo, esta diferencia se traduce en 2771 segundos contra apenas 221. 
% Asimismo, para \texttt{TreeMap} con el método \texttt{put}, el uso de los métodos generadores de objectos en 
% \textit{scope} 5 permite mantener el análisis por debajo de los 3100 segundos, mientras que 
% la versión con todos los métodos alcanza el límite de tiempo (TO que es igual a 5000 segundos).

% Además de la reducción en el tiempo de verificación y en los estados explorados, la comparación 
% entre ambos controladores confirma que el conjunto reducido de métodos generadores de objectos es suficiente 
% para generar todas las configuraciones relevantes de entrada que permiten evaluar correctamente 
% las propiedades del programa. Esto implica que, en la práctica, los métodos adicionales disponibles 
% en la API no contribuyen a la diversidad estructural de las entradas, sino que solo introducen 
% complejidad innecesaria en el espacio de búsqueda.

% En resumen, estos resultados evidencian que la identificación automática de métodos generadores no 
% solo mejora la calidad de las entradas en contextos como la generación de tests, sino que también 
% es una estrategia efectiva para escalar técnicas de verificación formal a estructuras de datos más 
% complejas. Al eliminar métodos superfluos y reducir el espacio de búsqueda, se mejora significativamente 
% el rendimiento de herramientas como JPF, sin sacrificar precisión ni completitud en el análisis.




\section{Generación exhaustiva acotada basada en la API}
\label{sec:experimentalBeapi}

En esta sección evaluamos empíricamente la técnica de generación exhaustiva
acotada BEAPI. 
Examinamos su eficiencia (tiempo de generación y escalabilidad respecto de los
\emph{scopes}) con respecto a enfoques existentes, y el impacto de las optimizaciones de BEAPI sobre su desempeño.
También realizamos una evaluación preliminar que muestra que BEAPI puede ser útil para chequear la
consistencia entre los objetos que puede generar la API y las restricciones que
imponen las especificaciones formales de invariantes de representación de estructuras (\texttt{repOK}). 
Por último, evaluamos preliminarmente que tan buenos son los objetos producidos por BEAPI en el
contexto del \emph{testing} parametrizado.

Así, en la evaluación de BEAPI buscaremos responder a las siguientes preguntas de
investigación:

\begin{itemize}
\item \emph{RQ4}: ¿Qué tan eficiente es BEAPI para la generación exhaustiva acotada de objetos?
\item\emph{RQ5}: ¿Cuál es el impacto de las optimizaciones propuestas en el rendimiento de BEAPI?
\item\emph{RQ6}: ¿Puede BEAPI ayudar a encontrar discrepancias entre especificaciones de invariantes de clase (repOK) y la capacidad de generación de objetos de la API?
\item\emph{RQ7}: ¿Qué tan efectivos son los objetos producidos por BEAPI para el testing parametrizado?  
\end{itemize}

\subsection{Configuración experimental}
\label{sec:confExperimentalBeapi}

Como casos de estudio, utilizamos implementaciones de estructuras de datos
tomadas de tres benchmarks existentes: \textsf{Kiasan} \cite{Deng06}, 
\textsf{FAJITA} \cite{Abad13}, \textsf{ROOPS} \cacho{VER CITA}. También
incluimos las clases utilizadas en la evaluación de
\textsf{Korat} \cite{Boyapati02}, y un conjunto de clases de proyectos reales de
software que la denominamos \textsf{Real World}, donde se incluyen clases de
\emph{java.util} (TreeMap, TreeSet, LinkedList, Hasmap), de \emph{Apache Commons
Collections} (NodeCachingLinkedList) y un Scheduler de SIR.

Estos casos de estudio cubren una amplia variedad de estructuras de datos complejas, tales como listas doblemente enlazadas, árboles binarios de búsqueda, 
árboles rojo-negro, heaps binomiales, heaps de Fibonacci, entre otras. Una característica clave 
de estos casos de estudio es que incluyen especificaciones formales de invariante de clase (métodos 
\texttt{repOK}) escritas por sus propios autores, lo que resulta fundamental
para realizar las comparaciones con Korat, que requiere invariantes para
realizar la generación, y para los experimentos para responder a RQ6.

Para la evaluación se utilizó el prototipo de la herramienta BEAPI, que se
describe en la literatura en los trabajos de Politano et. al. \cite{Politano23,
Politano24}.

\cacho{Lo mejore un poco}
En BEAPI, la identificación de métodos generadores de objetos se realiza en una 
etapa previa a la generación exhaustiva, utilizando el algoritmo de 
\emph{Hill Climbing} con la función objetivo basada en el generador exhaustivo acotado. (ver Sección: \ref{sec:eficienciaBuilders})
\pp{Poner cual de los algoritmos de la tesis usamos. Charlemoslo}, 
Para esta etapa, se empleó un \emph{scope} de 5 en todos los casos. 
\cacho{Nuevo esto, saque lo de todos los benchmarks para que no haya confunsion}
 El tiempo máximo requerido por el proceso de identificación fue de 68 segundos en el peor caso 
 (en el benchmarks \textsf{Real World}, el caso de estudio TreeMap, que tiene 32 métodos en su API).
% con 32 métodos, el más costoso). 
% El tiempo máximo requerido por el proceso de identificación fue de 
% 26 segundos en los benchmarks \textsf{Kiasan}, \textsf{FAJITA}, \textsf{ROOPS} y 
% \textsf{Korat} (siendo el caso más costoso SLL de ROOPS, con 11 métodos), 
% y de 68 segundos en los benchmarks \textsf{Real World} (siendo TreeMap, 
% con 32 métodos, el más costoso). 
En todos los casos verificamos manualmente que los métodos identificados 
constituyeran un conjunto suficiente de métodos generadores de objetos.
\pp{Hay que ver qué hacer con esto: Reportamos los tiempos de identificación de
builders en el sitio web del artículo [26]. Tenemos una sección experimental
entera con resultados de builders, hay que linkearlos con eso me parece en
lugar de citar el paper.} \cacho{Se deberia sacar, lo agregue como referencia}

Nótese que la generación exhaustiva acotada suele realizarse de manera
incremental, para scopes cada vez más grandes, y que los generadores de objetos identificados 
pueden reutilizarse en las distintas ejecuciones. Por lo tanto, los 
tiempos de identificación de generadores de objetos se amortizan a través de las 
diferentes ejecuciones, lo que dificulta calcular cuánto agregan dichos tiempos 
a los tiempos totales de ejecución de BEAPI en cada caso. Por esta razón, no incluimos 
los tiempos de identificación de builders en los tiempos de ejecución de BEAPI en ninguno 
de los experimentos de las secciones a continuación. Además, para los scopes más
grandes y las estructuras más complejas —que son los más importantes para
evaluar la eficiencia de la generación exhaustiva acotada— el tiempo de identificación 
de métodos generadores de objetos suele ser relativamente pequeño en relación con los 
tiempos de generación.

En la comparación de eficiencia con técnicas existentes (RQ4) se utilizó Korat,
que es la herramienta existente de generación exhaustiva acotada más eficiente
\cite{Siddiqui09} 
\cacho{Hecho} \pp{No es la cita a Korat. Es la cita al paper que evalua Korat
contra otras técnicas de BEG}. Tener en cuenta, como
Korat requiere de repOKs, se emplearon los \texttt{repOK}s provistos por los autores 
de cada caso de estudio. 
En algunos casos se identificaron errores o especificaciones incompletas en los \texttt{repOK}s; 
estos se señalan de forma explícita en los resultados y se analizan en más
detalle en la Sección \ref{sec:existing-specs-analysis} (RQ6).

Para RQ7 se comparan los objetos generados por BEAPI, una técnica de generación
aleatoria (Randoop), y una variante de Randoop que sólo usa métodos generadores de objetos
(R-Builders). 

Los experimentos se llevaron a cabo en una computadora con procesador Intel Core i7-8700
(6 núcleos, 12 threads, frecuencia de hasta 4.6 GHz, 12 MB de caché, octava generación),
16 GB de RAM y sistema operativo Ubuntu Linux 20.04.

Para cada ejecución individual se estableció 
un tiempo máximo de 60 minutos. Todos los experimentos fueron diseñados para ser reproducibles, 
y el artefacto experimental con el código fuente se encuentra disponible en: \url{https://github.com/mpolitano/bounded-exhaustive-api}

\subsection{RQ4: Eficiencia de BEAPI}
\label{sec:beapi-eficiencia}


En esta sección comparamos BEAPI y Korat en términos de tiempo 
de ejecución, número de estructuras generadas y exploradas, y escalabilidad con
respecto a los \emph{scopes}.
Los resultados experimentales se muestran en la Tabla \ref{table:korat-beapi} y \ref{table:korat-beapi1}.
\pp{No entra la tabla en una página. Hay que dividirla en 2. Además tiene muchos
    errores, se corrieron varias líneas como FibHeap ROOPS s4, o SLList Fajita
s 8,9 y 10. Hay que revisar todo.}

\cacho{Hecho, FALTA CONTROLAR Y QUE ESTEN LOS MISMO CASOS QUE EN LA RQ siguiente}
La Tabla presenta una muestra representativa de los resultados para los benchmarks
\textsf{Korat}, \textsf{FAJITA}, \textsf{ROOPS} y \textsf{Kiasan} (intentamos mantener la misma proporción de casos buenos y malos para cada técnica en los datos que reportamos). \pp{Los casos
restantes se pueden encontrar online \cite{}. Queda raro que esta tabla sea la
única que esté recortada en la tesis. hay que ver si se puede hacer algo al
respecto.}.
Para cada clase evaluada, informamos los tiempos de generación 
(en segundos, columna \emph{Time}), la cantidad de estructuras válidas generadas (\emph{Generated}) y el total de estructuras exploradas (\emph{Explored}) por cada técnica. 
Incluimos, para cada enfoque, los \emph{scopes} más grandes (columna \emph{S})
alcanzados por las técnicas antes de llegar al tiempo límite de 60 minutos. De
esta manera, si surgieran problemas de escalabilidad, podrían identificarse fácilmente. Vale
aclarar que no se utilizó el benchamark \textsf{Real World} en esta sección, ya que
no se dispone de \texttt{repOK}s para estas clases, lo que impide una comparación justa con \textsf{Korat}.

Para obtener resultados de rendimiento adecuados para \textsf{BEAPI}, realizamos
testing exhaustivo de los métodos de la API de las clases para asegurar que
sean correctos. No intentamos modificar los \texttt{repOK} de ninguna manera, ya que eso alteraría el rendimiento de \textsf{Korat}, y uno de nuestros objetivos aquí es evaluar el rendimiento de \textsf{Korat} utilizando \texttt{repOK}s escritos por distintos programadores.

Es esperable que existan diferencias en las estructuras exploradas, dado que los
espacios de búsqueda de \textsf{Korat} y \textsf{BEAPI} son distintos. Sin
embargo, para el mismo caso de estudio y el mismo scope, cabría esperar que
ambos enfoques generen la misma cantidad de estructuras válidas. Esto ocurre en
la mayoría de los experimentos, con dos tipos notables de excepciones. Primero,
hay casos en los que el \texttt{repOK} contiene errores; estos casos aparecen
sombreados en las tablas (los errores de los invariantes se analizan en 
profundidad en la Sección \ref{sec:existing-specs-analysis}).

Segundo, la noción ligeramente diferente de scope
utilizada por cada técnica puede causar discrepancias. Esto solo sucede para
Árboles Rojo-Negro (\texttt{RBT}) y montículos de Fibonacci (\texttt{FibHeap}),
que se muestran en negrita. En estos casos, ciertas estructuras de tamaño $n$
solo pueden generarse a partir de estructuras más grandes, mediante inserciones
seguidas de eliminaciones y luego nuevas inserciones, para producir
configuraciones de balance específicas. \textsf{BEAPI} descarta secuencias generadas 
tan pronto como superan el tamaño máximo de la estructura, por lo que no puede generar 
estas estructuras.

En resumen, la Tabla \ref{table:korat-beapi} muestra que para el benchmark \emph{KORAT} la
herramienta \emph{Korat} es más veloz en 4 de 6 clases; 
en \textsf{FAJITA}, \emph{BEAPI} supera a \emph{Korat} en 3 de 4 casos; en
\emph{ROOPS}, \emph{BEAPI} lo hace en 5 de 7 casos; 
y en \textsf{Kiasan}, \textsf{Korat} es más rápido en 6 de 7 clases.
Observamos que \textsf{BEAPI} muestra un mejor rendimiento en estructuras con
invariantes de representación más fuertes, 
como \texttt{RBT} y árboles binarios de búsqueda (\texttt{BST}); a menudo estos
casos presentan una cantidad de relativamente más chica de estructuras válidas.

En los casos con invariantes más débiles, en los que la cantidad de estructuras
válidas crece más rápidamente con respecto al scope, como las listas doblemente
enlazadas (\texttt{DLList}), \textsf{Korat} suele ser más eficiente. Un mayor 
número de estructuras válidas implica que \textsf{BEAPI} debe crear un número
mayor de secuencias de test totales, lo que impacta negativamente en su rendimiento.

Como era de esperarse, la manera en que están escritos los \texttt{repOK} tiene
un impacto significativo en el rendimiento de \textsf{Korat}. Por ejemplo, para
montículos binomiales (\texttt{BinHeap}), \textsf{Korat} alcanza scope 8 usando
el \texttt{repOK} de \texttt{Roops}, scope 10 con el de \texttt{FAJITA} y scope
11 con el de \texttt{Korat} (todos equivalentes en cuanto a las estructuras
generadas). En la mayoría de los casos, los \texttt{repOK} del benchmark de
\texttt{Korat} resultan en un mejor rendimiento, ya que están afinados
específicamente para su uso con \textsf{Korat} (estos \texttt{repOK}s producen
    que la estrategia de poda de estructuras inválidas de \textsf{Korat} se
ejecute con más frecuencia, reduciendo significativamente la cantidad de
estructuras exploradas). 

En conclusión, \emph{BEAPI} es una herramienta eficiente para la generación exhaustiva acotada. 
En general, presenta una eficiencia y una escalabilidad comparable a la de
\textsf{Korat}, sin requerir de un \texttt{repOK} formal para la generación. 

\input{tables/tableEficienciaBEAPI.tex}
\input{tables/tableEficienciaBEAPI1.tex}

\subsection{RQ5: Impacto de las optimizaciones propuestas}
\label{sec:optimizations}

\input{tables/optimizaciones}
\pp{Hay errores de formato en las tablas. Algunas líneas están incompletas. Y
faltan guiones de TOs.}

En esta pregunta de investigación evaluamos el impacto que tienen las optimizaciones propuestas 
de \emph{BEAPI} en su rendimiento. 
Concretamente, analizamos cuatro configuraciones de la herramienta:
\textsf{CI/MGO}, \pp{cambiar SM/BLD por CI/MGO así queda igual en toda la
tesis. Chequear en toda la tesis.} \cacho{Hecho} que habilita tanto 
la coincidencia de estados (\emph{CI}) como la identificación de métodos
generadores de objetos (\emph{MGO}); 
\textsf{CI}, que activa únicamente la coincidencia de estados; \textsf{MGO}, que habilita únicamente 
la identificación de métodos generadores de objetos; y \textsf{NoOPT}, que desactiva ambas optimizaciones.

La Tabla \ref{tab:results-realWorld} resume los resultados para el conjunto \textsf{Real World} \pp{Y las
otras? Meté una tabla por benchmark y referencialas por separado}.
\cacho{Hago analisis por cada una?}
% PABLO: Esto está en la configuración experimental
%(seis implementaciones ampliamente usadas: \texttt{LinkedList} con 67 métodos, \texttt{TreeSet} con 34 métodos, \texttt{TreeMap} con 61 métodos, \texttt{HashMap} con 45 métodos, \texttt{NCL} con 34 métodos y \texttt{Schedule} con 12 métodos). 
%Resultados análogos se reportan para los demás benchmarks en las tablas correspondientes de esta sección; en conjunto, todos ellos muestran el mismo patrón y fundamentan las conclusiones presentadas.

Los resultados muestran de forma contundente que la configuración sin optimizaciones (\textsf{NoOPT}) 
tiene un rendimiento deficiente en general, inclusive en los estudios de caso más simples y
para los scopes más pequeños. 
Los \emph{scopes} pequeños (3) resultan inadecuados para generar conjuntos de
objetos útiles para revelar errores. 
\textsf{NoOPT} evidencia los problemas de escalabilidad que enfrentan 
las técnicas de generación exhaustiva en presencia de APIs ricas y estructuras complejas.

De las optimizaciones propuestas, la coincidencia de estados (\textsf{CI})
es la que mayor impacto tiene en la eficiencia de BEAPI (comparar los resultados
de NoOPT y CI). Por sí sola, permite escalar a scopes considerablemente más
grandes y reduce los tiempos de ejecución en varios órdenes de magnitud con respecto a \textsf{NoOPT}. 
Esto muestra que eliminar secuencias de test redundantes, es decir, que producen
los mismos objetos, es crucial para la eficiencia de BEAPI.

La segunda optimización, el uso de métodos generadores de objetos
(\textsf{MGO}), es mucho más relevante en casos donde el número de métodos en la API es grande (más de 10), y de manera notable en las estructuras de datos del mundo real (con 20 o más métodos). En 
estructuras del benchmark \textsf{Real World} \pp{Hay una mezcla de fuentes
importante en todo el texto. Hay que definir qué fuente para cada cosa y usarla
siempre igual.} como \texttt{TreeMap}, \texttt{LinkedList} o \texttt{HashMap}, donde existen 
decenas de métodos públicos, limitar la generación de secuencias a combinaciones 
de MGOs tiene un impacto significativo en la eficiencia del proceso de
generación. Como era esperable, en las clases que tienen pocos métodos, el impacto de utilizar MGOs es 
relativamente pequeño.

El mayor beneficio se observa en la configuración \textsf{CI/MGO}, donde ambas optimizaciones están 
habilitadas. En esta configuración, \emph{BEAPI} es significativamente más
rápido, y alcanza scopes mucho mayores que \textsf{NoOPT} en todos los casos
analizados.
Por ejemplo, en \textsf{ROOPS}, \textsf{CI/MGO} es más de un orden de magnitud
más rápido que \textsf{CI} en AVL y RBT, y alcanza al menos un scope adicional
en NCL, LList, y RBT. 
En \textsf{Real World}, el uso de MGOs permitió que \textsf{CI/MGO} escalara a
scopes significativamente mayores en todos los casos excepto TMap y TSet, donde igualmente \textsf{CI/MGO} muestra
una reducción importante en los tiempos de ejecución. 
En general, las optimizaciones propuestas tienen un impacto crucial en el
rendimiento y la escalabilidad de BEAPI, y ambas deben habilitarse para obtener
buenos resultados en la práctica.


\subsection{RQ6: Análisis de invariantes de representación usando BEAPI}
\label{sec:existing-specs-analysis}

Esta pregunta de investigación evalúa si \textsf{BEAPI} puede ser útil para asistir a los usuarios 
en la detección de fallas en los métodos \texttt{repOK} (descripciones formales de los invariantes 
de representación de las clases), a través de la comparación entre el conjunto 
de objetos que se pueden generar mediante la API y aquellos que se pueden generar a partir del 
invariante de representación.

Para ello, diseñamos el siguiente procedimiento automatizado. En primer lugar, ejecutamos 
\emph{BEAPI} para generar un conjunto de estructuras, denominado \texttt{SBEAPI}, utilizando únicamente 
la API de la clase, y ejecutamos \emph{Korat} para generar un segundo conjunto, \texttt{SKORAT}, 
utilizando únicamente el método \texttt{repOK}. Ambas ejecuciones se realizan
para un mismo \emph{scope}. En segundo lugar, canonizamos todas las estructuras
en ambos conjuntos utilizando el proceso de linearización (Sección
\ref{sec:stateMatching}), de modo que las estructuras isomorfas puedan 
compararse de forma directa. Finalmente, verificamos si los conjuntos
\texttt{SBEAPI} y \texttt{SKORAT} son iguales o no.  

El procedimiento anterior puede producir tres resultados. Cuando \texttt{SBEAPI} $\subset$ \texttt{SKORAT}, 
puede deberse a que la API genera un subconjunto de las estructuras válidas, a que \texttt{repOK} 
sufre de subespecificación (es decir, permite estructuras que no deberían ser válidas), o ambos. 
En estos casos, las estructuras que están en \texttt{SKORAT} pero no en
\texttt{SBEAPI} son evidencia de potenciales errores, y el usuario debe
analizarlas manualmente para determinar el origen del problema. Como en este
experimento \textsf{BEAPI} se ejecutó con métodos generadores de objetos que son
correctos, el error debe estar presente en el \texttt{repOK}.
Clasificamos estos errores como \texttt{sub} \pp{Yo lo reemplazaría por sub,
así queda en castellano}, \cacho{Hecho}
ya que el invariante permite más estructuras de las que debería.

Por el contrario, cuando \texttt{SKORAT} $\subset$ \texttt{SBEAPI}, puede
indicar que el \texttt{repOK} es demasiado restrictivo (sobreespecificado), que 
la API genera estructuras inválidas, o ambas cosas a la vez. Las estructuras en
\texttt{SBEAPI} que no pertenecen a \texttt{SKORAT} son testigos del error, y
nuevamente deben ser analizadas manualmente por el usuario. Nuevamente, como la
los métodos utilizados por \textsf{BEAPI} son correctos, el error en este caso
debe estar en el \texttt{repOK}. Reportamos los
errores de sobreespecificación en los \texttt{repOK}s (confirmados manualmente)
que fueron evidenciados por estas estructuras como \texttt{sobre} \pp{Yo lo
    reemplazaría por sobre, así queda en castellano}.\cacho{Hecho}

En algunos casos, se encontraron diferencias en ambas direcciones: estructuras
en \texttt{SKORAT} que no están presentes en \texttt{SBEAPI}, y estructuras \texttt{SBEAPI} que no 
pertenecen a  \texttt{SKORAT}. Estas diferencias pueden deberse a fallas en la
API, fallas en el \texttt{repOK}, o ambas. Reportamos las fallas en los
\texttt{repOK}s confirmadas manualmente que fueron evidenciadas por dichas
estructuras simplemente como \emph{error} (el \texttt{repOK} describe un
conjunto de estructuras distinto del que debería, ya que la API es correcta).

Cabe destacar que las diferencias entre las definiciones de scope en los
enfoques pueden provocar discrepancias entre los conjuntos \texttt{SBEAPI} y
\texttt{SKORAT}. Esto sólo ocurrió en las estructuras \texttt{RBT} y
\texttt{FibHeap}, donde \textsf{BEAPI} generó un conjunto más chico de 
estructuras que \textsf{Korat} para el mismo scope, debido a restricciones de balance 
(como se explicó en la Sección \ref{sec:beapi-eficiencia}). Sin embargo, estos "falsos positivos" 
debido a diferencias de scope pueden revelarse fácilmente, dado que todas las estructuras generadas 
por \textsf{Korat} siempre estaban incluidas en las generadas por \textsf{BEAPI} 
cuando se utilizaba un scope mayor para este último. Usando este criterio, descartamos manualmente 
los "falsos positivos" en \texttt{RBT} y \texttt{FibHeap}.

Los resultados del experimento se resumen en la Tabla \ref{table:bugs}. Se
encontraron errores en 9 de los 26 \texttt{repOK} analizados usando el enfoque
descrito anteriormente.  En particular, se detectaron errores de
subespecificación en clases como \texttt{RBTree}, \texttt{AVL},
\texttt{BinTree}, \texttt{FibHeap} y \texttt{NCL}, en los que el \texttt{repOK}
admiten estructuras inválidas.  También se identificaron casos de
sobreespecificación, como en \texttt{FibHeap}, donde el método \texttt{repOK}
rechaza heaps válidos cuyo nodo mínimo es \texttt{null}. Finalmente, se
encontraron errores en las definiciones de altura en clases como \texttt{AVL},
donde las hojas están inicializadas con valores inconsistentes con la
implementación. \pp{Chequear la tabla, hice algunos cambios pequeños en el texto
y puedo haber empeorado el formato} \cacho{Hecho}

Este experimento muestra la dificultad de escribir invariantes de representación correctos y eficientes 
para la generación exhaustiva acotada.  
%Incluso, por autores de benchmarks de testing tomados de la literatura. 
Los errores descubiertos evidencian que los problemas en los \texttt{repOK}s son difíciles de 
detectar manualmente, y que \textsf{BEAPI} puede ser de gran ayuda para esta tarea.


\input{tables/bugRepOk}

\subsection{RQ7: Evaluación de BEAPI y MGOs en el testing parametrizado}
\label{sec:parametrizedTesting}

\pp{Acá los nombres de las técnicas tienen que coincidir con los de la sección
    5.1.4 (RQ3). Además los budgets tienen que ser los mismos que en esa sección (60, 120 y 180,
    como mínimo. Sino parece que estamos eligiendo los budgets a propósito para
    que nos den bien los números. Y una vez que estén los números escribo como
    interpretar los resultados. Pensandolo mejor, estos números tienen que estar
    en RQ3, para responder bien RQ3, y después acá de nuevo agregando BEAPI Para
    responder RQ7. Tenemos que charlar urgente.
}



\pp{Y no olvidarse de mencionar todas las técnicas en la configuración
 experimental.}
\cacho{Lo charlamos, los tiempos son relativos, es algo diferente el experimento.}

\cacho{Nuevo todo esto}

En el experimento de MGO (ver \ref{sec:object_builders_randoop}), generamos objetos utilizando los MGO y toda la API junto con la herramienta \texttt{Randoop} y los empleamos posteriormente en un conjunto de tests parametrizados, con el objetivo de evaluar su impacto en la cobertura.

En esta seccion, evaluamos la calidad de los objetos generados por BEAPI en el contexto del testing parametrizado.
Para ello, comparamos BEAPI con los mismo enfoques de generación aleatoria utilizados en la Sección \ref{sec:object_builders_randoop}:


% A fin de evaluar y comparar el impacto de cada enfoque, se utilizó el benchmark basado en clases 
% del paquete \emph{java.util} debido a su rica API que tienen: \emph{HashSet}, \emph{HashMap}, \emph{TreeMap}, \emph{TreeSet} y \emph{LinkedList}.
% Para cada clase se definió un invariante que permite distinguir objetos válidos de inválidos, permitiendo así evaluar la calidad de los generadores.

\input{tables/randoopvsBeapi}


Vale aclarar,
que en el caso de BEAPI, este tiempo es el equivalente a lo que obtenido hasta maximizar cobertura. Vale recordar al lector, que BEAPI, al ser un generador exhaustivo,
tiene limite de scope y no de tiempo para generar.  (scope 5 en todos los casos).



La Tabla \ref{tab:hashSetTools_beapi} muestra una comparativa de diferentes técnicas aplicadas sobre 
\texttt{HashSet}. \textsf{BEAPI} logra una cobertura de ramas y 
mutación equivalente a \texttt{R-Serialize-MGO}, y superior a la alcanzada por \texttt{R-Serialize-API} y 
\texttt{Randoop} en lapsos de tiempo cortos. Esta eficiencia evidencia que BEAPI puede generar un 
conjunto reducido de objetos, pero de alta calidad para testing, en muy poco tiempo (menos de cinco 
segundos).Esto indica que los objetos generados por \textsf{BEAPI} son 
mas efectivos como entradas, permitiendo detectar fallas y explorar el espacio de estados 
de manera eficiente.

% \cacho{No hablo de porcentaje porq es poco, diria en algunos casos el 1\% mas, no me parece relevante ese dato}
% En términos de ejecución, mientras \texttt{R-Builders} y \texttt{R-Serialize} requieren ejecutar 
% millones de tests para alcanzar altos niveles de cobertura, \texttt{BEAPI} logra el mismo nivel 
% ejecutando apenas unos pocos miles. 

La Tabla \ref{tab:treeSetTools_beapi} presenta los resultados obtenidos para \texttt{TreeSet}. En 
este caso, las diferencias comienzan a notarse con mayor claridad. 
\texttt{BEAPI} alcanza una cobertura de ramas y mutación mejor en cuanto a mutantes matados con respecto a las otras tecnicas.
En el caso de la tabla \ref{tab:treeMapTools_beapi}, se observa que se mantiene la tendencia de que los objectos generados por BEAPI son efectivos para testing parametrizado, logrando coberturas competitivas con tiempos de generación muy bajos.
En este caso, \texttt{BEAPI} alcanza una cobertura de ramas y mutación superior pero se necesitaron 30 segundos, lo cual sigue siendo un tiempo bajo ya que la mejor tecnica (\texttt{R-Serialize-MGO}) necesita 180 segundos para lograr una cobertura similar.

% \texttt{R-Builders} logra 
% una cobertura y mutación ligeramente superior a \texttt{BEAPI}. 
% Una explicación para este comportamiento radica en que ciertas configuraciones válidas de \texttt{TreeSet} pueden 
% requerir secuencias de inserciones y eliminaciones que BEAPI no alcanza debido a su 
% limitación por scope. Las dos variantes con métodos generadores de objetos son claramente más eficientes en tiempo hasta cobertura que las demás. 
% \texttt{BEAPI} alcanza de manera estable 150 ramas y 109 mutación en solo 2 segundos (en todos los presupuestos). 
% \texttt{R-Builders} logra la mejor cobertura absoluta, pero necesita más tiempo, supera las 150 ramas recién a los 10 segundos (151 ramas, 111 mutación). 
% En contraste, \texttt{R-Serialize} tarda 40 segundos para generarar y 11 segundos para ejecutar para quedarse en 125 ramas y 109 mutación, y \textbf{Randoop} llega a menos cobertura (105 ramas) incluso con 150 y 33 segundos.
% Si comparamos, cobertura por segundo, \texttt{BEAPI} llega primero a una alta cobertura y \texttt{R-Builders} puede llegar a cubrir algun mutante mas, pero pagando un costo de tiempo mayor.

% El comportamiento observado se repite en la Tabla \ref{tab:treeMapTools}, donde se evalúa la 
% clase \texttt{TreeMap}. 
% En este caso, \texttt{R-Builders} obtiene más mutación que \textbf{BEAPI} (152 vs 140), es decir, mata, aproximadamente un 6\% mas de mutantes;
% en cambio, \texttt{BEAPI} logra mayor cobertura de ramas en todos los presupuestos (alrededor del 8\% a partir de 20 segundos de tiempo de generación).
% Nuevamente, la explicación podría estar relacionada con estructuras internas que BEAPI no puede construir directamente bajo un scope fijo, como 
% árboles parcialmente desbalanceados que luego se ajustan mediante rotaciones o reordenamientos. 
% A pesar de estas diferencias, \texttt{BEAPI} mantiene una cobertura muy competitiva y logra 
% resultados consistentes con una fracción del esfuerzo computacional, lo que refuerza su valor 
% como técnica efectiva, especialmente en contextos donde se requiere control y exhaustividad 
% estructural.

En cuanto a la Tabla \ref{tab:linkedListTools_beapi} que presenta los resultados obtenidos para la clase 
\texttt{LinkedList}. En este caso, se observa nuevamente que \texttt{BEAPI}, a pesar de 
generar un número reducido de objetos, mantiene niveles constantes de cobertura y mutación. 
Las técnicas \texttt{R-Serialize-API} y \texttt{R-Serialize-MGO} logran cubrir las mismas ramas y casi los mismos 
mutantes que \texttt{BEAPI}, esto se debe a que las listas enlazadas son estructuras más simples,
donde la mayoría de las configuraciones válidas pueden alcanzarse mediante secuencias cortas
de inserciones y eliminaciones.

Con respecto a la tabla  \ref{tab:hashMapTools_beapi}, 
correspondiente a \texttt{HashMap}, se aprecia un caso 
similar. \texttt{BEAPI} logra altos niveles de cobertura con tiempos de generación de 89 segundos, para alcanzar el maximo de cobertura que se puede obtener.
En este caso \texttt{R-Serialize-MGO} se acerca a ese valor pero no llega al mismo en 180 segundos.


Los resultados obtenidos permiten concluir que los objetos generados por \textsf{BEAPI} son 
efectivos y eficientes para ser utilizados en tests parametrizados. Aunque la cantidad total de 
objetos generados es menor, su calidad y representatividad permiten alcanzar los máximos niveles 
de cobertura con una mínima ejecución. Además, se observa que guiar a generadores aleatorios como 
Randoop mediante la identificación previa de métodos generadores de objectos también mejora significativamente 
la cobertura. Esto hace que se reafirme el valor del otro aporte de esta tesis: la identificación automática de métodos generadores.
Este conocimiento no solo permite mejorar herramientas existentes como Randoop (caso de \texttt{R-Builders}), 
sino que también da lugar a técnicas completamente nuevas como \textsf{BEAPI}, 

En contextos donde el tiempo de ejecución es un factor crítico, o donde se necesita eficiencia sin 
sacrificar cobertura, \textsf{BEAPI} se posiciona como una herramienta valiosa para la generación 
de objetos de prueba. Estos resultados también reafirman la importancia de identificar y utilizar 
adecuadamente los métodos generadores para mejorar la calidad de los tests automatizados.












% \texttt{R-Builders} logra 
% una cobertura y mutación ligeramente superior a \texttt{BEAPI}. 
% Una explicación para este comportamiento radica en que ciertas configuraciones válidas de \texttt{TreeSet} pueden 
% requerir secuencias de inserciones y eliminaciones que BEAPI no alcanza debido a su 
% limitación por scope. Las dos variantes con métodos generadores de objetos son claramente más eficientes en tiempo hasta cobertura que las demás. 
% \texttt{BEAPI} alcanza de manera estable 150 ramas y 109 mutación en solo 2 segundos (en todos los presupuestos). 
% \texttt{R-Builders} logra la mejor cobertura absoluta, pero necesita más tiempo, supera las 150 ramas recién a los 10 segundos (151 ramas, 111 mutación). 
% En contraste, \texttt{R-Serialize} tarda 40 segundos para generarar y 11 segundos para ejecutar para quedarse en 125 ramas y 109 mutación, y \textbf{Randoop} llega a menos cobertura (105 ramas) incluso con 150 y 33 segundos.
% Si comparamos, cobertura por segundo, \texttt{BEAPI} llega primero a una alta cobertura y \texttt{R-Builders} puede llegar a cubrir algun mutante mas, pero pagando un costo de tiempo mayor.

% El comportamiento observado se repite en la Tabla \ref{tab:treeMapTools}, donde se evalúa la 
% clase \texttt{TreeMap}. 
% En este caso, \texttt{R-Builders} obtiene más mutación que \textbf{BEAPI} (152 vs 140), es decir, mata, aproximadamente un 6\% mas de mutantes;
% en cambio, \texttt{BEAPI} logra mayor cobertura de ramas en todos los presupuestos (alrededor del 8\% a partir de 20 segundos de tiempo de generación).
% Nuevamente, la explicación podría estar relacionada con estructuras internas que BEAPI no puede construir directamente bajo un scope fijo, como 
% árboles parcialmente desbalanceados que luego se ajustan mediante rotaciones o reordenamientos. 
% A pesar de estas diferencias, \texttt{BEAPI} mantiene una cobertura muy competitiva y logra 
% resultados consistentes con una fracción del esfuerzo computacional, lo que refuerza su valor 
% como técnica efectiva, especialmente en contextos donde se requiere control y exhaustividad 
% estructural.

En cuanto a la Tabla \ref{tab:linkedListTools_Builders} que presenta los resultados obtenidos para la clase 
\texttt{LinkedList}. En este caso, se observa nuevamente que \texttt{BEAPI}, a pesar de 
generar un número reducido de objetos, mantiene niveles constantes de cobertura y mutación. 
Las técnicas \texttt{R-Serialize} y \texttt{R-Builders} logran cubrir las mismas ramas y casi los mismos 
mutantes que \texttt{BEAPI}, esto se debe a que las listas enlazadas son estructuras más simples,
donde la mayoría de las configuraciones válidas pueden alcanzarse mediante secuencias cortas
de inserciones y eliminaciones..

Con respecto a la tabla  \ref{tab:hashMapTools}, 
correspondiente a \texttt{HashMap}, se aprecia un caso 
similar. \texttt{BEAPI} logra altos niveles de cobertura desde tiempos de generación muy 
reducidos, mientras que las otras técnicas necesitan un volumen considerable de objetos y 
tests para alcanzar resultados comparables. En este caso \texttt{R-Builders} obtiene un puntaje de mutación 6–7\% mayor que \texttt{BEAPI} (127–128 vs 119–120). 
Una explicación es que varios mutantes exitosos están asociados a rutas de código y estructuras intermedias que \texttt{BEAPI} no utiliza:
por ejemplo, secuencias largas con inserciones/eliminaciones que pasan por estados transitorios (similar a los casos de \texttt{TreeMap} y texttt{Treeset}). 
\texttt{R-Builders}, al explorar más operaciones y estados no canónicos, tiende a tocar esos caminos y matar esos mutantes extra, aunque al costo de mucha más ejecución. 
De hecho, en ramas \texttt{BEAPI} iguala o supera a \texttt{R-Builders} ( 122 vs 119 a partir de 
GTime=10), reforzando la idea de que la ventaja de \texttt{R-Builders} en mutación proviene de probar más tipos de operaciones y estados que por cubrir mejor el nucleo de la estructura.

Los resultados obtenidos permiten concluir que los objetos generados por \textsf{BEAPI} son 
efectivos y eficientes para ser utilizados en tests parametrizados. Aunque la cantidad total de 
objetos generados es menor, su calidad y representatividad permiten alcanzar los máximos niveles 
de cobertura con una mínima ejecución. Además, se observa que guiar a generadores aleatorios como 
Randoop mediante la identificación previa de métodos generadores de objectos también mejora significativamente 
la cobertura. Esto hace que se reafirme el valor del otro aporte de esta tesis: la identificación automática de métodos generadores.
Este conocimiento no solo permite mejorar herramientas existentes como Randoop (caso de \texttt{R-Builders}), 
sino que también da lugar a técnicas completamente nuevas como \textsf{BEAPI}, 

En contextos donde el tiempo de ejecución es un factor crítico, o donde se necesita eficiencia sin 
sacrificar cobertura, \textsf{BEAPI} se posiciona como una herramienta valiosa para la generación 
de objetos de prueba. Estos resultados también reafirman la importancia de identificar y utilizar 
adecuadamente los métodos generadores para mejorar la calidad de los tests automatizados.

