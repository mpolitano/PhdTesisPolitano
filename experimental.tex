%!TEX root = main.tex
\chapter[Evaluaci\'on]{Evaluaci\'on}
\label{cap:experimental}


En los capítulos \ref{cap:builders} y \ref{cap:beapi} presentamos enfoques para
identificar automáticamente un conjunto de métodos generadores de objetos, y una
técnica novedosa de generación exhaustiva acotada basada en la API. 
En este capítulo, realizamos una evaluación experimental de las técnicas
mencionadas. La sección \ref{sec:experimentalIdentificacion} analiza experimentalmente los
algoritmos de identificación de generadores de objetos, mientras que en la
sección \ref{sec:experimentalBeapi} se evalúa la generación exhaustiva acotada basada en la API.


\section{Algoritmos de identificación de métodos generadores de objetos}
\label{sec:experimentalIdentificacion}

En esta sección, evaluamos experimentalmente los enfoques presentados en el
Capítulo~\ref{cap:builders}. Analizaremos la eficiencia y precision de cada
algoritmo utilizando las funciones objetivo introducidas en la
Sección~\ref{sec:fitness}. Además, mostraremos cómo los métodos identificados
por nuestras técnicas pueden ser aprovechados en el contexto de herramientas
de verificación de software.

Con respecto a estas técnicas, las siguientes preguntas de investigación guían esta experimentación:

\begin{itemize}
\item \emph{RQ1}: ¿Qué tan eficientes son los algoritmos propuestos para
    identificar conjuntos de métodos generadores de objetos?
\item \emph{RQ2}: ¿Qué tan precisos son los algoritmos presentados para identificar métodos generadores de objetos?
\item \emph{RQ3}: ¿Cuál es el impacto de utilizar métodos generadores de objetos
    en el contexto del análisis automático de software?
\end{itemize}

\subsection{Configuraci\'on experimental}

\subsubsection{Casos de estudio.}


La evaluación se llevó a cabo sobre un conjunto de clases Java 
 que manipulan estructuras dinámicas complejas,
incluyendo: \verb"NCL" de Apache Commons Collections~\cite{apache};
\verb"BinaryTree", \verb"BinomialHeap" y \verb"FibonacciHeap", extraídos
de~\cite{Visser:2006}; y \verb"UnionFind", una implementación de conjuntos
disjuntos tomada de JGrapht~\cite{jgrapht}.
\cacho{
También se incluyeron componentes de proyectos reales de software, como 
\verb"Lits" de la implementación de Sat4j~\cite{sat4j}, utilizada previamente 
en~\cite{Loncaric:2018}. \verb"Lits" implementa la interfaz \verb"ILits" y 
administra el “vocabulario” interno del solver (variables y literales): traduce 
literales desde DIMACS a la representación interna, registra el nivel de decisión 
o propagación asociado a cada literal, y mantiene las listas de observación 
(\emph{watch lists}) usadas en la propagación booleana unitaria. En la práctica, 
esto equivale a guardar cuándo se asignó por última vez un literal y qué 
restricciones lo observan}

Por otro lado, se consideraron estructuras adicionales como \verb"Scheduler",
un planificador de procesos tomado del conjunto de benchmarks SIR~\cite{sir},
y varias colecciones del paquete estándar \verb"java.util" de Java%
\footnote{\url{https://docs.oracle.com/javase/8/docs/api/java/util/
package-summary.html}}, incluyendo \verb"TreeMap", \verb"TreeSet",
\verb"HashMap",\\
 \verb"HashSet" y \verb"LinkedList". 

\input{tables/groundTruth}

Para evaluar la precisión de nuestros algoritmos, se construyó manualmente un
conjunto de referencia o \emph{ground truth} que contiene los métodos
generadores considerados minimales y suficientes en cada caso de estudio. Esta
tarea implicó un análisis detallado y manual de cada API, lo que representó un
trabajo considerable, especialmente en los casos más complejos. La
Tabla~\ref{tab:groundTruth} resume este conjunto de referencia, indicando para
cada clase el número total de métodos en su API pública (\#API) y los métodos
identificados manualmente como generadores de objetos. Cabe destacar que en algunos
casos ciertos métodos pueden ser intercambiables (por ejemplo,
\texttt{addFirst} y \texttt{addLast} en \texttt{NCL}).


\subsubsection{Algoritmos evaluados.}

\cacho{NUEVO:}
El objetivo de esta sección es identificar automáticamente el subconjunto de métodos generadores de cada API. 
Modelamos la tarea como un problema de búsqueda sobre subconjuntos \(M \subseteq API\) y empleamos distintos algoritmos 
para explorar ese espacio. La calidad de cada candidato \(M\) se evalúa mediante funciones objetivo 
(\emph{fitness}) definidas en las secciones citadas.

\paragraph{Algoritmos}
\begin{itemize}
  \item \textbf{GA} (\emph{Algoritmo Genético}; véase Sección~\ref{alg:approachGA}): explora subconjuntos de métodos generadores mediante operadores de cruce y mutación, con selección por torneo.
  \item \textbf{HC} (\emph{Hill-Climbing}; \S~\ref{alg:approachHC}): búsqueda local que parte de una solución inicial y aplica modificaciones incrementales aceptando únicamente mejoras según la función objetivo.
\end{itemize}

\paragraph{Funciones objetivo}
Usamos dos funciones objetivo alternativas para evaluar cada conjunto candidato de métodos, 
que dan lugar a las variantes reportadas en las tablas:

\begin{itemize}
  \item \textbf{GE} (\emph{Generación Exhaustiva}; Sección~\ref{sec:fitnessGE}):
mide la cantidad/calidad de objetos distintos alcanzados mediante generación exhaustiva hasta un \emph{scope} fijado.
  \item \textbf{RC} (\emph{Randoop Coverage}; Sección~\ref{sec:fitnessRandoop}): maximiza la cobertura de código (líneas y ramas) alcanzada por los tests generados con Randoop restringidas al conjunto candidato. 
\end{itemize}

% \paragraph{Convenciones de nombres}
% Cuando un algoritmo se evalúa con una función objetivo específica, usamos la notación \textbf{Algoritmo–Fitness}. 
% Por ejemplo: \textbf{GA–GE}, \textbf{GA–RC}, \textbf{HC–GE}, \textbf{HC–RC}.

\pp{Falta describir brevemente los algoritmos evaluados, citando las secciones
correspondientes, y las funciones de fitness. Y darles los nombres abreviados
que después aparecen en las tablas.}

% Como se discutió en el Capítulo~\ref{cap:beapi}, nuestros algoritmos requieren
% la configuración de diversos parámetros que influyen en su eficiencia y
% efectividad. En particular, los parámetros más relevantes para el algoritmo
% genético incluyen la tasa de mutación, la tasa de cruce, el tamaño de la
% población y la estrategia de selección utilizada.

\cacho{Esto es nuevo}
Nuestros algoritmos requieren la configuración de diversos parámetros que influyen en su eficiencia y efectividad.
En particular, para el algoritmo genético (Sección~\ref{alg:approachGA}) utilizamos una tasa de cruce de 0{,}40, una tasa de mutación de 0{,}05, 
selección por torneo con cuatro participantes, una población de 100 individuos y 20 generaciones por ejecución. 
Estos valores se determinaron empíricamente mediante prueba y error, priorizando la eficiencia y la estabilidad de las soluciones y se mantuvieron constantes en todos los experimentos.

Para las evaluaciones con la función objetivo basada en Randoop (Sección~\ref{sec:fitnessRandoop}), 
fijamos un tiempo máximo de ejecución de 30 segundos por candidato 
y controlamos la aleatoriedad mediante semillas. Randoop utiliza esta semilla para controlar la generación de tests y
permitir la reproducción de resultados. Dado que Randoop introduce un grado
considerable de aleatoriedad, optamos por utilizar dos semillas distintas para capturar la variabilidad introducida por el 
generador aleatorio, registrándolas para permitir la reproducción de resultados. 
Esta configuración proporciona una medida más robusta del desempeño de los algoritmos en contextos con 
elementos no determin\'isticos.

En el caso de la función objetivo basada en generación exhaustiva (Sección~\ref{sec:fitnessGE}),
el parámetro central es el \emph{scope}, entendido como la cantidad máxima de objetos por clase/tipo que pueden crearse durante la evaluación.
En nuestros experimentos fijamos \(k=5\), es decir, un máximo de cinco objetos por tipo para cada conjunto candidato de métodos generadores. 
Además, para evitar diferencias espurias que no reflejan cambios semánticos en la representación, 
omitimos campos internos que no afectan la estructura lógica de los objetos generados (por ejemplo, \texttt{modCount}, común en colecciones de Java), 
el cual sólo contabiliza modificaciones internas sin alterar la estructura observada del objeto. \pp{Yo no pondría esto último porque no recuerdo que lo
hayamos explicado en ningún lado.} \cacho{Hay un apartado pequeño, lo podemos charlar}

Para cada combinación de caso de estudio y función objetivo, el algoritmo
genético y el de \emph{Hill Climbing} fue ejecutado cinco veces con los parámetros mencionados. Los
resultados reportados en las siguientes secciones corresponden al promedio de
estas ejecuciones. En el caso particular de la función objetivo basada en
generación exhaustiva, no fue necesario realizar múltiples repeticiones, dado
que dicha evaluación es determinística bajo una misma configuración.

Todos los experimentos fueron ejecutados en una máquina con procesador Intel
Core i7-6700 a 3.4\,GHz y 8\,GB de RAM, corriendo el sistema operativo
GNU/Linux.

\subsection{RQ1: Eficiencia del cómputo de generadores de objetos}

Medimos la eficiencia de nuestros algoritmos en la cantidad de segundos que
requiere la ejecución de los mismos para finalizar la búsqueda de métodos
generadores de objetos a partir de la API. 
%Para evaluar la eficiencia de nuestro algoritmo, 
%llevamos a cabo experimentos utilizando conjuntos de datos de muestra y medimos el tiempo de promedio de las 5 ejecuciones necesarias 
%para obtener las soluciones. 
Comparamos los resultados obtenidos con la técnica evolutiva (\emph{GA}) y con
la técnica greedy (\emph{HC}), y combinamos ambos enfoques en las distintas
funciones objetivo propuestas (\emph{RC} y \emph{GE}). Los resultados se presentan en la Tabla \ref{tab:eficiencia}.
\pp{Corregir el caption de la tabla.}

Como se observa en la tabla, el algoritmo Hill Climbing (\emph{HC}) es más
eficiente que el algoritmo genético (\emph{GA}) en todos los casos de estudio considerados.
El enfoque de Hill Climbing es razonablemente eficiente, tardando solo 20 minutos en el peor de los casos (NCL, debido a la cantidad y complejidad
de su API), mientras que el algoritmo genético puede tardar mas de una hora en
el peor de los casos (\emph{HashSet}). \pp{Hay que poner tiempos promedios además del
peor caso. Y agregarlos a las tablas.}.

\pp{Faltan los tiempos promedios}. 
%Esto se debe a que los algoritmos
%de Hill Climbing comienzan desde abajo hacia arriba, considerando menos métodos antes de considerar más métodos, a diferencia de los 
%algoritmos genéticos, que generan sucesores mediante operadores de cruce y mutación. Esta estrategia favorece al algoritmo Hill Climbing 
%para encontrar los métodos mínimos en tiempos más cortos. Cabe mencionar que en casos de estudio donde el algoritmo genético supera al 
%Hill Climbing, se debe a que la clase bajo prueba tiene pocos métodos, lo que permite que los algoritmos evolutivos converjan más rápidamente que el algoritmo Hill Climbing.
%

En relación con las funciones objectivo, observamos que la basada en el generador exhaustivo (GE) logra 
finalizar la búsqueda de métodos generadores de objetos en menos tiempo en comparación con la función basada en la 
cobertura generada por la suite de tests el algoritmo \pp{Poner el nombre del
algoritmo, y la sección donde está} (RC). \pp{Hay que poner tiempos promedios
para explicar esto. Y agregarlos a las tablas.}.

Esto se aplica a todos los algoritmos implementados. La función 
objetivo GE termina antes en los algoritmos debido a que genera objetos 
para un scope que debe ser especificado manualmente por el usuario (en este caso, 5 para todos 
los casos \pp{esto va antes, cuando se presentan los algoritmos}). Todas tienen un \emph{timeout} de 
30 segundos \pp{esto va antes, cuando se presentan los algoritmos}, pero muchas veces
la generación exhaustiva acotada termina la generación de objetos antes del llegar al timeout. Esto permite
que GE más rápida \pp{cuánto?} en comparación 
con la función objetivo RC. En cambio, el algoritmo HC debe agotar su presupuesto de tiempo
para cada candidato considerado. \pp{Sin embargo, la función objetivo basada en 
    algoritmo XX no requiere que el usuario provea la definición del scope, y
    podría ser más escalable que la generación exhaustiva acotada en casos donde
la cantidad de objetos a generar crezca exponencialmente respecto de los scopes 
(ver si hay alguno). Como trabajos futuros se planean realizar más experimentos 
para evaluar esta hipótesis.}

En conclusión, en términos de eficiencia de nuestro algoritmo, para los casos estudiados en esta tesis, es conveniente utilizar el 
algoritmo Hill Climbing con una función objetivo que cuente los objetos
generados por el generador exhaustivo (\emph{GE}). \pp{La conclusión (respuesta
    a la RQ) tiene que ser con números promedio para cada algoritmo, y debe contrastar los
algoritmos y las funciones de valuación.}
\input{tables/eficienciaBuilders}
\pp{Falta un número para la última fila}

\subsection{RQ2: Precisión de los algoritmos de cómputo de generadores de objetos}
\label{sec:experimentalIdentificacionPrecision}
La efectividad de nuestros algoritmos se mide en función de cuán cercanos están
a identificar el conjunto minimal de métodos 
generadores de objetos a partir de una API. 
Cabe destacar que en todos los casos, nuestros algoritmos 
logran identificar al menos un subconjunto suficiente de métodos generadores de objetos. No obstante, cuanto más reducido sea este subconjunto, el resultado se condsidera mejor desde el punto de vista de la
usabilidad de los generadores de objetos para el análisis de programas. (véase Tabla~\ref{tab:eficiencia}).

En el caso de \emph{NCL}, \textbf{GA+GE} encuentra el conjunto minimal exacto de métodos generadores en todas las ejecuciones.
Sin embargo, con \textbf{GA+RC} el conjunto hallado tiene en promedio \(\mathbf{3{,}80}\) métodos, \pp{acá el
promedio no es lo mejor, habría que explicar los casos por separado. Pero ya lo
hiciste así que fue. si lo cambiamos va a ser mucho laburo.}
lo que indica que en tres
de las cinco ejecuciones se incluyó un método adicional no necesario.
Por su parte, el algoritmo de \emph{Hill Climbing}
(\emph{HC}) encuentra en todas las ejecuciones los metodos correctos con ambas
funciones objetivo.

En estructuras como \emph{UnionFind}, \emph{FibonacciHeap}, \emph{BTree}, \emph{BinomialHeap} y \emph{Scheduler}, todos los 
algoritmos evaluados lograron encontrar el conjunto minimal exacto de métodos
generadores de objectos. Esto puede atribuirse a que estas 
estructuras presentan un menor número de métodos en su API, y sus invariantes son relativamente simples en comparación con otras 
clases evaluadas.

Para la estructura \emph{Lits}, la función objetivo basada en el generador exhaustivo (\emph{GE}) tiende a incluir al menos un 
método adicional. Este fenómeno se debe a cómo el generador maneja los valores iniciales de los arreglos internos en la clase 
\emph{List}. En particular, \emph{Lits} forma parte de una biblioteca Java utilizada para resolver problemas de satisfacibilidad 
booleana (SAT), y contiene colecciones que almacenan literales. Algunos métodos permiten crear o modificar arreglos con literales, 
pero el generador exhaustivo, sujeto al \emph{scope} limitado y a los valores iniciales, no logra producir ciertos arreglos sin 
utilizar métodos adicionales. Es importante aclarar que esto no implica que dichos métodos adicionales sean verdaderos generadores, 
sino que bajo las restricciones actuales (como el \emph{scope} configurado), no existe otra vía para construir determinadas 
estructuras sin agregarlos. Si se incrementa el \emph{scope} o se modifica la clase para exponer mejor sus constructores, entonces 
se logra encontrar el conjunto minimal real \pp{Y por qué no hicimos esto en
lugar de que calcule mal? No entiendo bien el problema, hay que charlarlo.} En contraste, la función \emph{RC} encuentra siempre el conjunto minimal, dado que su 
evaluación no depende del \emph{scope}, sino de la cobertura lograda en ejecución.


Para la estructura \emph{Lits}, la función objetivo basada en el generador
exhaustivo (\emph{GE}) tiende a incluir al menos un método adicional. Este
fenómeno se debe a cómo el generador maneja los valores iniciales de los
arreglos internos en la clase \emph{Lits}. En particular, \emph{Lits} forma
parte de una biblioteca Java utilizada para resolver problemas de
satisfacibilidad booleana (SAT), y contiene colecciones que almacenan
literales. Algunos métodos permiten crear o modificar arreglos con literales,
pero el generador exhaustivo, sujeto al \emph{scope} limitado y a los valores
iniciales, no logra producir ciertos arreglos sin utilizar métodos adicionales.
Es importante aclarar que esto no implica que dichos métodos adicionales sean
verdaderos generadores, sino que bajo las restricciones actuales (como el
\emph{scope} configurado), no existe otra vía para construir determinadas
estructuras sin agregarlos. Si se incrementa el \emph{scope} o se modifica la
clase para exponer mejor sus constructores, entonces se logra encontrar el
conjunto minimal real \pp{Y por qué no hicimos esto en lugar de que calcule
mal? No entiendo bien el problema, hay que charlarlo.} 
\cacho{Nuevo}
Para este trabajo mantuvimos un \emph{scope} fijo (\(k=5\)) para asegurar
la comparación entre sujetos y un presupuesto de búsqueda
constante. Aumentar \(k\) incrementa el costo de forma combinatoria. Del mismo
modo, no modificamos la clase (p.\,ej., exponiendo constructores adicionales)
porque alteraría el sujeto bajo prueba y sesgaría la comparación entre casos.
Bajo estas restricciones, el desvío de \emph{GE} en \emph{Lits} se interpreta
como una limitación operativa del criterio (no del algoritmo), que desaparece
si se relajan las restricciones.

En contraste, la función \emph{RC} encuentra siempre el conjunto minimal, dado que su
evaluación no depende del \emph{scope}, sino de la cobertura lograda en
ejecución.

En las estructuras pertenecientes al paquete \emph{java.util}, como \emph{TreeMap}, \emph{TreeSet}, \emph{HashMap}, \emph{HashSet} y \emph{LinkedList}, 
el comportamiento de los algoritmos evaluados varía significativamente en función de la complejidad de la estructura, 
el tamaño de la API y la función objetivo utilizada.

El algoritmo genético (\emph{GA}), en combinación con la función de cobertura (\emph{RC}), muestra una mayor tendencia a incluir métodos 
adicionales en el conjunto resultante, especialmente en estructuras donde existen muchas formas alternativas de construir una instancia válida. 
Por ejemplo, en \emph{TreeMap} y \emph{TreeSet}, el promedio de métodos identificados es de 5.6 y 3.2 respectivamente, 
frente al mínimo esperado de 3. Este comportamiento puede atribuirse a la naturaleza estocástica del algoritmo genético,
que depende de la exploración guiada por operadores evolutivos (selección, cruce y mutación) y de la presión de la función de aptitud. 
En escenarios donde múltiples combinaciones conducen a configuraciones aceptables, pero no óptimas, el algoritmo puede converger 
hacia soluciones suficientes pero no minimales, incluyendo métodos no esenciales
\pp{No debería pasar esto en teoría, salvo que
no le alcance el tiempo. Hay que charlarlo.}.

Además, la propia función \emph{RC}, basada en cobertura alcanzada mediante herramientas como \emph{Randoop}, 
puede inducir una saturación temprana al favorecer métodos que rápidamente maximizan cobertura parcial, 
aunque no necesariamente contribuyan a una construcción mínima. Esto puede explicar, por ejemplo, los resultados observados en \emph{HashSet} y \emph{HashMap}, 
donde se identifican más métodos de los requeridos para generar la estructura
objetivo. \pp{No debería pasar esto en teoría. Hay que charlarlo.}.
En el caso específico de \emph{HashMap}, incluso la función \emph{GE} tiende a incluir el método \texttt{keySet()}, 
necesario para construir configuraciones específicas que de otro modo no serían alcanzables bajo el \emph{scope} definido. \pp{No debería pasar esto en teoría. Hay que charlarlo.}.

En contraste, el algoritmo \emph{HC} muestra mayor estabilidad en la precisión, 
logrando identificar el conjunto minimal en la mayoría de los casos, 
independientemente de la función objetivo empleada. Su comportamiento determinista y 
su foco en mejoras incrementales lo hacen menos propenso a incluir métodos redundantes, 
aunque esto también puede limitar su capacidad de escape ante óptimos locales \pp{No debería pasar esto en teoría, salvo que
no le alcance el tiempo al genético. Hay que charlarlo.}.

En resumen, la precisión alcanzada por los algoritmos depende fuertemente tanto de la estrategia de búsqueda como de la función objetivo. 
La exploración estocástica de \emph{GA} le permite descubrir soluciones diversas, 
pero también lo expone a converger en conjuntos sobredimensionados si la presión selectiva o la función de evaluación no están cuidadosamente diseñadas. 
Estos resultados destacan la necesidad de calibrar adecuadamente los parámetros y de considerar cuidadosamente la métrica de evaluación para lograr una identificación eficiente y precisa de métodos generadores.
\input{tables/eficaciaBuilders.tex}

\subsection{RQ3: Impacto de los generadores de objetos en el análisis de
software}

En esta sección evaluamos la utilidad práctica de los métodos generadores de objetos 
identificados previamente, en el contexto de la generación automatizada de casos de prueba. 
Estos métodos permiten crear instancias válidas de estructuras de datos, que pueden ser utilizadas como 
entradas en tests unitarios o tests basadas en propiedades.

Para aquellos casos de estudio cuyas clases permiten medir el tamaño de los objetos generados 
(mediante el método \texttt{size()}) y comparar objetos por igualdad (a través de \texttt{equals()}), 
realizamos un experimento con la herramienta \emph{Randoop}. Generamos casos de prueba en dos 
configuraciones distintas: (i) utilizando toda la API disponible de la clase bajo análisis 
(denominada \texttt{}{AllMethods}), y (ii) utilizando únicamente los métodos generadores de objetos
(\texttt{Builders}) identificados por nuestro enfoque en el experimento anterior 
(ver Tabla~\ref{tab:results-obj}).

Para ambas configuraciones, medimos la \textit{cantidad de objetos distintos 
generados} (\textit{No. of Objs.}). Además, definimos tres valores de presupuesto de tiempo para la generación de 
tests: 60, 120 y 180 segundos. Los resultados obtenidos se resumen en la Tabla~\ref{tab:results-obj}.

Los resultados muestran de forma consistente que el uso exclusivo de los métodos generadores 
permite generar una cantidad significativamente mayor de objetos válidos en comparación con el uso 
de la API completa. En promedio, se observa que la cantidad de objetos generados con \texttt{Builders} 
es hasta cinco veces superior a la obtenida con \texttt{AllMethods} en el mismo presupuesto de tiempo. 

Estos hallazgos ponen de manifiesto el impacto positivo de identificar automáticamente métodos 
generadores adecuados. Su utilización en herramientas de prueba como \emph{Randoop} mejora sustancialmente 
la calidad y diversidad de los objetos generados, lo cual es especialmente relevante para clases con 
estado interno complejo. Por lo tanto, concluimos que la identificación automatizada de métodos generadores de objetos 
constituye una estrategia efectiva y valiosa para potenciar la generación automatizada de tests.
\input{tables/objectsBuilders.tex}

\vspace{10pt}
En un segundo experimento, evaluamos el impacto del uso de métodos \emph{builders} en la eficiencia 
del análisis de programas mediante verificación formal. Para ello, utilizamos \emph{Java PathFinder} 
(JPF)~\cite{Visser:2005}, una herramienta de verificación de modelos con exploración explícita del 
espacio de estados para programas Java.\footnote{\url{https://github.com/javapathfinder/jpf-core}}. 
Este experimento lo evaluamos en los casos de estudio del paquete \texttt{java.util}.

Las técnicas de verificación de modelos requieren definir \emph{controladores de prueba}, es decir, 
secuencias de métodos que permitan construir las entradas sobre las que se analizará el programa. 
Idealmente, estas secuencias deberían generarse a partir del conjunto minimal de métodos que permitan 
construir todas las instancias posibles dentro de un alcance definido (\emph{scope}). Sin embargo, 
la selección manual de estos métodos representa una barrera significativa para el uso de herramientas 
como JPF, ya que requiere un análisis exhaustivo de la API y una comprensión detallada de la 
semántica del módulo.

JPF permite definir controladores no deterministas que generan todas las secuencias posibles de 
llamadas a métodos de la API hasta una longitud determinada. Internamente, JPF explora todas las 
ejecuciones posibles, almacenando los estados visitados y retrocediendo ante duplicados. Para ello, 
utiliza puntos de elección no determinista definidos a través de métodos especiales como 
\verb|Verify.getInt(int lo, int hi)| y \verb|Verify.random(int n)|, los cuales introducen bifurcaciones 
en la ejecución.

Para ilustrar este mecanismo, consideramos el análisis del método \texttt{put} de \texttt{TreeMap}. 
La generación de entradas puede realizarse mediante un controlador como el mostrado en la immplentación mostrada en ~\ref{lst:driverAPI}, que incluye todos los métodos disponibles en la API de \texttt{TreeMap}. 
Este enfoque asegura cobertura, pero introduce un crecimiento exponencial en el número de ejecuciones 
a explorar, ya que cada paso del controlador puede seleccionar cualquier método de forma no 
determinista.

\begin{lstlisting}[language=Java,caption={Controlador con todos los métodos},label={lst:driverAPI},captionpos=b]
    private static TreeMap generateStructure(int scope) {
       int maxLength = Verify.getInt(0, scope);
       TreeMap t = new TreeMap();
       for (int i = 1; i <= maxLength; i++) {
          switch (Verify.random(n_methods)) {
             case 0:
                t.put(Verify.getInt(0,scope),Verify.getInt(0,scope));
                break;
             case 1:
                t.remove(Verify.getInt(0,scope));
                break;						
             case 2:
                t.clear();
                break;
             case 3:
                t.containsValue(Verify.getInt(0,scope));
                break;
             ...
             case 11: 
                t.putAll(l);
                break;
          }
       }
       return t;
    }
\end{lstlisting}

Para mitigar esta explosión combinatoria, propusimos utilizar únicamente los métodos generadores 
detectados automáticamente por nuestro enfoque con el algoritmo de \emph{HC} y la funcion objetivo de \emph{RC} (ver Sección~\ref{sec:experimentalIdentificacionPrecision}). 
Como se muestra en el framento de codigo ~\ref{lst:driverBLD}, el controlador basado exclusivamente en los métodos generadores de objectos de 
\texttt{TreeMap} selecciona sólo dos métodos, suficientes y minimal, para generar todas las instancias 
válidas dentro del alcance definido. En consecuencia, JPF puede explorar las mismas configuraciones 
de objetos que con la API completa, pero con un espacio de búsqueda significativamente más reducido.

\begin{lstlisting}[caption={Probando el método put de TreeMap con JPF},label={lst:driverBLD},language=Java,captionpos=b]
    public static void main(String[] args) {
       int scope = 3;
       TreeMap t = generateStructure(scope);
       t.put(Verify.getInt(0,scope),Verify.getInt(0,scope));
       assert t.repOK();
    }
\end{lstlisting}

Los resultados de los experimentos, presentados en las Tablas~\ref{tab:results-jpf1} y 
\ref{tab:results-jpf2}, demuestran que los controladores construidos a partir de nuestros 
métodos generadores permiten una mejora sustancial en eficiencia y escalabilidad. Se observa una 
reducción considerable en el tiempo de análisis y en la cantidad de estados explorados, sin 
comprometer la cobertura de las estructuras objetivo. Esta evidencia refuerza la utilidad práctica 
de nuestra técnica en entornos de verificación formal, donde el costo computacional de la exploración 
del espacio de estados es un factor crítico.

El patrón se mantiene de forma consistente a través de múltiples estructuras de datos de 
\texttt{java.util}, incluyendo \texttt{HashMap}, \texttt{HashSet}, \texttt{LinkedList}, 
\texttt{TreeMap} y \texttt{TreeSet}. Por ejemplo, en el análisis del método \texttt{remove} 
en \texttt{HashMap}, al aumentar el \textit{scope} de 4 a 6, el número de estados explorados 
con el controlador basado en la API completa crece de 627361 a más de 22 millones, con tiempos 
de ejecución que alcanzan los 3011 segundos (aproximadamente 50 minutos). En cambio, utilizando únicamente los métodos generadores de objetos, 
se recorren menos de 2.3 millones de estados, en un tiempo de análisis significativamente menor.

Este mismo comportamiento se replica en otras estructuras. En el caso de \texttt{LinkedList}, 
el uso de todos los métodos de la API lleva a una explosión de estados (más de 39 millones en 
\textit{scope} 6), mientras que con \emph{builders}, el número de estados se reduce a aproximadamente 
1.1 millones para el mismo scope. En términos de tiempo, esta diferencia se traduce en 2771 segundos contra apenas 221. 
Asimismo, para \texttt{TreeMap} con el método \texttt{put}, el uso de \emph{builders} en 
\textit{scope} 5 permite mantener el análisis por debajo de los 3100 segundos, mientras que 
la versión con todos los métodos alcanza el límite de tiempo (TO que es igual a 5000 segundos).

Además de la reducción en el tiempo de verificación y en los estados explorados, la comparación 
entre ambos controladores confirma que el conjunto reducido de métodos \emph{builders} es suficiente 
para generar todas las configuraciones relevantes de entrada que permiten evaluar correctamente 
las propiedades del programa. Esto implica que, en la práctica, los métodos adicionales disponibles 
en la API no contribuyen a la diversidad estructural de las entradas, sino que solo introducen 
complejidad innecesaria en el espacio de búsqueda.

En resumen, estos resultados evidencian que la identificación automática de métodos generadores no 
solo mejora la calidad de las entradas en contextos como la generación de tests, sino que también 
es una estrategia efectiva para escalar técnicas de verificación formal a estructuras de datos más 
complejas. Al eliminar métodos superfluos y reducir el espacio de búsqueda, se mejora significativamente 
el rendimiento de herramientas como JPF, sin sacrificar precisión ni completitud en el análisis.

\input{tables/tableJPFBuilders}
\input{tables/tableJPFBuilders1}

\section{Generación exhaustiva acotada basada en la API}
\label{sec:experimentalBeapi}

En la evaluación de BEAPI buscaremos responder a las siguientes preguntas de
investigación:

\begin{itemize}
\item \emph{RQ4}: ¿Qué tan eficiente es BEAPI para la generación exhaustiva                                                                                       
    acotada de objetos?
\item\emph{RQ5}: ¿Cuál es el impacto de las optimizaciones propuestas en el
    rendimiento de BEAPI?
\item\emph{RQ6}: ¿Puede BEAPI ayudar a encontrar discrepancias entre
    especificaciones de invariantes de clase (repOK) y la capacidad de generación de objetos de la API?
\item\emph{RQ7}: ¿Qué tan efectivos son los objetos producidos por BEAPI para el
    testing parametrizado?  
\end{itemize}

\subsection{Configuración experimental}

Como estudios de caso, utilizamos implementaciones de estructuras de datos tomadas de cuatro bancos 
de pruebas reconocidos: \textsf{Korat}~\cite{Boyapati02}, \textsf{Kiasan}~\cite{Deng06}, 
\textsf{FAJITA}~\cite{Abad13}, \textsf{ROOPS} \cacho{VER CITA}y un conjunto de clases de proyectos reales de software que la denominamos \textsf{realWorld}
donde incluyen clases de \emph{java.util} (TreeMap, TreeSet, LinkedList, Hasmap), de \emph{Apache Commons Collections} (NodeCachingLinkedList) y un Scheduler de SIR

Estos bancos cubren una amplia variedad de estructuras de datos complejas, tales como listas doblemente enlazadas, árboles binarios de búsqueda, 
árboles rojo-negro, montículos binomiales, heaps de Fibonacci, entre otras. Una característica clave 
de estos bancos es que incluyen especificaciones explícitas de invariante de clase (métodos 
\texttt{repOK}) escritas por sus propios autores, lo que garantiza una base confiable para evaluar 
algoritmos de generación de estructuras válidas.

Los experimentos se llevaron a cabo en una computadora con procesador Intel Core i7-8700 
(3.2 GHz) y 16 GB de RAM, ejecutando Ubuntu Linux 20.04. Para cada ejecución individual se estableció 
un tiempo máximo de espera de 60 minutos. Todos los experimentos fueron diseñados para ser reproducibles, 
y el artefacto experimental con el código fuente se encuentra disponible en: ???.

Cada técnica evaluada se ejecutó sobre los mismos casos de estudio, con parámetros estandarizados 
en términos de alcance (\textit{scope}), número de métodos disponibles, y criterios de generación. 
Para la comparación con \textsf{Korat}, se emplearon los \texttt{repOK}s provistos por los autores 
de cada benchmark, sin modificaciones, a fin de preservar la equivalencia semántica entre enfoques 
y evitar sesgos en la evaluación del rendimiento. En los casos donde se identificaron errores o 
especificaciones incompletas en los \texttt{repOK}s, estos se señalan de forma explícita en los 
resultados y se analizan en la Sección~\ref{sec:existing-specs-analysis}.

\cacho{los parametros usados en GA}


\subsection{RQ4: Eficiencia de BEAPI}

Esta pregunta busca evaluar si \emph{BEAPI} es lo suficientemente eficiente como para ser una 
alternativa viable frente a otras técnicas de generación exhaustiva acotada, en particular 
\textsf{Korat}. Para ello, comparamos el desempeño de ambas herramientas en términos de tiempo 
de ejecución, número de estructuras generadas y exploradas, y escalabilidad con respecto al 
\emph{scope} de entrada.

Los resultados experimentales, presentados en la Tabla~\ref{table:korat-beapi}, cubren una muestra 
representativa de casos tomados de los bancos de pruebas \textsf{Korat}, \textsf{FAJITA}, 
\textsf{ROOPS} y \textsf{Kiasan}. Para cada clase evaluada, informamos los tiempos de generación 
(en segundos), la cantidad de estructuras válidas generadas y el total de estructuras exploradas 
por cada técnica. Incluimos, para cada enfoque, el mayor \emph{scope} alcanzado exitosamente dentro 
del tiempo límite de 60 minutos, indicando en negrita los tiempos correspondientes a los límites 
de escalabilidad. Vale aclarar que no se utilizo el benchamarks \textsf{realWorld} en esta sección, ya que
no se dispone de \texttt{repOK}s para estas clases, lo que impide una comparación justa con \textsf{Korat}.

En estos experimentos, se aseguraron condiciones comparables: los \texttt{repOK}s utilizados con 
\textsf{Korat} fueron los provistos originalmente por los autores de cada benchmark y no se 
modificaron para este estudio, evitando así alterar la eficiencia inherente a la técnica. 
Para \emph{BEAPI}, se probó exhaustivamente que los métodos de la API empleados fueran correctos 
y suficientes para el análisis. Como era de esperarse, pueden existir diferencias en los conjuntos 
de estructuras exploradas por cada técnica, debido a los distintos espacios de búsqueda que inducen. 
Sin embargo, para un mismo caso y alcance, ambas herramientas deberían generar la misma cantidad 
de estructuras válidas, salvo en casos donde hay errores en los \texttt{repOK}s o diferencias sutiles 
en la definición de \emph{scope}.

Los resultados muestran un comportamiento mixto. En estructuras con restricciones fuertes —como los 
árboles rojo-negro (\texttt{RBT}) y los árboles binarios de búsqueda (\texttt{BST})— \emph{BEAPI} 
supera a \textsf{Korat}, al explorar menos estados y terminar en menos tiempo. Esto ocurre porque 
en estas estructuras el número de configuraciones válidas es más bajo, lo que permite a \emph{BEAPI} 
recorrer el espacio de soluciones de forma eficiente. Por el contrario, en estructuras más permisivas, 
como listas doblemente enlazadas (\texttt{DLList}) o montículos binomiales (\texttt{BinHeap}), 
\textsf{Korat} logra un mejor rendimiento gracias a su estrategia de poda temprana basada en 
\texttt{repOK}.

Cabe destacar dos tipos de excepciones en los resultados. En primer lugar, algunos casos incluyen 
\texttt{repOK}s con errores, que impactan negativamente en la eficiencia de \textsf{Korat}; estos 
casos están sombreados en la tabla y se analizan más en detalle en la Sección~\ref{sec:existing-specs-analysis}. 
En segundo lugar, en estructuras como \texttt{RBT} y \texttt{FibHeap}, la diferencia semántica en la 
noción de \emph{scope} entre ambas técnicas produce discrepancias. Por ejemplo, hay estructuras de 
tamaño $n$ que solo pueden obtenerse mediante secuencias que superan temporalmente dicho tamaño 
(inserciones seguidas de eliminaciones y nuevas inserciones para provocar reordenamientos de balanceo). 
Como \emph{BEAPI} descarta automáticamente las secuencias que exceden el \emph{scope} máximo, no puede 
llegar a estas configuraciones, mientras que \textsf{Korat} sí lo logra.

Un aspecto clave que influye en la eficiencia de \emph{BEAPI} es la cantidad de estructuras válidas 
generables para un determinado alcance. A medida que crece esta cardinalidad, aumenta también el 
número de secuencias que \emph{BEAPI} debe evaluar, lo cual impacta directamente en su tiempo de 
ejecución. Sin embargo, un punto a favor de \emph{BEAPI} es que no depende de la eficiencia del 
\texttt{repOK}; esto lo hace más robusto ante especificaciones incorrectas o mal escritas, mientras 
que \textsf{Korat} puede verse severamente afectado por ellas.

En cuanto al rendimiento observado, \textsf{Korat} muestra mejor desempeño en 4 de los 6 casos 
pertenecientes a su propio banco de pruebas. En el benchmark \textsf{FAJITA}, \emph{BEAPI} supera 
a \textsf{Korat} en 3 de 4 casos, mientras que en \textsf{ROOPS} lo hace en 5 de 7. Finalmente, 
en el banco \textsf{Kiasan}, \textsf{Korat} resulta más rápido en 6 de 7 casos. Esta tendencia confirma 
que \emph{BEAPI} ofrece un mejor rendimiento en estructuras altamente restringidas —donde el número 
de configuraciones válidas es reducido— mientras que \textsf{Korat} mantiene la ventaja en dominios 
más amplios, donde la cantidad de estructuras válidas crece de forma acelerada. Esto se debe a que 
en tales escenarios, \emph{BEAPI} debe generar y evaluar muchas más secuencias de entrada en cada 
iteración, lo cual penaliza su desempeño.

Un caso ilustrativo es el de \texttt{BinHeap}, donde \textsf{Korat} alcanza el \emph{scope} 8 utilizando 
el \texttt{repOK} de \texttt{ROOPS}, el \emph{scope} 10 con el de \texttt{FAJITA}, y el \emph{scope} 11 
con su propia especificación, todas ellas equivalentes en términos de estructuras generadas. Esto evidencia 
la sensibilidad de \textsf{Korat} a la forma en que se escriben los \texttt{repOK}s, los cuales, si están 
ajustados específicamente para su motor de búsqueda, pueden mejorar significativamente su rendimiento. 
No obstante, cuando los \texttt{repOK}s presentan errores o están mal diseñados, su impacto negativo 
puede ser severo, como se analiza en profundidad en la Sección~\ref{sec:existing-specs-analysis}.

En conclusión, \emph{BEAPI} es una herramienta eficiente para la generación exhaustiva acotada. 
Ofrece tiempos competitivos y, en varios casos, mejores que \textsf{Korat}, especialmente cuando 
se trata de estructuras con invariantes complejas o bajo número de soluciones. Su independencia 
respecto al \texttt{repOK} lo vuelve una alternativa robusta y complementaria para contextos donde 
la calidad de la especificación no está garantizada.
\input{tables/tableEficienciaBEAPI.tex}

\subsection{RQ5: Impacto de las optimizaciones propuestas}
\label{sec:optimizations}

En esta pregunta de investigación evaluamos el impacto que tienen las optimizaciones propuestas 
de \emph{BEAPI} en su rendimiento general frente al enfoque de generación exhaustiva tradicional. 
Concretamente, analizamos cuatro configuraciones de la herramienta: \textsf{SM/BLD}, que habilita tanto 
la coincidencia de estados (State Matching) como la identificación de constructores (\emph{builders}); 
\textsf{SM}, que activa únicamente la coincidencia de estados; \textsf{BLD}, que habilita únicamente 
la identificación de constructores; y \textsf{NoOPT}, que desactiva ambas optimizaciones, funcionando 
como un enfoque de fuerza bruta.

Para este análisis, nos enfocamos en el benchmark \texttt{Real World}, que incluye seis 
implementaciones reales de estructuras de datos ampliamente utilizadas: \texttt{LinkedList} 
(67 métodos en la API), \texttt{TreeSet} (34), \texttt{TreeMap} (61), \texttt{HashMap} (45), \texttt{NCL} (34) y 
\texttt{Schedule} (12). La Tabla~\ref{tab:results-realWorld} 
resume los resultados obtenidos. Cabe mencionar que se realizaron experimentos equivalentes para 
el resto de los benchmarks utilizados en este capítulo, pero se omiten en las tablas en esta tesis por motivos de espacio.
El conjunto \texttt{Real World} representa adecuadamente el comportamiento observado en los demás casos, 
y resulta suficiente para sustentar las conclusiones extraídas en esta sección.

Los resultados muestran de forma contundente que la configuración sin optimizaciones (\textsf{NoOPT}) 
tiene un rendimiento deficiente incluso en los estudios de caso más simples y con alcances bajos. 
Estos \emph{scopes}, si bien pequeños, ya resultan inadecuados para generar conjuntos de prueba de 
calidad y revelan de inmediato la falta de escalabilidad del enfoque de fuerza bruta. Esta limitación 
es representativa de la problemática general que enfrentan las técnicas de generación exhaustiva en 
presencia de APIs amplias y estructuras complejas.

Entre las optimizaciones propuestas, la coincidencia de estados (\textsf{SM}) demuestra ser la más 
impactante a nivel general. Por sí sola, permite escalar a alcances considerablemente mayores y reduce 
los tiempos de ejecución en varios órdenes de magnitud respecto a \textsf{NoOPT}. Su efecto se vuelve 
crítico en aquellas estructuras donde múltiples secuencias de llamadas a métodos pueden generar el mismo estado 
intermedio, lo que genera redundancia innecesaria si no se cuenta con un mecanismo de detección y poda 
de estados previamente explorados.

La segunda optimización, la identificación previa de métodos \emph{builders} (\textsf{BLD}), adquiere 
mayor relevancia a medida que se incrementa la cantidad de métodos disponibles en la API. En 
estructuras reales como \texttt{TreeMap}, \texttt{LinkedList} o \texttt{HashMap}, donde existen 
docenas de métodos públicos con efectos variados, limitar la generación de secuencias a combinaciones 
de constructores válidos tiene un impacto significativo en la eficiencia del proceso de generación. 
Aunque por sí sola esta optimización no siempre permite escalar a scopes altos, su combinación con la 
coincidencia de estados resulta esencial para lograr tiempos razonables de ejecución.

El mayor beneficio se observa en la configuración \textsf{SM/BLD}, donde ambas optimizaciones están 
habilitadas. En esta configuración, \emph{BEAPI} escala significativamente mejor, alcanzando scopes 
mucho mayores en todos los casos analizados, con tiempos de ejecución hasta cientos de veces menores 
que con las demás configuraciones. Por ejemplo, para \texttt{NCL}, se logra alcanzar \textit{scope} 6 
en 73.78 segundos con \textsf{SM/BLD}, mientras que con \textsf{SM} apenas se alcanza \textit{scope} 4 
en más de 3 segundos, y \textsf{NoOPT} no logra ejecutar siquiera scopes bajos. Para \texttt{TreeSet}, 
el contraste es aún más pronunciado: \textsf{SM/BLD} alcanza \textit{scope} 13 en 226 segundos, mientras 
que \textsf{SM} requiere casi 900 segundos para el mismo alcance.

Estos resultados evidencian que las optimizaciones propuestas no solo reducen tiempos de ejecución, 
sino que amplían sustancialmente los límites de escalabilidad de la herramienta. Sin estas mejoras, 
\emph{BEAPI} se comporta como un generador exhaustivo tradicional, enfrentando rápidamente la explosión 
combinatoria del espacio de búsqueda al aumentar el número de métodos o el tamaño de las secuencias. 
En cambio, con las optimizaciones activadas, la técnica logra mantener el proceso dentro de márgenes 
computacionalmente viables incluso en contextos realistas y exigentes.

En conclusión, la activación conjunta de coincidencia de estados e identificación de \emph{builders} 
resulta crucial para obtener un rendimiento competitivo. Estas optimizaciones transforman a \emph{BEAPI} 
en una herramienta capaz de escalar a scopes altos sin sacrificar exhaustividad ni precisión, 
permitiendo su aplicación efectiva en casos de uso reales.

Para todos los experimentos presentados en esta sección, se empleó un \emph{scope} fijo de 5 durante la 
identificación de métodos generadores, y el tiempo máximo requerido por esta etapa fue de 132 segundos, 
correspondiente a la estructura \emph{TreeMap}, que posee 61 métodos públicos. Verificamos manualmente que, 
en todos los casos, el conjunto de métodos identificados fuera suficiente para construir los objetos válidos.

Cabe señalar que la identificación de métodos generadores se realiza típicamente una única vez por clase, 
y puede reutilizarse en múltiples ejecuciones posteriores de \emph{BEAPI}. En consecuencia, su costo se 
amortiza a lo largo del tiempo, para los scopes más grandes, que son los más importantes, el tiempo de identificación de 
métodos generadores es insignificante en relación con los tiempos de generación.Por esta razón, decidimos no incluir los tiempos de identificación 
en los tiempos de ejecución reportados para \textsf{BEAPI} en los distintos experimentos.
No obstante, es importante destacar que este preprocesamiento introduce una optimización significativa en 
términos de eficiencia, con un costo computacional bajo en relación con el proceso completo de generación. 


\input{tables/optimizaciones}


\subsection{RQ6: BEAPI para analizar invariantes de representación}
\label{sec:existing-specs-analysis}

Esta pregunta de investigación evalúa si \textsf{BEAPI} puede ser útil para asistir a los usuarios 
en la detección de fallas en los métodos \texttt{repOK}, a través de la comparación entre el conjunto 
de objetos que se pueden generar mediante la API y aquellos que se pueden generar a partir del 
invariante de representación.

Para ello, diseñamos un procedimiento automatizado en tres pasos. En primer lugar, ejecutamos 
\emph{BEAPI} para generar un conjunto de estructuras, denominado \texttt{SA}, utilizando únicamente 
la API de la clase, y ejecutamos \emph{Korat} para generar un segundo conjunto, \texttt{SR}, 
utilizando únicamente el método \texttt{repOK}. Ambas ejecuciones se realizaron con el mismo 
\emph{scope}. En segundo lugar, canonizamos todas las estructuras de ambos conjuntos utilizando el 
proceso de linearización (Sección~\ref{sec:stateMatching}), de modo que estructuras isomorfas puedan 
compararse de forma directa. Finalmente, comparamos los conjuntos \texttt{SA} y \texttt{SR} para 
identificar diferencias que indiquen discrepancias semánticas entre la definición operativa de la 
clase (la API) y su especificación declarativa (\texttt{repOK}).

El procedimiento puede producir tres resultados distintos. Cuando \texttt{SA} $\subset$ \texttt{SR}, 
puede deberse a que la API genera un subconjunto de las estructuras válidas, que \texttt{repOK} 
sufre de subespecificación (es decir, permite estructuras que no deberían ser válidas), o ambos. 
En estos casos, las estructuras que están en \texttt{SR} pero no en \texttt{SA} son evidencia potencial 
de errores, y fueron inspeccionadas manualmente para confirmar si se trataba efectivamente de fallas 
en \texttt{repOK}. Clasificamos estos errores como \texttt{under}, ya que el invariante permite más 
de lo que debería.

Por el contrario, cuando \texttt{SR} $\subset$ \texttt{SA}, puede indicar que \texttt{repOK} es demasiado 
restrictivo (sobreespecificado), que la API permite estructuras no válidas, o ambas cosas. En este 
caso, se analizan las estructuras que aparecen en \texttt{SA} pero no en \texttt{SR}. Los errores 
confirmados manualmente en esta categoría se etiquetan como \texttt{over}.

En algunos casos, se encontraron diferencias en ambas direcciones: estructuras válidas generadas por 
\textsf{Korat} que no están presentes en \texttt{SA}, y estructuras generadas por \emph{BEAPI} que 
no son aceptadas por \texttt{repOK}. Estas diferencias simultáneas pueden deberse a fallas en ambas 
fuentes (API y \texttt{repOK}), o a errores más profundos de especificación. En estos casos, cuando se 
confirma una falla manualmente, la clasificamos como \texttt{error}.

Cabe destacar que diferencias menores en la definición de \emph{scope} entre los enfoques pueden 
producir "falsos positivos" en esta comparación. Esto ocurrió solamente en las estructuras 
\texttt{RBT} y \texttt{FibHeap}, donde \emph{BEAPI} no generó ciertas estructuras válidas dentro del 
alcance debido a restricciones de balanceo interno que requieren secuencias más largas que el 
\emph{scope} establecido. Estos casos se identificaron y descartaron manualmente, observando que al 
aumentar el \emph{scope}, las estructuras de \textsf{Korat} sí aparecen en la salida de \emph{BEAPI}.

Los resultados del experimento se resumen en la Tabla~\ref{table:bugs}. Encontramos errores en 9 de 
los 26 métodos \texttt{repOK} analizados. Esta proporción evidencia la dificultad de escribir 
invariantes de representación correctos, incluso en bibliotecas diseñadas por expertos, y pone de 
manifiesto el valor de \textsf{BEAPI} como herramienta de apoyo para detectar inconsistencias.

En particular, se detectaron errores de subespecificación en clases como \texttt{RBTree}, \texttt{AVL}, 
\texttt{BinTree}, \texttt{FibHeap} y \texttt{NCL}, donde \texttt{repOK} permite estructuras 
incompletas, con campos nulos o claves no válidas, que no pueden ser generadas mediante la API 
original. También se identificaron casos de sobreespecificación, como en \texttt{FibHeap}, donde 
el método \texttt{repOK} rechaza heaps válidos cuyo nodo mínimo es nulo de forma transitoria. 
Finalmente, se confirmaron errores estructurales en las definiciones de altura y ordenamiento en 
clases como \texttt{AVL}, donde las hojas están mal inicializadas o los cálculos son inconsistentes 
con la implementación.

En conclusión, \emph{BEAPI} no solo es útil para generar estructuras exhaustivamente, sino que 
también actúa como un verificador complementario de especificaciones de invariante, permitiendo 
descubrir errores difíciles de detectar mediante revisión manual o prueba dinámica convencional. 
Este enfoque ofrece una vía práctica y automatizada para mejorar la calidad de las especificaciones 
de clase, lo que redunda en mayor confiabilidad del software verificado.

\input{tables/bugRepOk}

\subsection{RQ7: Usando BEAPI para el testing parametrizado}

En esta etapa de la evaluación, se llevó a cabo un análisis exhaustivo para determinar la 
utilidad de los métodos generadores identificados mediante nuestro enfoque, particularmente 
en el contexto de la generación automatizada de casos de prueba. Estos métodos permiten construir 
objetos que pueden utilizarse como entradas en test suites parametrizadas.

Los tests parametrizados constituyen una técnica eficiente para mejorar la cobertura en tests 
automatizados. En lugar de definir casos de prueba específicos para cada configuración, se 
establece una única prueba que se ejecuta múltiples veces con distintos parámetros. En nuestro 
contexto, dichos parámetros son objetos generados por distintas técnicas, los cuales sirven como 
entrada a los métodos de la clase bajo prueba.

Para llevar a cabo este experimento, diseñamos una test suite parametrizada en la que cada método 
público de la clase bajo análisis es ejercitado con objetos generados por distintas técnicas. Las 
herramientas utilizadas para generar estos objetos fueron: la versión estándar de Randoop, que 
genera directamente una suite de tests sin reutilizar objetos previos; una variante de Randoop 
denominada \emph{R-Serialize}, que serializa los objetos construidos durante la generación de 
tests, permitiendo su reutilización como entrada en tests parametrizados; una versión modificada 
de Randoop que prioriza el uso de métodos generadores de objetos identificados previamente mediante 
nuestro enfoque (denominada \emph{R-Builders}); y por último, la herramienta \emph{BEAPI}, que 
produce directamente un conjunto reducido pero válido de objetos a partir de su propia lógica de 
exploración exhaustiva acotada.

A fin de evaluar y comparar el impacto de cada enfoque, se utilizó un benchmark basado en clases 
del paquete \emph{java.util}: \emph{HashSet}, \emph{HashMap}, \emph{TreeMap}, \emph{TreeSet} y \emph{LinkedList}.
Para cada clase se definió un invariante que permite distinguir objetos válidos de inválidos, permitiendo así evaluar la calidad de los generadores.

Las métricas consideradas fueron: la cantidad de objetos válidos e inválidos generados, el tiempo 
de generación (\emph{GTime}), este tiempo es el que se le da cada herramienta para ejecutar. Vale aclarar,
que en el caso de BEAPI, este tiempo es el equivalente a cierto scope. Vale recordar al lector, que BEAPI, al ser un generador exhaustivo,
tiene limite de scope y no de tiempo para generar. 
Ademas se tuvo en cuenta las metricas de la cantidad de casos de prueba ejecutados (\emph{Test}), el 
tiempo total de ejecución de la test suite (\emph{T(Seg)}), la cobertura de ramas alcanzada 
(\emph{Ramas}) y la cantidad de mutantes eliminados (\emph{Mutacion}). Es importante destacar 
que la test suite parametrizada es común a los enfoques \emph{R-Serialize}, \emph{R-Builders} 
y \emph{BEAPI}, permitiendo así una comparación justa sobre los objetos generados. En el caso de 
\emph{Randoop}, la herramienta genera su propia test suite, por lo que los resultados deben 
interpretarse con cautela.

\input{tables/randoopvsBeapi}

La Tabla~\ref{tab:hashSetTools} muestra una comparativa de diferentes técnicas aplicadas sobre 
\texttt{HashSet}. Se observa que al priorizar métodos generadores de objectos en Randoop, \emph{R-Builders}, la cantidad de 
objetos generados se incrementa significativamente (más de 300\% en comparación con la versión 
estándar, \emph{R-Serialize}), lo cual impacta positivamente en la cobertura, principalmente en tiempos de generacion más cortos.

En contraste, \textsf{BEAPI} genera sólo 32 objetos válidos, pero logra una cobertura de ramas y 
mutación equivalente a \texttt{R-Builders}, y superior a la alcanzada por \texttt{R-Serialize} y 
\texttt{Randoop} en lapsos de tiempo cortos. Esta eficiencia evidencia que BEAPI puede generar un 
conjunto reducido de objetos, pero de alta calidad para testing, en muy poco tiempo (menos de cinco 
segundos).

En términos de ejecución, mientras \texttt{R-Builders} y \texttt{R-Serialize} requieren ejecutar 
millones de tests para alcanzar altos niveles de cobertura, \texttt{BEAPI} logra el mismo nivel 
ejecutando apenas unos pocos miles. Esto indica que los objetos generados por \textsf{BEAPI} son 
altamente efectivos como entradas, permitiendo detectar fallas y explorar el espacio de estados 
de manera eficiente.

La Tabla~\ref{tab:treeSetTools} presenta los resultados obtenidos para \texttt{TreeSet}. En 
este caso, las diferencias comienzan a notarse con mayor claridad. \texttt{R-Builders} logra 
una cobertura y mutación ligeramente superior a \texttt{BEAPI}. Una posible explicación para 
este comportamiento radica en que ciertas configuraciones válidas de \texttt{TreeSet} pueden 
requerir secuencias de inserciones y eliminaciones que BEAPI no alcanza debido a su 
limitación por scope. Por el contrario, al ejecutarse sin restricciones estructurales, 
\texttt{R-Builders} puede explorar más libremente el espacio de estados, incluso si requiere 
pasar por configuraciones intermedias más complejas que luego se simplifican.

El comportamiento observado se repite en la Tabla~\ref{tab:treeMapTools}, donde se evalúa la 
clase \texttt{TreeMap}. En este caso, \texttt{R-Builders} vuelve a superar a \texttt{BEAPI} 
en métricas de cobertura y mutación. Nuevamente, la explicación podría estar relacionada con 
estructuras internas que BEAPI no puede construir directamente bajo un scope fijo, como 
árboles parcialmente desbalanceados que luego se ajustan mediante rotaciones o reordenamientos. 
A pesar de estas diferencias, \texttt{BEAPI} mantiene una cobertura muy competitiva y logra 
resultados consistentes con una fracción del esfuerzo computacional, lo que refuerza su valor 
como técnica efectiva, especialmente en contextos donde se requiere control y exhaustividad 
estructural.

En cuanto a la Tabla~\ref{tab:linkedListTools} que presenta los resultados obtenidos para la clase 
\texttt{LinkedList}. En este caso, se observa nuevamente que \texttt{BEAPI}, a pesar de 
generar un número reducido de objetos, mantiene niveles constantes de cobertura y mutación. 
Las técnicas \texttt{R-Serialize} y \texttt{R-Builders} logran cubrir las mismas ramas y casi los mismos 
mutantes que \texttt{BEAPI}, pero requieren ejecutar un volumen mucho mayor de tests. Con respecto a la tabla ~\ref{tab:hashMapTools}, 
correspondiente a \texttt{HashMap}, se aprecia un caso 
similar. \texttt{BEAPI} logra altos niveles de cobertura desde tiempos de generación muy 
reducidos, mientras que las otras técnicas necesitan un volumen considerable de objetos y 
tests para alcanzar resultados comparables. Un aspecto distintivo de este caso es que las 
estructuras de tipo clave-valor introducen combinaciones adicionales, y aún así, la técnica 
mantiene su eficacia. Esto evidencia que los objetos generados por \texttt{BEAPI} no solo son 
válidos, sino también representativos de situaciones relevantes para la clase, permitiendo 
maximizar el valor de los tests ejecutados.
\cacho{pensar porque R-builders le gana en mutacion a beapi, creo que eran sobren estructuras que no la usa BEAPI}

Los resultados obtenidos permiten concluir que los objetos generados por \textsf{BEAPI} son 
efectivos y eficientes para ser utilizados en tests parametrizados. Aunque la cantidad total de 
objetos generados es menor, su calidad y representatividad permiten alcanzar los máximos niveles 
de cobertura con una mínima ejecución. Además, se observa que guiar a generadores aleatorios como 
Randoop mediante la identificación previa de métodos generadores de objectos también mejora significativamente 
la cobertura. Esto hace que se reafirme el valor del otro aporte de esta tesis: la identificación automática de métodos generadores.
Este conocimiento no solo permite mejorar herramientas existentes como Randoop (caso de \texttt{R-Builders}), 
sino que también da lugar a técnicas completamente nuevas como \textsf{BEAPI}, 

En contextos donde el tiempo de ejecución es un factor crítico, o donde se necesita eficiencia sin 
sacrificar cobertura, \textsf{BEAPI} se posiciona como una herramienta valiosa para la generación 
de objetos de prueba. Estos resultados también reafirman la importancia de identificar y utilizar 
adecuadamente los métodos generadores para mejorar la calidad de los tests automatizados.












\cacho{Esto es viejo}

\cacho{ya esta en el capitulo, lo movi para alla
}
Para implementar el algoritmo genético, utilizamos una biblioteca muy popular en Java llamada \emph{Jenetics}\footnote{https://jenetics.io/}. Esta biblioteca está diseñada específicamente para algoritmos evolutivos y nos proporcionó las herramientas necesarias para desarrollar nuestro enfoque genético.

\emph{Jenetics} es una biblioteca robusta y versátil que ofrece una amplia gama de funcionalidades para la implementación de algoritmos genéticos. Nos permitió definir y manipular genes, cromosomas y poblaciones, así como utilizar operadores genéticos como selección, cruzamiento y mutación. Además, cuenta con un sólido conjunto de herramientas de optimización y técnicas de evolución que nos permitieron adaptar el algoritmo a nuestras necesidades específicas.
Gracias a \emph{Jenetics}, pudimos implementar el algoritmo genético de manera eficiente y efectiva, lo que nos permitió explorar y encontrar subconjuntos óptimos de métodos \emph{builders} para las diferentes estructuras de datos en nuestro estudio. Además, \emph{Jenetics} es muy fácil de usar, con una documentación completa y una comunidad activa de usuarios que proporciona soporte y ayuda. 


% \subsubsection{Variacion de acuerdo a los Parametros}

% Ademas, examinamos la sensibilidad de los parámetros del Algoritmo Genético y exploramos los usos de los builders generados. Los resultados obtenidos proporcionaron información valiosa sobre el rendimiento y la utilidad de nuestra técnica en la generación de builders y su aplicación en diversas tareas.

% Nuestra comparacion se basa en medir la cantidad de tiempo que le lleva a cada Algortimo terminar la ejecucio, en la cantidad de candidatos que evalua la funcion de valoracion y cuan bueno es en eficacia para encontrar el minimo y suficiente subconjunto de metodos que pudimos observar en nuestro ground truth \cacho{Agregar seccion}. Ejecutamos el algoritmo 10 veces con el resto de los parametros que no esta en evaluacion con un valor promedio (Crossover=0.5, mutation 0.1, Tournanament 4).
% Tambien utilizamos 30 segundos para la fitness con randoop y scope 6 para BEAPI.

% En la tabla \ref{tab:CrossOverGA} se puede observar como se comporta el algoritmo cuando se utiliza diferente rate para el operador de CrossOver. 



% \subsection{Uso de BEAPI para analizar especificaciones}
% \label{sec:existing-specs-analysis}
% \input{tables/bugRepOk}
% La RQ3 aborda si \textsf{BEAPI} puede ser útil para ayudar al usuario a encontrar fallas en \texttt{repOK}s, mediante la comparación del conjunto de objetos que se pueden generar utilizando la API y el conjunto de objetos generados a partir de utilizar un invariante, como es el caso del \texttt{repOK}. Diseñamos el siguiente procedimiento automatizado. Primero, ejecutamos \emph{BEAPI} para generar un conjunto, \texttt{SA}, 
% de estructuras a partir de la API, y utilizamos \emph{Korat} para generar un conjunto, \texttt{SR}, a partir de \texttt{repOK}, 
% utilizando el mismo ámbito para ambas herramientas. En segundo lugar,  . En tercer lugar, comparamos 
%  los conjuntos \texttt{SA} y \texttt{SR} en cuanto a igualdad. Las diferencias en esta comparación 
%  señalan una discrepancia entre \texttt{repOK} y la API. Existen tres posibles resultados para este 
%  procedimiento automatizado. Si \texttt{SA} $\subset$ \texttt{SR}, es posible que la API genere un 
%  subconjunto de las estructuras válidas, que \texttt{repOK} sufra de subespecificación (\texttt(under)) 
%  (restricciones faltantes), o ambos. En este caso, las estructuras en \texttt{SR} que no pertenecen a
%   \texttt{SA} son evidencia del problema, y el usuario debe analizarlas manualmente para descubrir 
%   dónde está el error. Aquí, informamos los errores de subespecificación (confirmados manualmente) 
%   en \emph{repOK}s que son evidenciados por las estructuras mencionadas. En contraste, cuando \texttt{SR}
%    $\subset$ \texttt{SA}, puede ser el caso de que la API genere un superconjunto de las estructuras válidas
%    , que \texttt{repOK} sufra de sobreespecificación, \texttt(over), (\texttt{repOK} es demasiado restrictivo)
%    , o ambos. Las estructuras en \texttt{SA} que no pertenecen a \texttt{SR} podrían indicar la raíz del error,
%     y nuevamente deben ser analizadas manualmente por el usuario. Informamos los errores de sobreespecificación 
%     (confirmados manualmente) en \texttt{repOK}s que son evidenciados por estas estructuras. 
%     Finalmente, puede darse el caso de que haya estructuras en \texttt{SR} que no pertenecen 
%     a \texttt{SA}, y que haya estructuras (distintas de las anteriores) en \texttt{SA} que no
%      pertenecen a \texttt{SR}. Estos pueden ser debidos a fallos en la API, fallas en \texttt{repOK}, 
%      o ambos. Informamos las fallas confirmadas manualmente en \texttt{repOK}s que son evidenciadas por
%       tales estructuras simplemente como errores (\texttt{repOK} describe un conjunto de estructuras 
%       diferente al que debería). Observa que las diferencias en las definiciones de ámbito de los enfoques 
%       pueden hacer que los conjuntos \texttt{SA} y \texttt{SR} difieran. Esto solo fue el caso en las 
%       estructuras \texttt{RBT} y \texttt{FibHeap}, donde \textsf{BEAPI} generó un conjunto más pequeño de 
%       estructuras para el mismo ámbito que \textsf{Korat} debido a restricciones de balance (como se explica
%        en la Sección \ref{sec:evaluation-vs-korat}). Sin embargo, estos "falsos positivos" se pueden revelar 
%        fácilmente, ya que todas las estructuras generadas por \textsf{Korat} siempre estuvieron incluidas en
%         las estructuras generadas por \textsf{BEAPI} si se utilizaba un ámbito más amplio para este último
%          enfoque. Utilizando esta información, descartamos manualmente los "falsos positivos" debido a las 
%          diferencias de ámbito en \texttt{RBT} y \texttt{FibHeap}.

% Los resultados de este experimento se resumen en la Tabla \ref{table:bugs}. Encontramos fallas en 9 de 26 \texttt{repOK}s utilizando el enfoque descrito anteriormente. El alto número de fallas descubiertas evidencia que los problemas en \texttt{repOK}s son difíciles de encontrar manualmente, y que \textsf{BEAPI} puede ser de gran ayuda para esta tarea.

% \subsection{Comparativa de BEAPI con otras tecnicas de generacion de test}

% \hspace{1cm}

% En esta etapa de la evaluación, se llevó a cabo un análisis exhaustivo para determinar la utilidad de los métodos builders identificados en el contexto del análisis de programas, específicamente en la generación automatizada de casos de prueba. Estos builders se consideran objetos clave que pueden ser utilizados como entradas en test parametrizadas.

% Los test parametrizados son una técnica utilizada en el campo de la generación automatizada de casos de prueba para aumentar la eficiencia y la cobertura de las pruebas. En lugar de escribir casos de prueba individuales para cada escenario posible, los test parametrizados permiten definir un conjunto de parámetros que se utilizan para generar automáticamente múltiples casos de prueba.

% En el contexto de la evaluación experimental, se utilizó la técnica de test parametrizados para alimentar una test suite con objetos creados por diferentes técnicas. Esto significa que se definieron parámetros que representan diferentes características o propiedades de los objetos, y luego se generaron automáticamente casos de prueba utilizando estos parámetros.

% Los test parametrizados son una técnica poderosa en la generación automatizada de casos de prueba, ya que permiten explorar diferentes combinaciones de parámetros y generar una variedad de casos de prueba de manera eficiente. En el contexto de la evaluación experimental, se utilizaron para evaluar y comparar el desempeño de diferentes técnicas en la generación de objetos y su impacto en la calidad de las pruebas.

% Para llevar a cabo este experimento, se creó una test suite parametrizada que serviría como marco de prueba para evaluar los distintos enfoques. Esta test suite parametrizada fue, básicamente, crear un test por método que contiene la clase y ejercitarlos con los objetos creados por las diferentes técnicas. Se utilizaron varias técnicas y herramientas para generar los objetos necesarios. En primer lugar, se empleó la conocida herramienta \texttt{Randoop} utilizando como es su forma estander, con todos los métodos disponibles en su API, lo que da lugar a una suite de pruebas tradicional generada por Randoop. En este caso no se utiliza una test suite parametrizada, ya que \texttt{Randoop}  no genera objetos, sino que crea sus propias tests suite.

% Ahora si, se utilizó una variante de Randoop llamada \texttt{R-Serialize} para serializar las secuencias de pruebas generadas anteriormente. Esto permitió generar objetos que, a su vez, se utilizaron para alimentar la test suite parametrizada, ampliando así el alcance de los casos de prueba. Estos objetos fueron generados utilizando todos los métodos de la API de Randoop, tal como se hizo en el enfoque anterior.

% Otra herramienta utilizada en el experimento fue una versión modificada de \texttt{Randoop}, diseñada específicamente para utilizar únicamente los métodos builders identificados previamente mediante nuestro enfoque (\ref{cap:builders}). Los objetos generados por esta variante especial de Randoop también se serializaron y se incorporaron a la test suite parametrizada, permitiendo una comparación directa entre los objetos generados por los builders identificados y los generados sin utilizar la información de estos métodos builders.

% Por último, se emplearon los objetos generados por la herramienta \texttt{BEAPI}. Estos objetos, creados utilizando su propia API, se integraron en la test suite parametrizada para evaluar su efectividad en la generación de casos de prueba. De esta manera, se obtuvo una visión completa y comparativa de las diferentes técnicas utilizadas en términos de generación de objetos y su impacto en la calidad de las pruebas.

% Mediante este enfoque meticuloso y riguroso, se buscó determinar la capacidad de los builders identificados para mejorar la generación automatizada de casos de prueba y, en última instancia, contribuir a la mejora de la calidad del análisis de programas. Los resultados obtenidos en esta evaluación experimental proporcionaron información valiosa sobre la utilidad y efectividad de los builders en el contexto del análisis de programas, abriendo así nuevas oportunidades para futuras investigaciones y desarrollos en este campo.

% A continuación, se realiza la comparativa de las diferentes herramientas utilizadas en el benchmark de \emph{java.util} previamente mencionado. Los casos de estudio son: \emph{HashSet}, \emph{HashMap}, \emph{TreeMap}, \emph{TreeSet} y \emph{LinkedList}. La tabla de resultados muestra varias métricas, como el tiempo en segundos (\texttt{GTime}) que es tiempo que lleva generar estos objectos/tests, la herramienta utilizada (\texttt{Tool}), la cantidad de objetos válidos generados (\texttt{Valid}) y la cantidad de objetos inválidos generados (\texttt{Invalid}).
% Es importante destacar que se definió un invariante para cada clase bajo evaluación, el cual establece qué estructuras son válidas e inválidas. Todos los objetos generados fueron sometidos a la prueba de estos invariantes para determinar su validez. Sin embargo, es importante mencionar que la técnica \texttt{Randoop} genera directamente una test suite en lugar de objetos individuales, por lo que no se aplica directamente el concepto de validez e invalidez a los casos generados por esta herramienta.

% Además, se realiza una comparación en función de la cantidad de tests generados por cada técnica. Esta medida se refleja en las columnas \texttt{Test}, \texttt{T(Seg)}, que indica la cantidad de tests que se ejecutan y sus respectivos segundos que son necesarios para ejecutar la test suite parametrizada, o no (en el caso de \texttt{Ranndop}), correspondiente.

% Finalmente, se evalúa la calidad de las test suites generadas mediante la comparación de la cobertura de ramas y la cantidad de mutantes eliminados. Estas métricas permiten determinar qué tan efectivas son las test suites en términos de su capacidad para cubrir diferentes ramas del código y eliminar mutantes generados para introducir fallas.

% A continuación analizaremos caso por caso con sus respectivas tablas.

% \input{tables/randoopvsBeapi}


% En esta etapa de la evaluación, se llevó a cabo un análisis exhaustivo para determinar la utilidad de los metodos builders identificados en el contexto del análisis de programas, específicamente en la generación automatizada de casos de prueba. Estos builders se consideran objetos clave que pueden ser utilizados como entradas en test parametrizadas. 
% Los test parametrizados son una técnica utilizada en el campo de la generación automatizada de casos de prueba para aumentar la eficiencia y la cobertura de las pruebas. En lugar de escribir casos de prueba individuales para cada escenario posible, los test parametrizados permiten definir un conjunto de parámetros que se utilizan para generar automáticamente múltiples casos de prueba.
% En el contexto de la evaluación experimental, se utilizó la técnica de test parametrizados para alimentar una test suite con objetos creados por diferentes técnicas. Esto significa que se definieron parámetros que representan diferentes características o propiedades de los objetos, y luego se generaron automáticamente casos de prueba utilizando estos parámetros.
% Los test parametrizados son una técnica poderosa en la generación automatizada de casos de prueba, ya que permiten explorar diferentes combinaciones de parámetros y generar una variedad de casos de prueba de manera eficiente. En el contexto de la evaluación experimental, se utilizaron para evaluar y comparar el desempeño de diferentes técnicas en la generación de objetos y su impacto en la calidad de las pruebas.

% Para llevar a cabo este experimento, se creó una test suite parametrizada que serviría como marco de prueba para evaluar los distintos enfoques. Esta test suite parametrizada fue, basicamente, crear un test por metodos que contiene la clase y ejercitalos con los objectos creados por las distintas técnicas. Se utilizaron varias técnicas y herramientas para generar los objetos necesarios. En primer lugar, se empleó la conocida herramienta \texttt{Randoop}, utilizando todos los métodos disponibles en su API estándar, lo que dio lugar a una suite de pruebas tradicional generada por Randoop.

% Además, se utilizó una variante de Randoop llamada \texttt{R-Serialize} para serializar las secuencias de pruebas generadas anteriormente. Esto permitió generar objetos que, a su vez, se utilizaron para alimentar la test suite parametrizada, ampliando así el alcance de los casos de prueba. Estos objetos fueron generados utilizando todos los métodos de la API de Randoop, tal como se hizo en el enfoque anterior.

% Otra herramienta utilizada en el experimento fue una versión modificada de \texttt{Randoop}, diseñada específicamente para utilizar únicamente los métodos builders identificados previamente mediante nuestro enfoque (\ref{cap:builders}). Los objetos generados por esta variante especial de Randoop también se serializaron y se incorporaron a la test suite parametrizada, permitiendo una comparación directa entre los objetos generados por los builders identificados y los generados sin utilizar la informacion de estos metodos builders.

% Por último, se emplearon los objetos generados por la herramienta \texttt{BEAPI}. Estos objetos, creados utilizando su propia API, se integraron en la test suite parametrizada para evaluar su efectividad en la generación de casos de prueba. De esta manera, se obtuvo una visión completa y comparativa de las diferentes técnicas utilizadas en términos de generación de objetos y su impacto en la calidad de las pruebas.

% Mediante este enfoque meticuloso y riguroso, se buscó determinar la capacidad de los builders identificados para mejorar la generación automatizada de casos de prueba y, en última instancia, contribuir a la mejora de la calidad del análisis de programas. Los resultados obtenidos en esta evaluación experimental proporcionaron información valiosa sobre la utilidad y efectividad de los builders en el contexto del análisis de programas, abriendo así nuevas oportunidades para futuras investigaciones y desarrollos en este campo.


%  A continuacion hacemos la comparativa de las diferentes tools, explicada en el parrafo anterior, en el benchmarks de \emph{java.util} utilizado previamente. Los casos de estudios son; \emph{HashSet},\emph{HashMap},\emph{TreeMap},\emph{TreeSet}, \emph{LinkedList}. La comparacion se realiza sobre, \texttt{GTime}, que representa el tiempo en milisegundos, \texttt{Tool} indica la herramienta utilizada, \texttt{Valid} muestra la cantidad de objectos válidos, \texttt{Invalid} muestra la cantidad de objetos inválidos que son generados. Para saber que objetos son válidos e inválidos, escribimos un invariante para cada clase bajo evaluacion, y este nos dice que estructura es valida y que estructura es invalida. Puede observar que para la tecnica \texttt{Randoo} esto no cuenta, ya que recuerde que aqui utilizamos la tool \texttt{Randoop} que generan test suite directamnete y no objectos. Ademas, BEAPI siempre va a generar objectos validos, esto se debe a que como estos objectos son construidos desde la API, no hay posibilidad de generar objectos invalidos. Vale aclarar que estos objectos tambien fueron puesto bajo prueba del mismo invariante que escribimos para cada clase. 
%  Ademas, comparamos de acuerdo a la cantidad de test que cada tecnica genera. Para realizar esta medida, en las tools que utilizan la test suite parametrizada, indica la cantidad de test que se ejecutan y es un valor de que tiene relacion con los objectos que se alimentan a la test suite. En base a esto \texttt{T(Seg)} es la cantidad de segundo que lleva ejecutar esta test suite.
%  Por ultimo, comparamos que tan buena es estas test suite, comprando la cobertura de ramas y de mutantes que matan.


