%!TEX root = main.tex
\chapter[Evaluaci\'on]{Evaluaci\'on}
\label{cap:evaluation}



En los capítulos \ref{cap:builders} y \ref{cap:beapi} presentamos varios enfoques para mejorar la generación exhaustiva acotada:

\begin{itemize}
\item Identificación de métodos \emph{builders} para construir todos los objetos disponibles hasta una cota en una API. En este sentido, presentamos los siguientes algoritmos para encontrar estos métodos:
\begin{itemize}
\item Algoritmo Genético.
\item Algoritmo Greedy.
\item Algoritmo basado en clases de equivalencia.
\end{itemize}
\item Generación exhaustiva acotada a partir de métodos de la API.
\end{itemize}

En este capítulo, realizamos una evaluación experimental de estas técnicas con el objetivo de analizar la eficiencia y efectividad de las suites producidas en comparación con la generación exhaustiva acotada. En otras palabras, las siguientes preguntas de investigación guían esta experimentación:

\cacho{TODO: pensar mejor las RQ}
\begin{itemize}
\item \emph{RQ1}: ¿Qué tan eficientes son los algoritmos (Algoritmo Genético, Greedy, Clases de Equivalencia) para identificar conjuntos de métodos suficientes? 
\item \emph{RQ2}: ¿Qué tan efectivos son los algoritmos presentados para identificar conjuntos de métodos \emph{builders} suficientes de cada API?
\emph{builders} y cuánto tiempo computacional requiere cada enfoque?
\item \emph{RQ3}: ¿Cuál es el impacto de utilizar métodos Builders identificados en comparación con el uso de todos los métodos de la API en el contexto de verificación de software?
\item \emph{RQ4}: ¿Puede realizarse eficientemente la generación exhaustiva utilizando métodos de la API (BEAPI)?
\item\emph{RQ4}: ¿Cuál es el impacto de las optimizaciones propuestas en el rendimiento de BEG a partir de la API?
\item\emph{RQ6}: ¿Puede BEAPI ayudar a encontrar discrepancias entre las especificaciones repOK y la capacidad de generación de objetos de la API?
\item\emph{RQ7}: ¿Qué tan efectivas son las suites de prueba reducidas en comparación con la generación exhaustiva acotada?
\end{itemize}

% \section{Algoritmos de Identificacion de metodos builders}

\section{Evaluación Experimental en identificación de métodos builders}
En esta sección, evaluamos experimentalmente nuestros enfoques presentados en el capítulo \ref{cap:builders}. Describiremos como es la eficiencia y efectividad de cada algoritmo con las funciones de valoración presentada en la sección \ref{sec:fitness}. Además, mostraremos el uso de estos métodos identificados por nuestras técnicas durante el uso de una herramienta de verificación de software.

La evaluación se basa en un conjunto de implementaciones de estructuras de datos que se utilizan como referencia, incluyendo: \verb"NCL" de Apache Collections \cite{apache}; \verb"BinaryTree", \verb"BinomialHeap", \verb"FibonacciHeap" tomados de \cite{Visser:2006}; \verb"UnionFind", una implementación de conjuntos disjuntos tomada de JGrapht \cite{jgrapht}. También evaluamos nuestra técnica en componentes de proyectos de software reales, como \verb"Lits" de la implementación de Sat4j \cite{sat4j}, tomada de \cite{Loncaric:2018}, que consiste en un almacén de variables que controla cuándo se realizó la última suposición sobre el valor de una variable y si hay oyentes observando el estado de esa variable; \verb"Scheduler", una implementación de un planificador de procesos tomada de \cite{sir}; y estructuras de datos conocidas del paquete \verb"java.util" \footnote{https://docs.oracle.com/javase/8/docs/api/java/util/package-summary.html} como: \verb"TreeMap", \verb"TreeSet", \verb"HashMap", \verb"HashSet" y \verb"LinkedList". 

Para corroborar nuestros resultados, necesitamos conocer cuáles son los métodos builders mínimos y suficiente de cada caso de estudio. Para esto hemos creado un \emph{ground Truth} manualmente. Fue necesario inspeccionar cada caso de manera manual. Esto llevo un largo trabajo para estudiar los casos de estudio más complejo. El \emph{ground truth} de nuestros casos de estudios se pueden observar en la tabla \ref{tab:groundTruth}. Esta incluye el número de rutinas en la API completa (\#API), y el conjunto de los \emph{builders} identificados manualmente (algunos métodos pueden intercambiarse en diferentes ejecuciones, por ejemplo, \texttt{addFirst} y \texttt{addLast} en NCL)
    
Todos los experimentos se ejecutaron en máquinas Intel Core i7-6700 de 3.4 GHz con 8GB de RAM, utilizando GNU/Linux como sistema operativo. 

\input{tables/groundTruth}

\subsection{Parámetros y Evaluación}
Como se discutió en el capítulo \ref{cap:beapi}, es necesario configurar ciertos parámetros para el funcionamiento de nuestros algoritmos y funciones de valoración. Los parámetros más importantes que afectan la eficiencia y efectividad de nuestro algoritmo genético son la tasa de mutación y de cruce, el tamaño de la población y el número de torneos en la selección. 

Para nuestra función de evaluación de Randoop, es importante conocer el tiempo disponible para generar objetos. Ademas, Randoop  necesita de una semilla. Esta semilla en Randoop se utiliza para controlar la generación de pruebas aleatorias y permite reproducir los mismos resultados en ejecuciones posteriores si utilizas la misma semilla. Esto puede ser útil para la reproducibilidad y comparación de resultados en el contexto de pruebas de software. Los resultados de Randoop, siempre tiene una aleatoriedad que, entre otras cosas, depende de esta semilla. En nuestros experimentos, cada vez que ejecutamos nuestra técnica, elegimos aleatoriamente una semilla para poder ver como se comporta. 
En cuanto a la función de valoración de Bounded Exhaustive, es crucial determinar el alcance de las estructuras generadas. Además, debemos omitir ciertos valores de los campos de nuestras clases que son irrelevantes para la generación de objetos. Estos campos son aquellos que cambian la representación del objeto pero no su estructura en sí. Un ejemplo común en varias clases de prueba es el campo \emph{ModCount}, que es un contador de modificaciones. Esta variable se utiliza a menudo en estructuras de datos para realizar un seguimiento de las modificaciones o cambios realizados en los datos almacenados, pero claramente no afecta la estructura generada.

Para establecer estos valores en la experimentación, realizamos una tarea de prueba y error para determinar los valores que mejor se ajustan a nuestros algoritmos. Esto es comúnmente realizado en algoritmos genéticos para determinar los parámetros óptimos para un problema específico.

Los mejores valores que hemos obtenido para nuestros problemas son los siguientes:
\begin{itemize}
    \item Algoritmo Genético:
    \begin{itemize}
        \item Tasa de cruce: 0.4
        \item Tasa de mutación: 0.05
        \item Selección: Torneo (4)
        \item Tamaño de población: 100
        \item Número de evoluciones: 20
    \end{itemize}
    \item Función de valoración, Cobertura de Randoop:
    \begin{itemize}
        \item Tiempo de ejecución de Randoop: 30 segundos
        \item Seeds: 2 (aleatorias para cada ejecución)
    \end{itemize}
     \item Función de valoración, Generador BE:
    \begin{itemize}
        \item Scope: 5
        \item omisión de campos: modCount
    \end{itemize}
\end{itemize}

Para realizar la evaluación de la técnica de identificación de métodos \emph{builders}, cada resultado que se muestra en la siguiente sección corresponde a la ejecución del algoritmo 5 veces para cada función de valoración con los parámetros mencionados anteriormente. Luego de estas evaluaciones, se calcula el promedio de los datos obtenidos.
Vale aclarar que para la función de valoración que utiliza el generador exhaustivo, no hace falta ejecutarlo diferentes veces, debido a que este resultado es determinístico si se utiliza los mismos parámetros.


Para implementar el algoritmo genético, utilizamos una biblioteca muy popular en Java llamada \emph{Jenetics}\footnote{https://jenetics.io/}. Esta biblioteca está diseñada específicamente para algoritmos evolutivos y nos proporcionó las herramientas necesarias para desarrollar nuestro enfoque genético.

\emph{Jenetics} es una biblioteca robusta y versátil que ofrece una amplia gama de funcionalidades para la implementación de algoritmos genéticos. Nos permitió definir y manipular genes, cromosomas y poblaciones, así como utilizar operadores genéticos como selección, cruzamiento y mutación. Además, cuenta con un sólido conjunto de herramientas de optimización y técnicas de evolución que nos permitieron adaptar el algoritmo a nuestras necesidades específicas.
Gracias a \emph{Jenetics}, pudimos implementar el algoritmo genético de manera eficiente y efectiva, lo que nos permitió explorar y encontrar subconjuntos óptimos de métodos \emph{builders} para las diferentes estructuras de datos en nuestro estudio. Además, \emph{Jenetics} es muy fácil de usar, con una documentación completa y una comunidad activa de usuarios que proporciona soporte y ayuda. 

% necesarias para obtener las soluciones. Comparando los resultados obtenidos con la técnica evolutiva y la técnica greedy, pudimos evaluar la eficiencia de ambos enfoques y determinar cuál es más rápido en términos de tiempo de ejecución.
\subsection{Eficiencia}
La eficiencia de nuestros algoritmos es la cantidad de segundos que le lleva a cada uno de nuestros algoritmos con cada función de valoración para encontrar el subconjunto de métodos \emph{builders} suficientes. Para evaluar la eficiencia de nuestro algoritmo, llevamos a cabo experimentos utilizando conjuntos de datos de muestra y medimos el tiempo de promedio de las 5 ejecuciones necesarias para obtener las soluciones. Comparamos los resultados obtenidos con la técnica evolutiva (\emph{GA}), la técnica greedy (\emph{HC}) y la técnica de clase de equivalencia (\emph{SubSet}) para evaluar la eficiencia de ambos enfoques en función de la función de valoración utilizada. Los resultados se presentan en la Tabla \ref{tab:eficiencia}.


Como se observa en la tabla, el algoritmo Hill Climbing (\emph{HC}) es más efectivo que el algoritmo genético (\emph{GA}) y el algoritmo de subconjuntos (\emph{SubSet}) para calcular los constructores de métodos en todos los casos de estudio considerados. El enfoque de Hill Climbing es razonablemente eficiente, tardando solo 20 minutos en el peor de los casos (NCL, debido a la cantidad y complejidad de su API), mientras que el algoritmo genético puede tardar mas de una hora en el peor de los casos. Esto se debe a que los algoritmos de Hill Climbing comienzan desde abajo hacia arriba, considerando menos métodos antes de considerar más métodos, a diferencia de los algoritmos genéticos, que generan sucesores mediante operadores de cruce y mutación. Esta estrategia favorece al algoritmo Hill Climbing para encontrar los métodos mínimos en tiempos más cortos. Cabe mencionar que en casos de estudio donde el algoritmo genético supera al Hill Climbing, se debe a que la clase bajo prueba tiene pocos métodos, lo que permite que los algoritmos evolutivos converjan más rápidamente que el algoritmo Hill Climbing.

En cuanto a nuestro último algoritmo, el de subconjuntos, suele tener una menor eficiencia en casos donde la API bajo prueba tiene una gran cantidad de métodos, como se observa en el caso de \emph{LinkedList} que es el caso que tiene más métodos. En este caso puede tardar hasta 3 horas con la fitness que utiliza cobertura de ramas como métrica. Esto se debe a que el algoritmo de subconjuntos debe realizar muchas combinaciones de métodos, y cuanto mayor sea la variedad de valores de la función de valoración, más lento será su proceso de convergencia.

En relación con las funciones de valoración, observamos que la función de valoración basada en el generador exhaustivo (GE) logra finalizar la búsqueda de métodos \emph{builders} en mucho menos tiempo en comparación con la función de valoración basada en la cobertura generada por la suite de pruebas modificada del Randoop (RC). Esto se aplica a todos los algoritmos implementados. La función de valoración GE converge antes en los algoritmos debido a que genera objetos hasta el alcance especificado (en este caso, 5 para todos los casos) y luego finaliza la ejecución del candidato actual para continuar con los siguientes. Todas tienen un \emph{timeout} de 30 segundos para el caso de pueda generar muchas estrucutras con el scope dado. Esto permite una convergencia más rápida en comparación con la función de valoración basada en Randoop, que agota su presupuesto de tiempo para cada candidato ejecutado en el algoritmo. Es importante destacar que todos los algoritmos se ejecutan con 10 hilos simultáneamente, evaluando así 10 candidatos a la vez para cada algoritmo.

En conclusión, en términos de eficiencia de nuestro algoritmo, para los casos estudiados en esta tesis, es conveniente utilizar el algoritmo Hill Climbing con una función de valoración que cuente los objetos generados por el generador exhaustivo (\emph{GE}).

\input{tables/eficienciaBuilders}

\subsection{Efectividad}
\cacho{Poner que beapi va hasta el scope o haste el tiempo dado. Pero casi siempre es scope porq es pequeno}

La efectividad de nuestros algoritmos se basa en qué tan cerca están de encontrar el conjunto mínimo de métodos \emph{builders} para las diferentes estructuras de datos. Es importante tener en cuenta que siempre encontramos al menos un subconjunto de métodos \emph{builders} que es suficiente para su utilización. Sin embargo, cuanto más mínimo sea este subconjunto, mejores resultados obtendremos en términos de generación de inputs para testing.

Realizamos experimentos utilizando las estructuras de datos especificadas en nuestra primera sección de este capítulo. Evaluamos tres algoritmos: Hill Climbing (\emph{HC}), Subset (\emph{SubSet}). Para cada algoritmo, utilizamos dos funciones de valoración: una basada en el generador exhaustivo (\emph{BE}) y otra basada en la cobertura de ramas (\emph{RC}).
Es importante destacar que la función de valoración que utiliza la test suite generada por Randoop, la cual hemos modificado para medir la cobertura de ramas, produce resultados variables según la semilla utilizada. La semilla tiene importancia en la aleatoriedad que posee Randoop, lo que puede afectar los resultados de la función de valoración.

Para la estructura \emph{NCL}, el algoritmo \emph{HC} con la función de valoración que cuenta objectos de nuestro generador exhaustivo, \emph{BE} encuentra un subconjunto de métodos \emph{builders} que coincide con el conjunto mínimo, es decir, no agrega métodos adicionales. Sin embargo, al utilizar la función de valoración \emph{RC}, el algoritmo \emph{HC} encuentra un subconjunto con 0.6 métodos adicionales en promedio. Esto indica que en 3 de las 5 ejecuciones se agrega un método extra.
En cuanto al algoritmo \emph{SubSet}, se comporta de manera similar al algoritmo \emph{HC} para ambas funciones de valoración.
El algoritmo evolutivo, en las 5 ejecuciones, tiene encuentra un método de más con la función de valoración que utiliza cobertura, mientras que la función de valoración con el generador exhaustivo en 4 de las 5 ejecuciones encuentra un método de más. Esto se debe a que el algoritmo genético es muy sensible a los parámetros que se utiliza y puede depender de cada caso. Si se le da el tiempo, los parámetros y las evoluciones necesarias, este algoritmo siempre encontraría el mínimo sin ningún método extra. Así mismo con la función de valoración, tiene un método de más en todas las ejecuciones.


En el caso de las estructuras \emph{UnionFind, FibonacciHeap, BTree, BinomialHeap y Scheduler}, todos los algoritmos logran encontrar el conjunto mínimo de métodos \emph{builders}, ya que no se encuentran métodos adicionales en ninguno de los casos. Esto tambien tiene que ver con que tienen menos cantidad de metodos en sus API y la complejidad de los mismos es diferente al resto de las clases que forman parte de nuestro benchmarks.

En la estructura \emph{Lits}, la función de valoración basada en el generador exhaustivo siempre encuentra al menos un método adicional. Esto se debe a que el generador exhaustivo depende de los valores con los que están inicializados los arreglos en la API de List, lo que puede generar un método adicional en el conjunto mínimo de métodos \emph{builders}. 
En la API de \emph{List}, la cual, es una biblioteca Java para resolver problemas booleanos de satisfacibilidad (SAT). Esta contiene una colección de arreglos que almacena los literales utilizados en un problema SAT. Proporciona métodos para manipular y acceder a literales, como agregar literales a la estructura, recuperar literales por sus índices, negar literales, etc. Nuestro generador exhaustivo depende de con que valores estén inicializados estos arreglos, y el scope que le hemos pasado. Es por esto que agrega un método para crear arreglos que no tiene posibilidad de hacerlo con los métodos builders y el scope que se le dio. Esto no quiere decir que sea builders el método que se agrega, solo que con el scope dado, no tiene posibilidad de crearlo con métodos alternativos. Si aumentamos el scope y modificamos la clase, nos encuentra siempre el subconjunto mínimo.
En cambio, la función de valoración basada en la cobertura de ramas no depende del scope dado y nos encuentra siempre el mismo subconjunto de métodos que es el mínimo y suficiente.

Para las estructuras de \emph{java.util}, el algoritmo evolutivo evaluado a través de la cobertura de ramas depende mucho del tiempo y los parámetros utilizados. Esto se debe a que estas estructuras tienen una mayor cantidad de métodos en su API, lo que hace más desafiante encontrar el conjunto mínimo de métodos \emph{builders}. Por un lado, la función de valoración basada en el generador exhaustivo encuentra el conjunto mínimo en la mayoría de las ejecuciones, con la excepción de HashSet, que agrega un método adicional que es un constructor de la clase que permite generar estructuras que no son posibles con los otros métodos hasta el scope dado.
Por otro lado, la función de valoración de de randoop \cacho{TODO, puede no encontrar el suficiente porq satura coverage.}


En resumen, la efectividad de nuestros algoritmos varía dependiendo de la estructura de datos y la función de valoración utilizada. Algunos algoritmos y funciones de valoración logran encontrar el conjunto mínimo de métodos \emph{builders}, mientras que otros pueden agregar métodos adicionales en ciertos casos.


% La efectividad de nuestros algoritmos se basa en qué tan cerca están de encontrar el conjunto mínimo de métodos \emph{builders}. Es importante destacar que nuestros algoritmos siempre encuentran al menos un subconjunto de métodos \emph{builders} que es suficiente. Sin embargo, cuanto más mínimo sea este subconjunto, mejores resultados obtendremos al utilizarlo, por ejemplo, durante la generación de inputs para testing.

\input{tables/eficaciaBuilders}


% Además de analizar la eficiencia, también investigamos la sensibilidad de algunos parámetros del GA. Ajustamos parámetros como el tamaño de la población, la tasa de mutación y el número de generaciones, y observamos cómo estos cambios afectaron el rendimiento del algoritmo y los resultados obtenidos. Esto nos permitió identificar las configuraciones óptimas de los parámetros y entender cómo influyen en el proceso de búsqueda.

Adicionalmente, examinamos los usos de los builders generados por nuestra técnica. Analizamos las características y las funcionalidades de los builders aprendidos y evaluamos su utilidad en diferentes tareas. Estudiamos cómo los builders pueden ser aplicados en el análisis y la manipulación de datos, y evaluamos su efectividad en términos de rendimiento y calidad de los resultados obtenidos.



% \paragraph{Randoop Objects}
% \label{sec:randoopObjectsExp}




% \paragraph{Bounded Exhuastive}
% \label{sec:BEExp}



% \subsubsection{Variacion de acuerdo a los Parametros}

% Ademas, examinamos la sensibilidad de los parámetros del Algoritmo Genético y exploramos los usos de los builders generados. Los resultados obtenidos proporcionaron información valiosa sobre el rendimiento y la utilidad de nuestra técnica en la generación de builders y su aplicación en diversas tareas.

% Nuestra comparacion se basa en medir la cantidad de tiempo que le lleva a cada Algortimo terminar la ejecucio, en la cantidad de candidatos que evalua la funcion de valoracion y cuan bueno es en eficacia para encontrar el minimo y suficiente subconjunto de metodos que pudimos observar en nuestro ground truth \cacho{Agregar seccion}. Ejecutamos el algoritmo 10 veces con el resto de los parametros que no esta en evaluacion con un valor promedio (Crossover=0.5, mutation 0.1, Tournanament 4).
% Tambien utilizamos 30 segundos para la fitness con randoop y scope 6 para BEAPI.

% En la tabla \ref{tab:CrossOverGA} se puede observar como se comporta el algoritmo cuando se utiliza diferente rate para el operador de CrossOver. 





% \input{tables/tableMutation}
\subsection{Uso de Builders para generar inputs de programas}
En esta parte de la evaluación nos referimos a cuán útiles son los métodos \emph{builders} identificados en el contexto de un análisis de programa, en particular, la generación automatizada de casos de prueba. Estos objetos podrían utilizarse, por ejemplo, como entradas en pruebas unitarias parametrizadas. Para los estudios de caso que proporcionan mecanismos para medir el tamaño de los objetos y comparar objetos por igualdad (es decir, los métodos size y equals de las estructuras de datos), generamos pruebas con Randoop utilizando todos los métodos disponibles en la API (API), y luego generamos pruebas con Randoop utilizando solo los métodos \emph{builders} (BLD) identificados por nuestro enfoque en el experimento anterior (Tabla \ref{tab:results-compute-bld}). Luego, comparamos el número de objetos diferentes (No. of Objs.) y el tamaño del objeto más grande (Max Obj. Size) creado por las pruebas generadas a partir de la API, en comparación con las pruebas generadas utilizando solo los métodos de BLD. Configuramos tres presupuestos diferentes para la generación de pruebas: 60, 120 y 180 segundos (Budget). Los resultados se resumen en la Tabla \ref{tab:results-obj}. Los resultados muestran que, en el mismo presupuesto de pruebas, BLD genera en promedio un 500 porciento más de objetos que API. En todos los casos, BLD también genera objetos significativamente más grandes que API. A la luz de estos resultados, queda claro que la identificación automatizada de \emph{builders} vale la pena para la generación automatizada de estructuras para clases con estado.
\input{tables/objectsBuilders}


\subsection{Uso de Builders en Verificación}

En el ultimo experimento sobre la identificación de \emph{builders}, utilicé Java PathFinder \cite{Visser:2005} (JPF) para realizar pruebas de generación de entradas de software para estructuras de datos de \emph{java.util}. JPF \footnote{https://github.com/javapathfinder/jpf-core}  es un verificador de modelos de estado explícito para programas escritos en Java. Para realizar la verificación, las técnicas de versificación de modelos de software se basan en la definición de controladores de métodos: combinaciones de métodos que permiten construir las entradas con las que se ejecutará el programa. Intuitivamente, es deseable seleccionar el menor conjunto de métodos posible, cuyas combinaciones permitan construir todas las estructuras acotadas para el módulo (para analizar el software con todas las entradas posibles). La dificultad de escribir controladores de pruebas es un obstáculo importante para el uso de un verificador de modelos. Esta selección de métodos, que generalmente se realiza manualmente, no es una tarea fácil: requiere un análisis exhaustivo de las rutinas disponibles en el módulo y una comprensión profunda de su semántica.
Es posible construir un método no determinista (harness de test) que genere todas las secuencias de llamadas a métodos de la API hasta un tamaño especificado por el usuario (scope). JPF se utiliza para enumerar todas estas secuencias. JPF almacena todos los estados explorados y retrocede cuando visita un estado previamente explorado.

JPF admite anotaciones de programa que se agregan a los programas a través de llamadas a métodos de una clase especial Verify.
Utilizamos los siguientes métodos de la biblioteca JPF \verb"Verify":
\\
\begin{itemize}
\item El método \verb"Verify.getInt(int lo, int hi)" devuelve un valor entre \verb"lo" y \verb"hi", inclusive. Crea un punto de elección no determinista: JPF necesita explorar las ejecuciones para todos los valores en el rango.
\item \verb"random(int n)" devuelve valores de 0 a \verb"n", de manera no determinista.
\end{itemize}

Si se desea verificar que el método \emph{put}  de TreeMap cumple con el repOK (predicado imperativo que verifica las invariantes de clase) de la estructura de datos, es necesario escribir algo como: 
\\

\begin{lstlisting}[caption={Probando el método put de TreeMap con JPF},label={lst:label},language=Java,captionpos=b]
public static void main(String[] args) {
   int scope = 3;
   TreeMap t = generateStructure(scope);
   t.put(Verify.getInt(0,scope),Verify.getInt(0,scope));
   assert t.repOK();
}
\end{lstlisting}

Para realizar el análisis de esta propiedad, es necesario proporcionar a JPF los mecanismos para generar todo el árbol de entrada (\textit{generateStructure}). 
En el siguiente ejemplo, mostramos un controlador de prueba construido con todos los métodos de la estructura de datos \textit{TreeMap}:
\\
\\
\begin{lstlisting}[caption={Controlador con todos los métodos},label={lst:driverAPI},language=Java,captionpos=b]
private static TreeMap generateStructure(int scope) {
   int maxLength = Verify.getInt(0, scope);
   TreeMap t = new TreeMap();
   for (int i = 1; i <= maxLength; i++) {
      switch (Verify.random(n_methods)) {
         case 0:
            t.put(Verify.getInt(0,scope),Verify.getInt(0,scope));
            break;
         case 1:
            t.remove(Verify.getInt(0,scope));
            break;						
         case 2:
            t.clear();
            break;
         case 3:
            t.containsValue(Verify.getInt(0,scope));
            break;
         ...
         case 11: 
            t.putAll(l);
            break;
      }
   }
   return t;
}
\end{lstlisting}

El método controlador anterior, en primer lugar, selecciona el número de métodos a ejecutar, \textit{maxLength}, un número entre 0 y \textit{scope}. (Linea 2). Cada iteración del ciclo (Linea 4 a 23) corresponde a la ejecución de un solo método, seleccionado de manera no determinista entre todos los disponibles. En el caso de que el usuario no conozca el conjunto de métodos builders (y no quiera hacer el complicado trabajo de seleccionarlos manualmente), la solución más segura para evitar descartar métodos importantes es utilizar todos los métodos disponibles en el módulo, como se muestra en el método controlador descrito anteriormente. En el cuerpo del bucle, a cada método se le asigna un número entero entre 0 y \textit{n\_methods}. Se elige de manera no determinista el método a ejecutar en el ciclo actual. Por ejemplo, si \textit{n\_methods}=1, se ejecuta el método \textit{remove}. Es fácil ver que el número de ejecuciones posibles que se deben explorar por JPF crece exponencialmente con el número de métodos disponibles.

%(for every method that is executed in one iteration, there are \textit{n_methods} possible methods to execute in the next iteration).

Para evitar este crecimiento exponencial en este experimento, se propone utilizar únicamente los constructores detectados por nuestro enfoque explicado en la sección \ref{sec:builders}.
\\
\\
\begin{lstlisting}[caption={Controlador con métodos constructores},label={lst:driverBLD},language=Java,captionpos=b]
private static TreeMap generateStructure(int scope) {
   int maxLength = Verify.getInt(0,scope);
   TreeMap t = new TreeMap();
   for (int i = 1; i <= maxLength; i++) {
      switch (Verify.random(11)) {
         case 0:
            t.put(Verify.getInt(0,scope),Verify.getInt(0,scope));
            break;
         case 1:
            t.remove(Verify.getInt(0,scope));
            break;						
      }
   }
   return t;
}
\end{lstlisting}

Como se muestra en el controlador anterior, solo 2 métodos componen un conjunto mínimo y suficiente para construir controladores para TreeMap de \textit{java.util}. Utilizando solo esos métodos, generamos exactamente los mismos objetos TreeMap que antes (porque los constructores son suficientes y mínimos), y por lo tanto JPF explora las mismas ejecuciones de propiedades con ambos controladores.

Los resultados de la tabla \ref{tab:results-jpf1} y \ref{tab:results-jpf2} muestran que la construcción del controlador a partir de nuestro enfoque presentado permite aumentar la eficiencia y escalabilidad a estructuras más grandes en el análisis utilizando JPF. Esto se debe a la reducción de los métodos utilizados en los controladores (evitando métodos superfluos) y manteniendo la capacidad de construir todos los objetos acotados posibles (debido a la suficiencia de los métodos elegidos) en menos tiempo.

\input{tables/tableJPFBuilders}
\input{tables/tableJPFBuilders1}


\section{BEAPI}

\subsection{Eficiencia}

\input{tables/tableEficienciaBEAPI}

\subsection{Impacto de las Optimizaciones realizas en BEAPI}


\input{tables/optimizaciones}

\subsection{Uso de BEAPI para analizar especificaciones}

\subsection{Comparativa de BEAPI con otras tecnicas de generacion de test}

\input{tables/randoopvsBeapi}
\hspace{1cm}





\subsection{Uso de Builders en BEAPI}
% En la segunda parte de la evaluación, analizamos qué tan útiles son los builders identificados en el contexto de un análisis de programas, específicamente en la generación automatizada de casos de prueba. Estos objetos pueden utilizarse, por ejemplo, como entradas en test parametrizados. Para los estudios de caso que proporcionan mecanismos para medir el tamaño de los objetos y compararlos por igualdad (es decir, los métodos size y equals de las estructuras de datos), generamos pruebas con Randoop utilizando todos los métodos disponibles en la API (API), y luego generamos pruebas con Randoop utilizando solo los métodos builders (BLD) identificados por nuestro enfoque en el experimento anterior (Tabla \ref{tab:results-compute-bld}). Luego comparamos el número de objetos diferentes (No. de Objs.) y el tamaño del objeto más grande (Max Obj. Size) generados por las pruebas generadas a partir de la API, en comparación con las pruebas generadas utilizando solo los métodos de BLD. Establecimos tres budget diferentes para la generación de pruebas: 60, 120 y 180 segundos (Budget). Los resultados se resumen en la Tabla \ref{tab:results-obj}. Los resultados muestran que, en el mismo presupuesto de pruebas, BLD genera en promedio un 500 porciento más de objetos que la API. En todos los casos, BLD también genera objetos significativamente más grandes que la API. A la luz de estos resultados, queda claro que la identificación automatizada de builders es beneficiosa para la generación automatizada de estructuras para clases con estado.

% Además, comparamos las suites de pruebas generadas con Randoop midiendo la cobertura de ramas y líneas de código. Al igual que en el experimento anterior, la suite de pruebas utilizada para la comparación se generó a partir de los métodos builders (BLD) en comparación con el uso de todos los métodos disponibles de la API con un Randoop predeterminado. Establecimos cinco presupuestos diferentes para la generación de pruebas: 60, 120, 180, 300 y 600 segundos (Budget). Los resultados se resumen en la Tabla \ref{tab:results-coverage}.

% % Cabe destacar que la tabla \ref{tab:results-coverage} muestra que, para los casos que manipulan estructuras de datos complejas (por ejemplo, java.util.TreeMap), las pruebas generadas solo con los métodos BLD obtienen una mejor cobertura tanto en ramas como en líneas de código
.
% \begin{table}[!thb]
% \scriptsize
% \centering
% \caption{Tiempo de ejecución de BEAPI con diferentes configuración.}


% \end{table}

% \begin{table}[!thb]
% \scriptsize

% \centering
% \begin{tabular}{ l r | r | r | r | r  }
%   \toprule
%   \multicolumn{6}{c}{\textbf{Real World}} \\
%   \midrule 
%   \textbf{Class} & \textbf{S} & \textbf{SM/BLD} & \textbf{SM}  & \textbf{BLD} & \textbf{NoOPT}  \\
%   \midrule
%   NCL
%   & 3 & .10 & .47 & -  & -  \\
%   & 4 & .41 & 3.48  &  - & -  \\
%   & 5 & 3.33  &  - &  - & -  \\
%   & 6 & 73.78 &  - &  - &  - \\
%   \midrule
%   TSet
%   & 3 & .03 & .07 & 56.82 & - \\
%   & 11  & 21.52 & 86.06 &  - & -  \\
%   & 12  & 69.98 & 276.85  &  - & -  \\
%   & 13  & 226.66  & 887.83  &  - & -  \\
%     \midrule
%   TMap
%   & 3 & .11 & .25 & - & - \\
%   & 4 & .75 & 2.36  &  - &  - \\
%   & 5 & 15.97 & 57.64 &  - & -  \\
%   & 6 & 839.87  & 2901.37 &  - &  - \\
%   \midrule
%   LList
%   & 3 & .02 & .13 & .64 & - \\
%   & 6 & .96 & 258.85  & -  &  - \\
%   & 7 & 12.98 &  - &  - &  - \\
%   & 8 & 286.21  & -  & -  & -  \\
%   \midrule
%   HMap
%   & 3 & .10 & 11.49 & - & - \\
%   & 4 & .55 & -  & -  & -  \\
%   & 5 & 5.33  & -  &  - &  - \\
%   & 6 & 119.87  &  - &  - &  - \\
%   \bottomrule

% \end{tabular}

% \label{table:beapi}

% \end{table}




